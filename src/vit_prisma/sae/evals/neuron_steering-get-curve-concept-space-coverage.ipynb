{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating your SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from vit_prisma.sae.config import VisionModelSAERunnerConfig\n",
    "\n",
    "selected_sae_weights = \"sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-11-hook_resid_post-l1-0.0001/n_images_2600058.pt\"\n",
    "\n",
    "# int(selected_sae_weights.split(\"layer\")[1].split(\"-\")[1])\n",
    "selected_sae_layer = int(selected_sae_weights.split(\"layer\")[1].split(\"-\")[1])\n",
    "# selected_sae_hook_point = \"mlp_out\"\n",
    "selected_sae_hook_point = \"resid_post\"\n",
    "\n",
    "saved_sae_dir_path = \"\"\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig(VisionModelSAERunnerConfig):\n",
    "    sae_path: str = f'/workspace/saved_saes/{selected_sae_weights}'\n",
    "    model_name: str = \"open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "    model_type: str =  \"clip\"\n",
    "    patch_size: str = 32\n",
    "\n",
    "    dataset_path = \"/workspace\"\n",
    "    dataset_train_path: str = \"/workspace/ILSVRC/Data/CLS-LOC/train\"\n",
    "    dataset_val_path: str = \"/workspace/ILSVRC/Data/CLS-LOC/val\"\n",
    "\n",
    "    verbose: bool = True\n",
    "\n",
    "    device: bool = 'cuda'\n",
    "\n",
    "    eval_max: int = 50_000 # 50_000\n",
    "    batch_size: int = 32\n",
    "        \n",
    "#     hook_point_layer: int = 11\n",
    "    # make the max image output folder a subfolder of the sae path\n",
    "\n",
    "\n",
    "    @property\n",
    "    def max_image_output_folder(self) -> str:\n",
    "        # Get the base directory of sae_checkpoints\n",
    "        sae_base_dir = os.path.dirname(os.path.dirname(self.sae_path))\n",
    "        \n",
    "        # Get the name of the original SAE checkpoint folder\n",
    "        sae_folder_name = os.path.basename(os.path.dirname(self.sae_path))\n",
    "        \n",
    "        # Create a new folder path in sae_checkpoints/images with the original name\n",
    "        output_folder = os.path.join(sae_base_dir, 'max_images', sae_folder_name)\n",
    "        output_folder = os.path.join(output_folder, f\"layer_{self.hook_point_layer}\") # Add layer number\n",
    "\n",
    "        \n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        return output_folder\n",
    "\n",
    "cfg = EvalConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.models.base_vit import HookedViT\n",
    "\n",
    "model_name = \"open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "model = HookedViT.from_pretrained(model_name, is_timm=False, is_clip=True).to(cfg.device)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import vit_prisma\n",
    "# importlib.reload(vit_prisma.dataloaders.imagenet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "import open_clip\n",
    "from vit_prisma.utils.data_utils.imagenet_utils import setup_imagenet_paths\n",
    "from vit_prisma.dataloaders.imagenet_dataset import get_imagenet_transforms_clip, ImageNetValidationDataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "og_model_name = \"hf-hub:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "og_model, _, preproc = open_clip.create_model_and_transforms(og_model_name)\n",
    "processor = preproc\n",
    "\n",
    "size=224\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                     std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "    \n",
    "imagenet_paths = setup_imagenet_paths(cfg.dataset_path)\n",
    "imagenet_paths[\"train\"] = \"/workspace/ILSVRC/Data/CLS-LOC/train\"\n",
    "imagenet_paths[\"val\"] = \"/workspace/ILSVRC/Data/CLS-LOC/val\"\n",
    "imagenet_paths[\"val_labels\"] = \"/workspace/LOC_val_solution.csv\"\n",
    "imagenet_paths[\"label_strings\"] = \"/workspace/LOC_synset_mapping.txt\"\n",
    "print()\n",
    "train_data = torchvision.datasets.ImageFolder(cfg.dataset_train_path, transform=data_transforms)\n",
    "val_data = ImageNetValidationDataset(cfg.dataset_val_path, \n",
    "                                imagenet_paths['label_strings'], \n",
    "                                imagenet_paths['val_labels'], \n",
    "                                data_transforms,\n",
    "                                return_index=True,\n",
    ")\n",
    "val_data_visualize = ImageNetValidationDataset(cfg.dataset_val_path, \n",
    "                                imagenet_paths['label_strings'], \n",
    "                                imagenet_paths['val_labels'],\n",
    "                                torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),]), return_index=True)\n",
    "\n",
    "print(f\"Validation data length: {len(val_data)}\") if cfg.verbose else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.sae.training.activations_store import VisionActivationsStore\n",
    "# import dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# activations_loader = VisionActivationsStore(cfg, model, train_data, eval_dataset=val_data)\n",
    "val_dataloader = DataLoader(val_data, batch_size=cfg.batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained SAE to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.sae.sae import SparseAutoencoder\n",
    "sparse_autoencoder = SparseAutoencoder(cfg).load_from_pretrained(f\"/workspace/saved_saes/{selected_sae_weights}\")\n",
    "sparse_autoencoder.to(cfg.device)\n",
    "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who \n",
    "\n",
    "# topk config needs editing for this to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Labeling AutoInterp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.dataloaders.imagenet_dataset import get_imagenet_index_to_name\n",
    "ind_to_name = get_imagenet_index_to_name()\n",
    "\n",
    "all_imagenet_class_names = []\n",
    "for i in range(len(ind_to_name)):\n",
    "    all_imagenet_class_names.append(ind_to_name[str(i)][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are these missing ReLU?\n",
    "# A: No, relu is in encode_* which is run as part of normal sae forward.\n",
    "# topk runs as usual\n",
    "\n",
    "def neuron_steering_hook_fn_cls_only(\n",
    "    activations, cfg, hook, sae, steering_indices, steering_strength=1.0, mean_ablation_values=None, include_error=False\n",
    "\n",
    "):\n",
    "    activations[:, 0, steering_indices] = torch.tensor(steering_strength).cuda()\n",
    "    return activations.clone()\n",
    "\n",
    "\n",
    "def neuron_steering_hook_fn(\n",
    "    activations, cfg, hook, sae, steering_indices, steering_strength=1.0, mean_ablation_values=None, include_error=False\n",
    "\n",
    "):\n",
    "    activations[:, :, steering_indices] = torch.tensor(steering_strength).cuda()\n",
    "    return activations.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = sample size of features\n",
    "# for neurons, this is b_dec.shape[0]\n",
    "random_feat_idxs = np.random.choice(range(0, sparse_autoencoder.b_dec.shape[0]), size=(768), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given feature, set it high/low on maxim activ. imgs and high/low on non-activ images\n",
    "# hook SAE and replace desired feature with 0 or 1 \n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_feature_activations_set_feat(\n",
    "    images: torch.Tensor,\n",
    "    model: torch.nn.Module,\n",
    "    sparse_autoencoder: torch.nn.Module,\n",
    "    encoder_weights: torch.Tensor,\n",
    "    encoder_biases: torch.Tensor,\n",
    "    feature_ids: List[int],\n",
    "    feature_categories: List[str],\n",
    "    top_k: int = 10,\n",
    "    steering_strength: float = 10.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the highest activating tokens for given features in a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        images: Input images\n",
    "        model: The main model\n",
    "        sparse_autoencoder: The sparse autoencoder\n",
    "        encoder_weights: Encoder weights for selected features\n",
    "        encoder_biases: Encoder biases for selected features\n",
    "        feature_ids: List of feature IDs to analyze\n",
    "        feature_categories: Categories of the features\n",
    "        top_k: Number of top activations to return per feature\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping feature IDs to tuples of (top_indices, top_values)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "#     hook_point_local = \"blocks.0.hook_mlp_out\"\n",
    "    hook_point_local = f\"blocks.{selected_sae_layer}.hook_{selected_sae_hook_point}\"        \n",
    "        \n",
    "    \n",
    "    recons_image_embeddings_feat_altered_list = []\n",
    "    for idx in np.array(range(sparse_autoencoder.W_dec.shape[0]))[random_feat_idxs]:\n",
    "        steering_hook = partial(neuron_steering_hook_fn,\n",
    "                                cfg=cfg,\n",
    "                                sae=sparse_autoencoder,\n",
    "                                steering_indices=[idx],\n",
    "                                steering_strength=steering_strength,\n",
    "                                mean_ablation_values = [1.0],\n",
    "                                include_error=True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        recons_image_embeddings_feat_altered = model.run_with_hooks(\n",
    "            images,\n",
    "            fwd_hooks=[(hook_point_local, steering_hook)],\n",
    "        )\n",
    "        recons_image_embeddings_feat_altered_list.append(recons_image_embeddings_feat_altered)\n",
    "\n",
    "    \n",
    "    # output is in clip embedding space\n",
    "    recons_image_embeddings_default = model.run_with_hooks(\n",
    "        images,\n",
    "        fwd_hooks=[(hook_point_local, lambda x, hook: x)],\n",
    "    )\n",
    "    \n",
    "    return recons_image_embeddings_feat_altered_list, recons_image_embeddings_default\n",
    "\n",
    "print(f\"blocks.{selected_sae_layer}.hook_{selected_sae_hook_point}\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from collections import defaultdict\n",
    "max_samples = cfg.eval_max\n",
    "\n",
    "encoder_biases = sparse_autoencoder.b_enc\n",
    "encoder_weights = sparse_autoencoder.W_enc\n",
    "\n",
    "steering_strengths = [0.0, 150.0]\n",
    "\n",
    "\n",
    "# these two things need to be pickled and documented well\n",
    "# they cache run results\n",
    "steering_strength_image_results = defaultdict(dict)\n",
    "steering_strength_info = {}\n",
    "steering_strength_info_dicts = {}\n",
    "\n",
    "og_model.cuda()\n",
    "\n",
    "vocabulary_file = \"/workspace/clip_dissect_raw.txt\"\n",
    "\n",
    "\n",
    "for steering_strength in steering_strengths:\n",
    "    print(f\"{'==============' * 2} steering_strength: {steering_strength} {'==============' * 2}\")\n",
    "    # ===== Get Steered and Default CLIP Outputs =====\n",
    "    top_k=10\n",
    "    processed_samples = 0\n",
    "    default_embeds_list = []\n",
    "    feature_steered_embeds = defaultdict(list)\n",
    "    l = 0\n",
    "    \n",
    "    # remove tqdm\n",
    "    for batch_images, _, batch_indices in tqdm(val_dataloader, total=max_samples // cfg.batch_size):\n",
    "        batch_images = batch_images.to(cfg.device)\n",
    "        batch_indices = batch_indices.to(cfg.device)\n",
    "        batch_size = batch_images.shape[0]\n",
    "\n",
    "        altered_embeds_list, default_embeds = compute_feature_activations_set_feat(\n",
    "            batch_images, model, sparse_autoencoder, encoder_weights, encoder_biases,\n",
    "            None, None, top_k, steering_strength\n",
    "        )\n",
    "        default_embeds_list.append(default_embeds)\n",
    "        for j, altered_embeds in enumerate(altered_embeds_list):\n",
    "            feature_steered_embeds[random_feat_idxs[j]].extend(altered_embeds)\n",
    "        # either label embeds or optimize to maximal token in text transformer embedding face\n",
    "        l += 1\n",
    "        if l >= 6:\n",
    "            break    \n",
    "    default_embeds = torch.cat(default_embeds_list)\n",
    "    \n",
    "    with open(vocabulary_file, \"r\") as f:\n",
    "        larger_vocab = [line[:-1] for line in f.readlines()][:5000]\n",
    "\n",
    "\n",
    "    # ===== CLIP Embeds =====\n",
    "    # use clip vocab here and compare embeds\n",
    "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "    text = tokenizer(larger_vocab)\n",
    "    text_features = og_model.encode_text(text.cuda()) # text *embeddings*\n",
    "    text_features_normed = text_features/text_features.norm(dim=-1, keepdim=True) # normalized text *embeddings*\n",
    "\n",
    "\n",
    "    print(f\"text_features_normed.shape: {text_features_normed.shape}\")\n",
    "    text_probs_altered_list = []\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        for key in feature_steered_embeds:\n",
    "            print(key)\n",
    "            # embeds already have L2 norm of 1\n",
    "            text_probs_altered = (100.0 * torch.stack(feature_steered_embeds[key]) @ text_features_normed.T).softmax(dim=-1)\n",
    "            text_probs_altered_list.append(text_probs_altered)\n",
    "        text_probs_default = (100.0 * default_embeds @ text_features_normed.T).softmax(dim=-1)\n",
    "\n",
    "    \n",
    "    # ===== Logit Difference =====\n",
    "    # indexed as such in steering_strength_image_results:\n",
    "    # per steering strength\n",
    "    # per feature\n",
    "    # per image\n",
    "    \n",
    "    selected_vocab = larger_vocab\n",
    "\n",
    "    top_concept_per_feat = {}\n",
    "    top_val_per_feat = {}\n",
    "    top_diff_per_feat = {}\n",
    "    steerability_per_feat = {}\n",
    "    steerability_dir_per_feat = {}\n",
    "    top_ratio_per_feat = {}\n",
    "    feat_polysemanticity_dist = {}\n",
    "    \n",
    "    # run this for sampled features over all of imagenet eval\n",
    "    for j, text_probs_altered in enumerate(text_probs_altered_list):\n",
    "        print(f\"{'============================================'*2}\\n\\nFor Feature {random_feat_idxs[j]}\")\n",
    "        default_vals_softmax, default_idxs_softmax = torch.topk(text_probs_default,k=10)\n",
    "\n",
    "        # this is for polysemanticity metric:\n",
    "        aggregate_prob_difference = (text_probs_altered - text_probs_default).mean(dim=0)\n",
    "        res = aggregate_prob_difference @ text_features_normed\n",
    "        diff = res - text_features_normed.sum(dim=0)/text_features_normed.sum(dim=0).norm(p=2.0)\n",
    "        feat_polysemanticity_dist[random_feat_idxs[j]] = diff.norm(p=2.0)\n",
    "\n",
    "        logit_diff = text_probs_altered - text_probs_default\n",
    "        logit_diff_aggregate = logit_diff.mean(dim=0)\n",
    "    \n",
    "        steerability_score = torch.square(logit_diff_aggregate)\n",
    "        steerability_directional = torch.mul(logit_diff_aggregate, torch.abs(logit_diff_aggregate))\n",
    "\n",
    "        # AHEM these are not logits - these are probabilities\n",
    "        logit_ratio = text_probs_altered/text_probs_default\n",
    "        logit_ratio_aggregate = logit_ratio.mean(dim=0)\n",
    "\n",
    "        text_probs_altered_softmax = text_probs_altered#.softmax(1)\n",
    "        vals_softmax, idxs_softmax = torch.topk(text_probs_altered_softmax,k=10)\n",
    "\n",
    "        vals_agg, idxs_agg = torch.topk(logit_diff_aggregate,k=10)\n",
    "        vals_least_agg, idxs_least_agg = torch.topk(logit_diff_aggregate,k=10,largest=False)\n",
    "\n",
    "        ratios_agg, ratios_idxs_agg = torch.topk(logit_ratio_aggregate,k=10)\n",
    "        ratios_least_agg, ratios_idxs_least_agg = torch.topk(logit_ratio_aggregate,k=10,largest=False)\n",
    "\n",
    "        vals, idxs = torch.topk(logit_diff,k=5)\n",
    "        vals_least, idxs_least = torch.topk(logit_diff,k=5,largest=False)\n",
    "\n",
    "        ratios, ratios_idxs = torch.topk(logit_ratio,k=5)\n",
    "        ratios_least, ratios_idxs_least = torch.topk(logit_ratio,k=5,largest=False)\n",
    "\n",
    "        # random_feat_idxs[j] is the index of the feature\n",
    "        for img_idx in range(batch_images.shape[0]):\n",
    "            if random_feat_idxs[j] not in steering_strength_image_results[str(steering_strength)].keys():\n",
    "                steering_strength_image_results[str(steering_strength)][random_feat_idxs[j].copy()] = []\n",
    "            # entries are torch.topk(k=10) results\n",
    "            steering_strength_image_results[str(steering_strength)][random_feat_idxs[j]].append((np.array(selected_vocab, copy=True)[idxs_softmax.cpu()][img_idx], torch.clone(vals_softmax[img_idx])))\n",
    "        \n",
    "        # per image\n",
    "        top_concept_per_feat[random_feat_idxs[j]] = np.array(selected_vocab)[idxs_softmax.cpu()][0][0]\n",
    "        top_val_per_feat[random_feat_idxs[j]] = vals_softmax[0][0]\n",
    "        \n",
    "        # aggregate\n",
    "        steerability_per_feat[random_feat_idxs[j]] = steerability_score\n",
    "        steerability_dir_per_feat[random_feat_idxs[j]] = steerability_directional\n",
    "        top_diff_per_feat[random_feat_idxs[j]] = vals_agg[0]\n",
    "        top_ratio_per_feat[random_feat_idxs[j]] = ratios_agg[0]\n",
    "\n",
    "\n",
    "        print(f\"\\nMost Changed, by Absolute Diff Over {logit_diff.shape[0]} Images:\\n{vals_agg}\")\n",
    "        print(np.array(selected_vocab)[idxs_agg.cpu()])\n",
    "\n",
    "        print(f\"\\nMost Changed, by Ratio Over {logit_diff.shape[0]} Images:\")\n",
    "        print(ratios_agg)\n",
    "        print(np.array(selected_vocab)[ratios_idxs_agg.cpu()])\n",
    "    \n",
    "    steering_strength_info[steering_strength] = (top_concept_per_feat,top_val_per_feat,top_ratio_per_feat,top_diff_per_feat,steerability_per_feat,steerability_dir_per_feat, feat_polysemanticity_dist)\n",
    "#     steering_strength_info_dicts[steering_strength] = {\"feat_topk_concepts_per_img\": top_concept_per_feat, \"feat_topk_probs_per_img\": top_val_per_feat, \"feat_topk_concepts_by_ratio\": top_ratio_per_feat, \"feat_topk_concepts_by_abs_diff\": top_diff_per_feat, \"feat_steerability_score_vector\": steerability_per_feat, \"feat_directional_steerability_score_vector\": steerability_dir_per_feat, \"feat_polysemanticity_distance_from_mean\": feat_polysemanticity_dist}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_name = selected_sae_weights.split(\"/\")[0].replace(\"-\", \"_\").replace(\":\", \"_\") + \"_neuron\"\n",
    "os.getcwd()\n",
    "os.chdir(\"/workspace/steerability_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(steering_strength_image_results, open(f\"steering_strength_image_results_{sae_name}.pkl\", \"wb\"))\n",
    "pickle.dump(steering_strength_info, open(f\"steering_strength_info_{sae_name}.pkl\", \"wb\"))\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# steering_strength_image_results = pickle.load(open(f\"steering_strength_image_results_{sae_name}.pkl\", \"rb\"))\n",
    "# steering_strength_info = pickle.load(open(f\"steering_strength_info_{sae_name}.pkl\", \"rb\"))\n",
    "# steering_strength_image_results = pickle.load(open(f\"steering_strength_image_results_{sae_name}.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steering_strength_image_results.keys(), steering_strength_image_results[str(steering_strength)].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "# 0: python, 3: bowl, 4: bed, 6: guinea\n",
    "image_idx = 0\n",
    "\n",
    "\n",
    "feat_num = list(steering_strength_image_results[str(steering_strength)].keys())[0]\n",
    "\n",
    "print(f\"=====================\\nfeat_num: {feat_num}\")\n",
    "feat_num_concept_arr = []\n",
    "feat_num_prob_arr = []\n",
    "for dict_key in steering_strengths:\n",
    "    # image, tuple position, idx of top-k\n",
    "    feat_num_concept_arr.append((dict_key, steering_strength_image_results[str(dict_key)][feat_num][image_idx][0][0]))\n",
    "    feat_num_prob_arr.append((dict_key, steering_strength_image_results[str(dict_key)][feat_num][image_idx][1][0].item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "x = [tup[0] for tup in sorted(feat_num_concept_arr)]\n",
    "y1 = [tup[1] for tup in sorted(feat_num_concept_arr)]\n",
    "y2 = [tup[1] for tup in sorted(feat_num_prob_arr)]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each line with different colors and markers\n",
    "plt.plot(np.array(x), y2, 'rs-', label='Prob, Label at Strength')  # Red line with squares\n",
    "\n",
    "# Label each point\n",
    "for i in range(len(x)):\n",
    "    # Labels for series\n",
    "    plt.annotate(f'({y2[i]:01f}, {y1[i]})', \n",
    "                (x[i], y2[i]), \n",
    "                textcoords=\"offset points\", \n",
    "                xytext=(0,-15),\n",
    "                ha='center')\n",
    "    \n",
    "# Customize the plot\n",
    "plt.xlabel('Feature Steering Strength (feat = Strength)')\n",
    "plt.ylabel('Probability of TPL, Top Predicted Label')\n",
    "plt.title(f'Most Likely Class by Feature Steering Strength, Feature {feat_num}\\n Label at 0.0: {steering_strength_image_results[str(0.0)][feat_num][image_idx][0][0]}. Label at max steered val ({str(max(steering_strengths))}): {steering_strength_image_results[str(max(steering_strengths))][feat_num][image_idx][0][0]}.')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout to prevent label overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# plt.savefig(\"test.svg\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_idx = image_idx\n",
    "feat_num = feat_num\n",
    "\n",
    "print(f\"=====================\\nfeat_num: {feat_num}\")\n",
    "feat_num_concept_arr = []\n",
    "feat_num_concept_arr_2 = []\n",
    "feat_num_concept_arr_3 = []\n",
    "feat_num_prob_arr = []\n",
    "feat_num_prob_arr_2 = []\n",
    "feat_num_prob_arr_3 = []\n",
    "for dict_key in steering_strengths:\n",
    "    # steering_strength_image_results is: [steering_strength][feat_num][image_idx][0:concept name,1:concept_probability][i in 0-9 for img i in top-k]\n",
    "    \n",
    "    # concept names\n",
    "    feat_num_concept_arr.append((dict_key, steering_strength_image_results[str(dict_key)][feat_num][image_idx][0][0]))\n",
    "    feat_num_concept_arr_2.append((dict_key, steering_strength_image_results[str(dict_key)][feat_num][image_idx][0][1]))\n",
    "    feat_num_concept_arr_3.append((dict_key, steering_strength_image_results[str(dict_key)][feat_num][image_idx][0][2]))\n",
    "\n",
    "    # probabilities\n",
    "    feat_num_prob_arr.append((dict_key, steering_strength_image_results[str(dict_key)][feat_num][image_idx][1][0].item()))\n",
    "    feat_num_prob_arr_2.append((dict_key, steering_strength_image_results[str(dict_key)][feat_num][image_idx][1][1].item()))\n",
    "    feat_num_prob_arr_3.append((dict_key, steering_strength_image_results[str(dict_key)][feat_num][image_idx][1][2].item()))\n",
    "\n",
    "\n",
    "# Sample data\n",
    "x = [tup[0] for tup in sorted(feat_num_concept_arr)]\n",
    "y1 = [tup[1] for tup in sorted(feat_num_concept_arr)]\n",
    "y2 = [tup[1] for tup in sorted(feat_num_prob_arr)]\n",
    "y1_2 = [tup[1] for tup in sorted(feat_num_concept_arr_2)]\n",
    "y2_2 = [tup[1] for tup in sorted(feat_num_prob_arr_2)]\n",
    "y1_3 = [tup[1] for tup in sorted(feat_num_concept_arr_3)]\n",
    "y2_3 = [tup[1] for tup in sorted(feat_num_prob_arr_3)]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(15, 9))\n",
    "\n",
    "# Plot each line with different colors and markers\n",
    "plt.plot(np.array(x), y2, 'rs-', label='Prob, Label at Strength')  # Red line with squares\n",
    "plt.plot(np.array(x), y2_2, 'ms-', label='Prob, Label at Strength')  # Red line with squares\n",
    "plt.plot(np.array(x), y2_3, 'ys-', label='Prob, Label at Strength')  # Red line with squares\n",
    "\n",
    "# Label each point\n",
    "for i in range(len(x)):\n",
    "    # Labels for series\n",
    "    plt.annotate(f'({y2[i]:01f}, {y1[i]})', \n",
    "                (x[i], y2[i]), \n",
    "                textcoords=\"offset points\", \n",
    "                xytext=(0,-15),\n",
    "                ha='center')\n",
    "    plt.annotate(f'({y2_2[i]:01f}, {y1_2[i]})', \n",
    "                (x[i], y2_2[i]), \n",
    "                textcoords=\"offset points\", \n",
    "                xytext=(0,-15),\n",
    "                ha='center')\n",
    "    plt.annotate(f'({y2_3[i]:01f}, {y1_3[i]})', \n",
    "                (x[i], y2_3[i]), \n",
    "                textcoords=\"offset points\", \n",
    "                xytext=(0,-15),\n",
    "                ha='center')\n",
    "    \n",
    "# Customize the plot\n",
    "plt.xlabel('Feature Steering Strength (feat = Strength)')\n",
    "plt.ylabel('Probability of TPL, Top Predicted Label')\n",
    "plt.title(f'Most Likely Class by Feature Steering Strength, Feature {feat_num}\\n Label at 0.0: {steering_strength_image_results[str(0.0)][feat_num][image_idx][0][0]}. Label at max steered val ({str(max(steering_strengths))}): {steering_strength_image_results[str(max(steering_strengths))][feat_num][image_idx][0][0]}.')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout to prevent label overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# plt.savefig(\"test.svg\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_strength_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_top_concept_dict = {}\n",
    "\n",
    "# Steerability metric\n",
    "for feat_num in steering_strength_image_results[str(steering_strength)].keys():\n",
    "    feat_num_steerability = feat_num\n",
    "    agg_diff_arr = []\n",
    "    steerability_arr = []\n",
    "    i = 0\n",
    "    for key in steering_strength_info:\n",
    "        steerability_arr.append(steering_strength_info[key][4][feat_num_steerability].sum().cpu().item())\n",
    "        agg_diff_arr.append(steering_strength_info[key][3][feat_num_steerability].cpu().item())\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Sample data\n",
    "    x = [tup[0] for tup in sorted(feat_num_concept_arr)]\n",
    "    y1 = agg_diff_arr\n",
    "    y2 = steerability_arr\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot each line with different colors and markers\n",
    "    plt.plot(np.array(x), y1, 'rs-', label='Strength, Aggregate Probability Difference')  # Red line with squares\n",
    "    plt.plot(np.array(x), y2, 'ms-', label='Strength, Steerability Score')  # Red line with squares\n",
    "\n",
    "    # Label each point\n",
    "    for i in range(len(x)):\n",
    "        # Labels for series\n",
    "        plt.annotate(f'({x[i]:.1f}, {y1[i]:.3f})', \n",
    "                    (x[i], y1[i]), \n",
    "                    textcoords=\"offset points\", \n",
    "                    xytext=(0,-15),\n",
    "                    ha='center')\n",
    "        plt.annotate(f'({x[i]:.1f}, {y2[i]:.3f})', \n",
    "                    (x[i], y2[i]), \n",
    "                    textcoords=\"offset points\", \n",
    "                    xytext=(0,-15),\n",
    "                    ha='center')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.xlabel('Feature Steering Strength (feat = Strength)')\n",
    "    plt.ylabel('Sum of Probability Difference Over All Tested Images')\n",
    "    plt.title(f'Aggregate Probability Difference at different steering strengths, Feature {feat_num_steerability}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Adjust layout to prevent label overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    # plt.savefig(\"test.svg\")\n",
    "    print(steering_strength_info[key][3][feat_num_steerability].sum().cpu().item())\n",
    "    print(steering_strength_info[key][4][feat_num_steerability].sum().cpu().item())\n",
    "    if steering_strength_info[key][4][feat_num_steerability].sum().cpu().item() > .10:\n",
    "        feat_top_concept_dict[feat_num] = steering_strength_image_results[str(key)][feat_num][image_idx][0][0]\n",
    "\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    # image, tuple position, idx of top-k\n",
    "    print(feat_num)\n",
    "    print(dict_key, steering_strength_image_results[str(dict_key)][feat_num][image_idx][0])\n",
    "    print(dict_key, \"probs\", steering_strength_image_results[str(dict_key)][feat_num][image_idx][1])\n",
    "    print(steering_strength_info[150.0][4][feat_num_steerability].shape)\n",
    "    vals, idxs = torch.topk(steering_strength_info[150.0][5][feat_num_steerability].cpu(), k=10)\n",
    "    print(\"probabilities * abs(probabilities)\", vals)\n",
    "    print(np.array(selected_vocab)[idxs.cpu()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "len(Counter(feat_top_concept_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(feat_top_concept_dict.values())\n",
    "import matplotlib.pyplot as plt\n",
    "featctr = Counter(feat_top_concept_dict.values())\n",
    "plt.bar(featctr.keys(), sorted(featctr.values(), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SAE steering power and specificity metrics:\n",
    "\n",
    "sae_asps = []\n",
    "sae_asy_str_specs = []\n",
    "sae_asrs = []\n",
    "for feat_num in steering_strength_image_results[str(steering_strength)].keys():\n",
    "    feat_num_steerability = feat_num\n",
    "    \n",
    "    asymptote_key = max([float(key) for key in steering_strength_info.keys()])\n",
    "    asymptotic_steering_specificity = steering_strength_info[asymptote_key][4][feat_num_steerability].sum().cpu().item()\n",
    "    asymptotic_steering_power = steering_strength_info[asymptote_key][3][feat_num_steerability].cpu().item()\n",
    "    \n",
    "    print(f\"feat: {feat_num_steerability}\")\n",
    "    print(f\"asymptotic_steering_specificity: {asymptotic_steering_specificity}\")\n",
    "    print(f\"asymptotic_steering_power: {asymptotic_steering_power}\")\n",
    "    print(f\"asymptotic_steering_power percent: {(asymptotic_steering_power * 100):.2f}%\")\n",
    "    print(f\"specificity/power: {asymptotic_steering_specificity/asymptotic_steering_power}\")\n",
    "    \n",
    "    sae_asps.append(asymptotic_steering_power)\n",
    "    sae_asy_str_specs.append(asymptotic_steering_specificity)\n",
    "    sae_asrs.append(asymptotic_steering_specificity/asymptotic_steering_power)\n",
    "\n",
    "\n",
    "print(f\"\\nSAE: {selected_sae_weights}\")\n",
    "print(f\"\\nSAE avg power: {np.array(sae_asps).mean()}\")\n",
    "print(f\"SAE avg specificity: {np.array(sae_asy_str_specs).mean()}\")\n",
    "print(f\"SAE avg s/p ratio: {np.array(sae_asrs).mean()}\")\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(sae_asps, bins=100)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(sae_asy_str_specs, bins=100)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(sae_asrs, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prev Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_steered_embeds[random_feat_idxs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_embeds.shape\n",
    "len(default_embeds_list)\n",
    "default_embeds = torch.cat(default_embeds_list)\n",
    "default_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(altered_embeds_list), altered_embeds_list[0].shape, default_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "og_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/clip_dissect_raw.txt\", \"r\") as f:\n",
    "    larger_vocab = [line[:-1] for line in f.readlines()][:5000]\n",
    "\n",
    "# with open(\"/workspace/better_img_desc.txt\", \"r\") as f:\n",
    "#     larger_vocab = [line[:-1] for line in f.readlines()][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use clip vocab here and compare embeds\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "text = tokenizer(larger_vocab)\n",
    "text_features = og_model.encode_text(text.cuda())\n",
    "text_features_normed = text_features/text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "print(f\"text_features_normed.shape: {text_features_normed.shape}\")\n",
    "text_probs_altered_list = []\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    for key in feature_steered_embeds:\n",
    "        print(key)\n",
    "        # embeds already have L2 norm of 1\n",
    "        text_probs_altered = (100.0 * torch.stack(feature_steered_embeds[key]) @ text_features_normed.T).softmax(dim=-1)\n",
    "        text_probs_altered_list.append(text_probs_altered)\n",
    "    text_probs_default = (100.0 * default_embeds @ text_features_normed.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs altered:\", text_probs_altered.shape)  # prints: [[1., 0., 0.]]\n",
    "print(\"Label probs default:\", text_probs_default.shape)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summed Logit Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# subtract from default, label, and print trends\n",
    "text_probs_altered.shape\n",
    "\n",
    "# selected_vocab = all_imagenet_class_names\n",
    "selected_vocab = larger_vocab\n",
    "\n",
    "top_concept_per_feat = {}\n",
    "top_val_per_feat = {}\n",
    "top_diff_per_feat = {}\n",
    "top_ratio_per_feat = {}\n",
    "# run this for sampled features over all of imagenet eval\n",
    "for j, text_probs_altered in enumerate(text_probs_altered_list):\n",
    "    print(f\"{'============================================'*2}\\n\\nFor Feature {random_feat_idxs[j]}\")\n",
    "    print(\"actual image content:\")\n",
    "    default_vals_softmax, default_idxs_softmax = torch.topk(text_probs_default,k=10)\n",
    "    print(default_vals_softmax, \"\\n\", np.array(selected_vocab)[default_idxs_softmax.cpu()])\n",
    "    \n",
    "    \n",
    "    logit_diff = text_probs_altered - text_probs_default\n",
    "    logit_diff_aggregate = logit_diff.sum(dim=0)\n",
    "    \n",
    "    logit_ratio = text_probs_altered/text_probs_default\n",
    "    logit_ratio_aggregate = logit_ratio.mean(dim=0)\n",
    "    \n",
    "    print(f\"text_probs_altered.softmax(): {text_probs_altered.softmax(1).shape}\")\n",
    "    text_probs_altered_softmax = text_probs_altered.softmax(1)\n",
    "    vals_softmax, idxs_softmax = torch.topk(text_probs_altered_softmax,k=10)\n",
    "    \n",
    "#     print(f\"text_probs_altered.softmax(): {text_probs_altered.sum(0).softmax(0).shape}\")\n",
    "#     text_probs_altered_softmax_agg = text_probs_altered.sum(0).softmax(0)\n",
    "#     vals_softmax_agg, idxs_softmax_agg = torch.topk(text_probs_altered_softmax_agg,k=10)\n",
    "    \n",
    "    print(f\"\\nSoftmax Over {text_probs_altered.shape[0]} Images:\\n{vals_softmax}\")\n",
    "    print(np.array(selected_vocab)[idxs_softmax.cpu()])\n",
    "    for i in range(vals_softmax.shape[0]):\n",
    "        print(vals_softmax[i], \"\\n\", np.array(selected_vocab)[idxs_softmax.cpu()][i])\n",
    "        break\n",
    "        \n",
    "#     print(f\"\\nAgg Softmax Over {text_probs_altered.shape[0]} Images:\\n{vals_softmax_agg}\")\n",
    "#     print(np.array(selected_vocab)[idxs_softmax_agg.cpu()])\n",
    "    \n",
    "    vals_agg, idxs_agg = torch.topk(logit_diff_aggregate,k=10)\n",
    "    vals_least_agg, idxs_least_agg = torch.topk(logit_diff_aggregate,k=10,largest=False)\n",
    "    \n",
    "    ratios_agg, ratios_idxs_agg = torch.topk(logit_ratio_aggregate,k=10)\n",
    "    ratios_least_agg, ratios_idxs_least_agg = torch.topk(logit_ratio_aggregate,k=10,largest=False)\n",
    "    \n",
    "    vals, idxs = torch.topk(logit_diff,k=5)\n",
    "    vals_least, idxs_least = torch.topk(logit_diff,k=5,largest=False)\n",
    "    \n",
    "    ratios, ratios_idxs = torch.topk(logit_ratio,k=5)\n",
    "    ratios_least, ratios_idxs_least = torch.topk(logit_ratio,k=5,largest=False)\n",
    "    \n",
    "    top_concept_per_feat[random_feat_idxs[j]] = np.array(selected_vocab)[idxs_softmax.cpu()][0][0]\n",
    "    top_val_per_feat[random_feat_idxs[j]] = vals_softmax[0][0]\n",
    "    top_diff_per_feat[random_feat_idxs[j]] = vals_agg[0]\n",
    "    top_ratio_per_feat[random_feat_idxs[j]] = ratios_agg[0]\n",
    "    \n",
    "    \n",
    "    print(f\"\\nMost Changed, by Absolute Diff Over {logit_diff.shape[0]} Images:\\n{vals_agg}\")\n",
    "    print(np.array(selected_vocab)[idxs_agg.cpu()])\n",
    "    print(vals_least_agg)\n",
    "    print(np.array(selected_vocab)[idxs_least_agg.cpu()])\n",
    "    \n",
    "    print(f\"\\nMost Changed, by Ratio Over {logit_diff.shape[0]} Images:\")\n",
    "    print(ratios_agg)\n",
    "    print(np.array(selected_vocab)[ratios_idxs_agg.cpu()])\n",
    "    print(vals_least_agg)\n",
    "    print(np.array(selected_vocab)[ratios_idxs_least_agg.cpu()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_concept_per_feat,top_val_per_feat,top_ratio_per_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_strength_info = {}\n",
    "steering_strength_info[steering_strength] = (top_concept_per_feat,top_val_per_feat,top_ratio_per_feat,top_diff_per_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_strength_info[steering_strength][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steering_strength_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for feat_num in steering_strength_info[steering_strength][0].keys():\n",
    "    print(f\"=====================\\nfeat_num: {feat_num}\")\n",
    "    feat_num_concept_arr = []\n",
    "    feat_num_prob_arr = []\n",
    "    feat_num_ratio_arr = []\n",
    "    for key in steering_strength_info:\n",
    "        print(key, steering_strength_info[key][0][feat_num])\n",
    "        feat_num_concept_arr.append((key, steering_strength_info[key][0][feat_num]))\n",
    "        print(key, steering_strength_info[key][1][feat_num])\n",
    "        feat_num_prob_arr.append((key, steering_strength_info[key][1][feat_num].item()))\n",
    "        print(key, steering_strength_info[key][2][feat_num])\n",
    "        feat_num_ratio_arr.append((key, steering_strength_info[key][2][feat_num].item()))\n",
    "    i += 1\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(feat_num_concept_arr),sorted(feat_num_prob_arr),sorted(feat_num_ratio_arr),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "x = [tup[0] for tup in sorted(feat_num_concept_arr)]\n",
    "y1 = [tup[1] for tup in sorted(feat_num_concept_arr)]\n",
    "y2 = [tup[1] for tup in sorted(feat_num_prob_arr)]\n",
    "# y3 = [tup[1] for tup in sorted(feat_num_ratio_arr)]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each line with different colors and markers\n",
    "# plt.plot(x, y1, 'bo-', label='Series 1')  # Blue line with circles\n",
    "plt.plot(np.array(x), y2, 'rs-', label='Series 2')  # Red line with squares\n",
    "# plt.plot(x, y3, 'gd-', label='Series 3')  # Green line with diamonds\n",
    "\n",
    "# Label each point for all three series\n",
    "for i in range(len(x)):\n",
    "#     # Labels for series 1\n",
    "#     plt.annotate(f'({x[i]}, {y1[i]})', \n",
    "#                 (x[i], y1[i]), \n",
    "#                 textcoords=\"offset points\", \n",
    "#                 xytext=(0,10),\n",
    "#                 ha='center')\n",
    "    \n",
    "    # Labels for series 2\n",
    "    plt.annotate(f'({y2[i]:01f}, {y1[i]})', \n",
    "                (x[i], y2[i]), \n",
    "                textcoords=\"offset points\", \n",
    "                xytext=(0,-15),\n",
    "                ha='center')\n",
    "    \n",
    "#     # Labels for series 3\n",
    "#     plt.annotate(f'({x[i]}, {y3[i]})', \n",
    "#                 (x[i], y3[i]), \n",
    "#                 textcoords=\"offset points\", \n",
    "#                 xytext=(0,10),\n",
    "#                 ha='center')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title(f'Most Likely Class by Feature Steering Strength, Feature {feat_num}\\n Label at 0.0: {steering_strength_info[0.0][0][feat_num]}. Label at max steered val: {steering_strength_info[max(list(steering_strength_info.keys()))][0][feat_num]}.')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout to prevent label overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enc/Dec Clustering/Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_weights_for_math = sparse_autoencoder.W_enc\n",
    "decoder_weights_for_math = sparse_autoencoder.W_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_from_feat_0 = encoder_weights_for_math[0] - encoder_weights_for_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_from_feat_0_normalized = encoder_weights_for_math[0]/encoder_weights_for_math[0].norm(p=2) - encoder_weights_for_math/encoder_weights_for_math.norm(p=2,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_from_feat_0.norm(p=2, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(bins[:-1], bins)\n",
    "plt.hist(dists_from_feat_0.norm(p=2, dim=0).cpu(), density=True, bins=1000, histtype='step')  # density=False would make counts\n",
    "plt.title('Encoder Dist from feat 0')\n",
    "plt.ylabel('L2 Distance')\n",
    "plt.xlabel('Density (of ~50k feats)');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(dists_from_feat_0.norm(p=2, dim=0),k=10,largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_dists_from_feat_0 = decoder_weights_for_math[0]/decoder_weights_for_math[0].norm(p=2) - decoder_weights_for_math/decoder_weights_for_math.norm(p=2)\n",
    "dec_dists_from_feat_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(bins[:-1], bins)\n",
    "plt.hist(dec_dists_from_feat_0.T.norm(p=2, dim=0).cpu(), density=True, bins=1000, histtype='step')  # density=False would make counts\n",
    "plt.title('Decoder Dist from feat 0')\n",
    "plt.ylabel('L2 Distance')\n",
    "plt.xlabel('Density (of ~50k feats)');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(dec_dists_from_feat_0.T.norm(p=2, dim=0),k=10,largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_dists_from_feat_0.T.norm(p=2, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
