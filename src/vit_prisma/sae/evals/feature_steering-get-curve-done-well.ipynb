{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating your SAE\n",
    "\n",
    "Code based off Rob Graham's ([themachinefan](https://github.com/themachinefan)) SAE evaluation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/ViT-Prisma/src/vit_prisma/sae/evals'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_per_buffer (millions): 0.032\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 158691\n",
      "Total training images: 13000000\n",
      "Total wandb updates: 15869\n",
      "Expansion factor: 16\n",
      "n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 158 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Gradient clipping with max_norm=1.0\n",
      "Using SAE initialization method: encoder_transpose_decoder\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from vit_prisma.sae.config import VisionModelSAERunnerConfig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig(VisionModelSAERunnerConfig):\n",
    "    sae_path: str = '/workspace/sae_checkpoints/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-11-hook_resid_post-l1-0.0001/n_images_2600058.pt'\n",
    "    model_name: str = \"open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "    model_type: str =  \"clip\"\n",
    "    patch_size: str = 32\n",
    "\n",
    "    dataset_path = \"/workspace\"\n",
    "    dataset_train_path: str = \"/workspace/ILSVRC/Data/CLS-LOC/train\"\n",
    "    dataset_val_path: str = \"/workspace/ILSVRC/Data/CLS-LOC/val\"\n",
    "\n",
    "    verbose: bool = True\n",
    "\n",
    "    device: bool = 'cuda'\n",
    "\n",
    "    eval_max: int = 50_000 # 50_000\n",
    "    batch_size: int = 32\n",
    "\n",
    "    # make the max image output folder a subfolder of the sae path\n",
    "\n",
    "\n",
    "    @property\n",
    "    def max_image_output_folder(self) -> str:\n",
    "        # Get the base directory of sae_checkpoints\n",
    "        sae_base_dir = os.path.dirname(os.path.dirname(self.sae_path))\n",
    "        \n",
    "        # Get the name of the original SAE checkpoint folder\n",
    "        sae_folder_name = os.path.basename(os.path.dirname(self.sae_path))\n",
    "        \n",
    "        # Create a new folder path in sae_checkpoints/images with the original name\n",
    "        output_folder = os.path.join(sae_base_dir, 'max_images', sae_folder_name)\n",
    "        output_folder = os.path.join(output_folder, f\"layer_{self.hook_point_layer}\") # Add layer number\n",
    "\n",
    "        \n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        return output_folder\n",
    "\n",
    "cfg = EvalConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7b346c14de70>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "Official model name open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "Converting OpenCLIP weights\n",
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "visual projection shape torch.Size([768, 512])\n",
      "Setting center_writing_weights to False for OpenCLIP\n",
      "Setting fold_ln to False for OpenCLIP\n",
      "Loaded pretrained model open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from vit_prisma.models.base_vit import HookedViT\n",
    "\n",
    "model_name = \"open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "model = HookedViT.from_pretrained(model_name, is_timm=False, is_clip=True).to(cfg.device)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import vit_prisma\n",
    "# importlib.reload(vit_prisma.dataloaders.imagenet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation data length: 50000\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "import open_clip\n",
    "from vit_prisma.utils.data_utils.imagenet_utils import setup_imagenet_paths\n",
    "from vit_prisma.dataloaders.imagenet_dataset import get_imagenet_transforms_clip, ImageNetValidationDataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "og_model_name = \"hf-hub:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "og_model, _, preproc = open_clip.create_model_and_transforms(og_model_name)\n",
    "processor = preproc\n",
    "\n",
    "size=224\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                     std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "    \n",
    "imagenet_paths = setup_imagenet_paths(cfg.dataset_path)\n",
    "imagenet_paths[\"train\"] = \"/workspace/ILSVRC/Data/CLS-LOC/train\"\n",
    "imagenet_paths[\"val\"] = \"/workspace/ILSVRC/Data/CLS-LOC/val\"\n",
    "imagenet_paths[\"val_labels\"] = \"/workspace/LOC_val_solution.csv\"\n",
    "imagenet_paths[\"label_strings\"] = \"/workspace/LOC_synset_mapping.txt\"\n",
    "print()\n",
    "train_data = torchvision.datasets.ImageFolder(cfg.dataset_train_path, transform=data_transforms)\n",
    "val_data = ImageNetValidationDataset(cfg.dataset_val_path, \n",
    "                                imagenet_paths['label_strings'], \n",
    "                                imagenet_paths['val_labels'], \n",
    "                                data_transforms,\n",
    "                                return_index=True,\n",
    ")\n",
    "val_data_visualize = ImageNetValidationDataset(cfg.dataset_val_path, \n",
    "                                imagenet_paths['label_strings'], \n",
    "                                imagenet_paths['val_labels'],\n",
    "                                torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),]), return_index=True)\n",
    "\n",
    "print(f\"Validation data length: {len(val_data)}\") if cfg.verbose else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.sae.training.activations_store import VisionActivationsStore\n",
    "# import dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# activations_loader = VisionActivationsStore(cfg, model, train_data, eval_dataset=val_data)\n",
    "val_dataloader = DataLoader(val_data, batch_size=cfg.batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained SAE to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_activation_fn received: activation_fn=relu, kwargs={}\n",
      "n_tokens_per_buffer (millions): 0.032\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 158691\n",
      "Total training images: 13000000\n",
      "Total wandb updates: 1586\n",
      "Expansion factor: 64\n",
      "n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 158 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Gradient clipping with max_norm=1.0\n",
      "Using SAE initialization method: encoder_transpose_decoder\n",
      "get_activation_fn received: activation_fn=relu, kwargs={}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseAutoencoder(\n",
       "  (hook_sae_in): HookPoint()\n",
       "  (hook_hidden_pre): HookPoint()\n",
       "  (hook_hidden_post): HookPoint()\n",
       "  (hook_sae_out): HookPoint()\n",
       "  (activation_fn): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vit_prisma.sae.sae import SparseAutoencoder\n",
    "sparse_autoencoder = SparseAutoencoder(cfg).load_from_pretrained(\"/workspace/sae_checkpoints/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-11-hook_resid_post-l1-0.0001/n_images_2600058.pt\")\n",
    "sparse_autoencoder.to(cfg.device)\n",
    "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Labeling AutoInterp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_imagenet_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.dataloaders.imagenet_dataset import get_imagenet_index_to_name\n",
    "ind_to_name = get_imagenet_index_to_name()\n",
    "\n",
    "all_imagenet_class_names = []\n",
    "for i in range(len(ind_to_name)):\n",
    "    all_imagenet_class_names.append(ind_to_name[str(i)][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/sae_checkpoints/max_images/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-11-hook_resid_post-l1-0.0001/layer_9'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.max_image_output_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering_hook_fn_cls_only(\n",
    "    activations, cfg, hook, sae, steering_indices, steering_strength=1.0, mean_ablation_values=None, include_error=False\n",
    "\n",
    "):\n",
    "    sae.to(activations.device)\n",
    "\n",
    "\n",
    "    sae_input = activations.clone()\n",
    "    sae_output, feature_activations, *data = sae(sae_input)\n",
    "    \n",
    "    steered_feature_activations = feature_activations.clone()\n",
    "    \n",
    "    # batch, stream, feats\n",
    "    # cls token is *last* in sequence\n",
    "    steered_feature_activations[:, 0, steering_indices] = steering_strength\n",
    "\n",
    "    steered_sae_out = einops.einsum(\n",
    "                steered_feature_activations,\n",
    "                sae.W_dec,\n",
    "                \"... d_sae, d_sae d_in -> ... d_in\",\n",
    "            ) + sae.b_dec\n",
    "\n",
    "    steered_sae_out = sae.run_time_activation_norm_fn_out(steered_sae_out)\n",
    "    \n",
    "    # print(f\"steering norm: {(steered_sae_out - sae_output).norm()}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    if include_error:\n",
    "        error = sae_input - sae_output\n",
    "        # print(f\"error.norm(): {error.norm()}\")\n",
    "        return steered_sae_out + error\n",
    "    return steered_sae_out\n",
    "\n",
    "def steering_hook_fn(\n",
    "    activations, cfg, hook, sae, steering_indices, steering_strength=1.0, mean_ablation_values=None, include_error=False\n",
    "\n",
    "):\n",
    "    sae.to(activations.device)\n",
    "\n",
    "\n",
    "    sae_input = activations.clone()\n",
    "    sae_output, feature_activations, *data = sae(sae_input)\n",
    "    \n",
    "    steered_feature_activations = feature_activations.clone()\n",
    "    \n",
    "    steered_feature_activations[:, :, steering_indices] = steering_strength\n",
    "\n",
    "    steered_sae_out = einops.einsum(\n",
    "                steered_feature_activations,\n",
    "                sae.W_dec,\n",
    "                \"... d_sae, d_sae d_in -> ... d_in\",\n",
    "            ) + sae.b_dec\n",
    "\n",
    "    steered_sae_out = sae.run_time_activation_norm_fn_out(steered_sae_out)\n",
    "    \n",
    "    # print(f\"steering norm: {(steered_sae_out - sae_output).norm()}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    if include_error:\n",
    "        error = sae_input - sae_output\n",
    "        # print(f\"error.norm(): {error.norm()}\")\n",
    "        return steered_sae_out + error\n",
    "    return steered_sae_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_feat_idxs = np.random.randint(0, high=3000, size=(25))\n",
    "random_feat_idxs[0] = 655\n",
    "random_feat_idxs[1] = 656\n",
    "random_feat_idxs[2] = 665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given feature, set it high/low on maxim activ. imgs and high/low on non-activ images\n",
    "# hook SAE and replace desired feature with 0 or 1 \n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_feature_activations_set_feat(\n",
    "    images: torch.Tensor,\n",
    "    model: torch.nn.Module,\n",
    "    sparse_autoencoder: torch.nn.Module,\n",
    "    encoder_weights: torch.Tensor,\n",
    "    encoder_biases: torch.Tensor,\n",
    "    feature_ids: List[int],\n",
    "    feature_categories: List[str],\n",
    "    top_k: int = 10,\n",
    "    steering_strength: float = 10.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the highest activating tokens for given features in a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        images: Input images\n",
    "        model: The main model\n",
    "        sparse_autoencoder: The sparse autoencoder\n",
    "        encoder_weights: Encoder weights for selected features\n",
    "        encoder_biases: Encoder biases for selected features\n",
    "        feature_ids: List of feature IDs to analyze\n",
    "        feature_categories: Categories of the features\n",
    "        top_k: Number of top activations to return per feature\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping feature IDs to tuples of (top_indices, top_values)\n",
    "    \"\"\"\n",
    "#     _, cache = model.run_with_cache(images, names_filter=[sparse_autoencoder.cfg.hook_point])\n",
    "    recons_image_embeddings_feat_altered_list = []\n",
    "    for idx in np.array(range(sparse_autoencoder.W_dec.shape[0]))[random_feat_idxs]:\n",
    "#         print(f\"Feature: {idx} ====================\")\n",
    "        \n",
    "        steering_hook = partial(\n",
    "            steering_hook_fn_cls_only,\n",
    "            cfg=cfg,\n",
    "            sae=sparse_autoencoder,\n",
    "            steering_indices=[idx],\n",
    "            steering_strength=steering_strength,\n",
    "            mean_ablation_values = [1.0],\n",
    "            include_error=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "        recons_image_embeddings_feat_altered = model.run_with_hooks(\n",
    "            images,\n",
    "            fwd_hooks=[(\"blocks.9.hook_mlp_out\", steering_hook)],\n",
    "        )\n",
    "        recons_image_embeddings_feat_altered_list.append(recons_image_embeddings_feat_altered)\n",
    "\n",
    "    \n",
    "    # output is in clip embedding space\n",
    "    recons_image_embeddings_default = model.run_with_hooks(\n",
    "        images,\n",
    "        fwd_hooks=[(\"blocks.9.hook_mlp_out\", lambda x, hook: x)],\n",
    "    )\n",
    "    \n",
    "#     print(f\"recons_image_embeddings_default: {recons_image_embeddings_default}\")\n",
    "#     print(f\"recons_image_embeddings_default.shape: {recons_image_embeddings_default.shape}\")\n",
    "#     print(f\"recons_image_embeddings_default: {recons_image_embeddings_default.shape}\")\n",
    "\n",
    "#     print(f\"recons_image_embeddings_feat_altered: {recons_image_embeddings_feat_altered}\")\n",
    "#     print(f\"recons_image_embeddings_feat_altered.shape: {recons_image_embeddings_feat_altered.shape}\")\n",
    "\n",
    "    return recons_image_embeddings_feat_altered_list, recons_image_embeddings_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================ steering_strength: 0.0 ============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                            | 0/1562 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from collections import defaultdict\n",
    "max_samples = cfg.eval_max\n",
    "\n",
    "encoder_biases = sparse_autoencoder.b_enc#[interesting_features_indices]\n",
    "encoder_weights = sparse_autoencoder.W_enc#[:, interesting_features_indices]\n",
    "\n",
    "steering_strengths = [0.0, 5.0, 10.0, 20.0, 50.0, 150.0, 300.0, 500.0]#, -200.0, -300.0]\n",
    "\n",
    "\n",
    "steering_strength_image_results = defaultdict(dict)\n",
    "steering_strength_info = {}\n",
    "\n",
    "og_model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for steering_strength in steering_strengths:\n",
    "    print(f\"{'==============' * 2} steering_strength: {steering_strength} {'==============' * 2}\")\n",
    "    # ===== Get Steered and Default CLIP Outputs =====\n",
    "    top_k=10\n",
    "    processed_samples = 0\n",
    "    default_embeds_list = []\n",
    "    feature_steered_embeds = defaultdict(list)\n",
    "    l = 0\n",
    "    \n",
    "    # remove tqdm\n",
    "    for batch_images, _, batch_indices in tqdm(val_dataloader, total=max_samples // cfg.batch_size):\n",
    "        batch_images = batch_images.to(cfg.device)\n",
    "        batch_indices = batch_indices.to(cfg.device)\n",
    "        batch_size = batch_images.shape[0]\n",
    "\n",
    "        altered_embeds_list, default_embeds = compute_feature_activations_set_feat(\n",
    "            batch_images, model, sparse_autoencoder, encoder_weights, encoder_biases,\n",
    "            None, None, top_k, steering_strength\n",
    "        )\n",
    "        default_embeds_list.append(default_embeds)\n",
    "        for j, altered_embeds in enumerate(altered_embeds_list):\n",
    "            feature_steered_embeds[random_feat_idxs[j]].extend(altered_embeds)\n",
    "        # either label embeds or optimize to maximal token in text transformer embedding face\n",
    "        l += 1\n",
    "        if l >= 1:\n",
    "            break    \n",
    "    default_embeds = torch.cat(default_embeds_list)\n",
    "    \n",
    "    with open(\"/workspace/clip_dissect_raw.txt\", \"r\") as f:\n",
    "        larger_vocab = [line[:-1] for line in f.readlines()][:5000]\n",
    "\n",
    "\n",
    "    # ===== CLIP Embeds =====\n",
    "    # use clip vocab here and compare embeds\n",
    "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "    text = tokenizer(larger_vocab)\n",
    "    text_features = og_model.encode_text(text.cuda())\n",
    "    text_features_normed = text_features/text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "    print(f\"text_features_normed.shape: {text_features_normed.shape}\")\n",
    "    text_probs_altered_list = []\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        for key in feature_steered_embeds:\n",
    "            print(key)\n",
    "            # embeds already have L2 norm of 1\n",
    "            text_probs_altered = (100.0 * torch.stack(feature_steered_embeds[key]) @ text_features_normed.T).softmax(dim=-1)\n",
    "            text_probs_altered_list.append(text_probs_altered)\n",
    "        text_probs_default = (100.0 * default_embeds @ text_features_normed.T).softmax(dim=-1)\n",
    "\n",
    "    print(\"Label probs altered:\", text_probs_altered.shape)  # prints: [[1., 0., 0.]]\n",
    "    print(\"Label probs default:\", text_probs_default.shape)  # prints: [[1., 0., 0.]]\n",
    "    \n",
    "    \n",
    "    # ===== Logit Difference =====\n",
    "    # indexed as such in steering_strength_image_results:\n",
    "    # per steering strength\n",
    "    # per feature\n",
    "    # per image\n",
    "    \n",
    "    selected_vocab = larger_vocab\n",
    "\n",
    "    top_concept_per_feat = {}\n",
    "    top_val_per_feat = {}\n",
    "    top_diff_per_feat = {}\n",
    "    top_ratio_per_feat = {}\n",
    "    \n",
    "    # run this for sampled features over all of imagenet eval\n",
    "    for j, text_probs_altered in enumerate(text_probs_altered_list):\n",
    "        print(f\"{'============================================'*2}\\n\\nFor Feature {random_feat_idxs[j]}\")\n",
    "        print(\"actual image content:\")\n",
    "        default_vals_softmax, default_idxs_softmax = torch.topk(text_probs_default,k=10)\n",
    "        print(default_vals_softmax, \"\\n\", np.array(selected_vocab)[default_idxs_softmax.cpu()])\n",
    "\n",
    "\n",
    "        logit_diff = text_probs_altered - text_probs_default\n",
    "        logit_diff_aggregate = logit_diff.sum(dim=0)\n",
    "\n",
    "        logit_ratio = text_probs_altered/text_probs_default\n",
    "        logit_ratio_aggregate = logit_ratio.mean(dim=0)\n",
    "\n",
    "        print(f\"text_probs_altered.softmax(): {text_probs_altered.softmax(1).shape}\")\n",
    "        text_probs_altered_softmax = text_probs_altered.softmax(1)\n",
    "        vals_softmax, idxs_softmax = torch.topk(text_probs_altered_softmax,k=10)\n",
    "\n",
    "    #     print(f\"text_probs_altered.softmax(): {text_probs_altered.sum(0).softmax(0).shape}\")\n",
    "    #     text_probs_altered_softmax_agg = text_probs_altered.sum(0).softmax(0)\n",
    "    #     vals_softmax_agg, idxs_softmax_agg = torch.topk(text_probs_altered_softmax_agg,k=10)\n",
    "\n",
    "        print(f\"\\nSoftmax Over {text_probs_altered.shape[0]} Images:\\n{vals_softmax}\")\n",
    "        print(np.array(selected_vocab)[idxs_softmax.cpu()])\n",
    "        for i in range(vals_softmax.shape[0]):\n",
    "            print(vals_softmax[i], \"\\n\", np.array(selected_vocab)[idxs_softmax.cpu()][i])\n",
    "            break\n",
    "\n",
    "    #     print(f\"\\nAgg Softmax Over {text_probs_altered.shape[0]} Images:\\n{vals_softmax_agg}\")\n",
    "    #     print(np.array(selected_vocab)[idxs_softmax_agg.cpu()])\n",
    "\n",
    "        vals_agg, idxs_agg = torch.topk(logit_diff_aggregate,k=10)\n",
    "        vals_least_agg, idxs_least_agg = torch.topk(logit_diff_aggregate,k=10,largest=False)\n",
    "\n",
    "        ratios_agg, ratios_idxs_agg = torch.topk(logit_ratio_aggregate,k=10)\n",
    "        ratios_least_agg, ratios_idxs_least_agg = torch.topk(logit_ratio_aggregate,k=10,largest=False)\n",
    "\n",
    "        vals, idxs = torch.topk(logit_diff,k=5)\n",
    "        vals_least, idxs_least = torch.topk(logit_diff,k=5,largest=False)\n",
    "\n",
    "        ratios, ratios_idxs = torch.topk(logit_ratio,k=5)\n",
    "        ratios_least, ratios_idxs_least = torch.topk(logit_ratio,k=5,largest=False)\n",
    "\n",
    "        # random_feat_idxs[j] is the index of the feature\n",
    "        for img_idx in range(batch_images.shape[0]):\n",
    "            if random_feat_idxs[j] not in steering_strength_image_results[str(steering_strength)].keys():\n",
    "                steering_strength_image_results[str(steering_strength)][random_feat_idxs[j].copy()] = []\n",
    "            # entries are torch.topk(k=10) results\n",
    "            steering_strength_image_results[str(steering_strength)][random_feat_idxs[j]].append((np.array(selected_vocab, copy=True)[idxs_softmax.cpu()][img_idx], torch.clone(vals_softmax[img_idx])))\n",
    "        \n",
    "        # per image\n",
    "        top_concept_per_feat[random_feat_idxs[j]] = np.array(selected_vocab)[idxs_softmax.cpu()][0][0]\n",
    "        top_val_per_feat[random_feat_idxs[j]] = vals_softmax[0][0]\n",
    "        \n",
    "        # aggregate\n",
    "        top_diff_per_feat[random_feat_idxs[j]] = vals_agg[0]\n",
    "        top_ratio_per_feat[random_feat_idxs[j]] = ratios_agg[0]\n",
    "\n",
    "\n",
    "        print(f\"\\nMost Changed, by Absolute Diff Over {logit_diff.shape[0]} Images:\\n{vals_agg}\")\n",
    "        print(np.array(selected_vocab)[idxs_agg.cpu()])\n",
    "#         print(vals_least_agg)\n",
    "#         print(np.array(selected_vocab)[idxs_least_agg.cpu()])\n",
    "\n",
    "        print(f\"\\nMost Changed, by Ratio Over {logit_diff.shape[0]} Images:\")\n",
    "        print(ratios_agg)\n",
    "        print(np.array(selected_vocab)[ratios_idxs_agg.cpu()])\n",
    "#         print(ratios_least_agg)\n",
    "#         print(np.array(selected_vocab)[ratios_idxs_least_agg.cpu()])\n",
    "    \n",
    "    steering_strength_info[steering_strength] = (top_concept_per_feat,top_val_per_feat,top_ratio_per_feat,top_diff_per_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_strength_image_results.keys(), steering_strength_image_results[str(steering_strength)].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "# 0: python, 3: bowl, 4: bed, 6: guinea\n",
    "image_idx = 3\n",
    "feat_num = 1657\n",
    "\n",
    "# to iterate over many features:\n",
    "# for feat_num in steering_strength_image_results[str(steering_strength)].keys():\n",
    "\n",
    "print(f\"=====================\\nfeat_num: {feat_num}\")\n",
    "feat_num_concept_arr = []\n",
    "feat_num_prob_arr = []\n",
    "for dict_key in steering_strengths:\n",
    "    # image, tuple position, idx of top-k\n",
    "    # modify this to do top-k at some point\n",
    "    print(str(dict_key), steering_strength_image_results[str(dict_key)][feat_num][image_idx][0][0])\n",
    "    feat_num_concept_arr.append((dict_key, steering_strength_image_results[str(dict_key)][feat_num][image_idx][0][0]))\n",
    "    print(str(dict_key), steering_strength_image_results[str(dict_key)][feat_num][image_idx][1][0].item())\n",
    "    feat_num_prob_arr.append((dict_key, steering_strength_image_results[str(dict_key)][feat_num][image_idx][1][0].item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "x = [tup[0] for tup in sorted(feat_num_concept_arr)]\n",
    "y1 = [tup[1] for tup in sorted(feat_num_concept_arr)]\n",
    "y2 = [tup[1] for tup in sorted(feat_num_prob_arr)]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each line with different colors and markers\n",
    "plt.plot(np.array(x), y2, 'rs-', label='Prob, Label at Strength')  # Red line with squares\n",
    "\n",
    "# Label each point\n",
    "for i in range(len(x)):\n",
    "    # Labels for series\n",
    "    plt.annotate(f'({y2[i]:01f}, {y1[i]})', \n",
    "                (x[i], y2[i]), \n",
    "                textcoords=\"offset points\", \n",
    "                xytext=(0,-15),\n",
    "                ha='center')\n",
    "    \n",
    "# Customize the plot\n",
    "plt.xlabel('Feature Steering Strength (feat = Strength)')\n",
    "plt.ylabel('Probability of TPL, Top Predicted Label')\n",
    "plt.title(f'Most Likely Class by Feature Steering Strength, Feature {feat_num}\\n Label at 0.0: {steering_strength_image_results[str(0.0)][feat_num][image_idx][0][0]}. Label at max steered val ({str(max(steering_strengths))}): {steering_strength_image_results[str(max(steering_strengths))][feat_num][image_idx][0][0]}.')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout to prevent label overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# plt.savefig(\"test.svg\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for feat_num in steering_strength_info[steering_strength][0].keys():\n",
    "#     print(f\"=====================\\nfeat_num: {feat_num}\")\n",
    "#     feat_num_concept_arr = []\n",
    "#     feat_num_prob_arr = []\n",
    "#     feat_num_ratio_arr = []\n",
    "#     for key in steering_strength_info:\n",
    "#         print(key, steering_strength_info[key][0][feat_num])\n",
    "#         feat_num_concept_arr.append((key, steering_strength_info[key][0][feat_num]))\n",
    "#         print(key, steering_strength_info[key][1][feat_num])\n",
    "#         feat_num_prob_arr.append((key, steering_strength_info[key][1][feat_num].item()))\n",
    "#         print(key, steering_strength_info[key][2][feat_num])\n",
    "#         feat_num_ratio_arr.append((key, steering_strength_info[key][2][feat_num].item()))\n",
    "#     i += 1\n",
    "#     if i > 7:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prev Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_steered_embeds[random_feat_idxs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_embeds.shape\n",
    "len(default_embeds_list)\n",
    "default_embeds = torch.cat(default_embeds_list)\n",
    "default_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(altered_embeds_list), altered_embeds_list[0].shape, default_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "og_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/clip_dissect_raw.txt\", \"r\") as f:\n",
    "    larger_vocab = [line[:-1] for line in f.readlines()][:5000]\n",
    "\n",
    "# with open(\"/workspace/better_img_desc.txt\", \"r\") as f:\n",
    "#     larger_vocab = [line[:-1] for line in f.readlines()][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use clip vocab here and compare embeds\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "text = tokenizer(larger_vocab)\n",
    "text_features = og_model.encode_text(text.cuda())\n",
    "text_features_normed = text_features/text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "print(f\"text_features_normed.shape: {text_features_normed.shape}\")\n",
    "text_probs_altered_list = []\n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    for key in feature_steered_embeds:\n",
    "        print(key)\n",
    "        # embeds already have L2 norm of 1\n",
    "        text_probs_altered = (100.0 * torch.stack(feature_steered_embeds[key]) @ text_features_normed.T).softmax(dim=-1)\n",
    "        text_probs_altered_list.append(text_probs_altered)\n",
    "    text_probs_default = (100.0 * default_embeds @ text_features_normed.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs altered:\", text_probs_altered.shape)  # prints: [[1., 0., 0.]]\n",
    "print(\"Label probs default:\", text_probs_default.shape)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summed Logit Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# subtract from default, label, and print trends\n",
    "text_probs_altered.shape\n",
    "\n",
    "# selected_vocab = all_imagenet_class_names\n",
    "selected_vocab = larger_vocab\n",
    "\n",
    "top_concept_per_feat = {}\n",
    "top_val_per_feat = {}\n",
    "top_diff_per_feat = {}\n",
    "top_ratio_per_feat = {}\n",
    "# run this for sampled features over all of imagenet eval\n",
    "for j, text_probs_altered in enumerate(text_probs_altered_list):\n",
    "    print(f\"{'============================================'*2}\\n\\nFor Feature {random_feat_idxs[j]}\")\n",
    "    print(\"actual image content:\")\n",
    "    default_vals_softmax, default_idxs_softmax = torch.topk(text_probs_default,k=10)\n",
    "    print(default_vals_softmax, \"\\n\", np.array(selected_vocab)[default_idxs_softmax.cpu()])\n",
    "    \n",
    "    \n",
    "    logit_diff = text_probs_altered - text_probs_default\n",
    "    logit_diff_aggregate = logit_diff.sum(dim=0)\n",
    "    \n",
    "    logit_ratio = text_probs_altered/text_probs_default\n",
    "    logit_ratio_aggregate = logit_ratio.mean(dim=0)\n",
    "    \n",
    "    print(f\"text_probs_altered.softmax(): {text_probs_altered.softmax(1).shape}\")\n",
    "    text_probs_altered_softmax = text_probs_altered.softmax(1)\n",
    "    vals_softmax, idxs_softmax = torch.topk(text_probs_altered_softmax,k=10)\n",
    "    \n",
    "#     print(f\"text_probs_altered.softmax(): {text_probs_altered.sum(0).softmax(0).shape}\")\n",
    "#     text_probs_altered_softmax_agg = text_probs_altered.sum(0).softmax(0)\n",
    "#     vals_softmax_agg, idxs_softmax_agg = torch.topk(text_probs_altered_softmax_agg,k=10)\n",
    "    \n",
    "    print(f\"\\nSoftmax Over {text_probs_altered.shape[0]} Images:\\n{vals_softmax}\")\n",
    "    print(np.array(selected_vocab)[idxs_softmax.cpu()])\n",
    "    for i in range(vals_softmax.shape[0]):\n",
    "        print(vals_softmax[i], \"\\n\", np.array(selected_vocab)[idxs_softmax.cpu()][i])\n",
    "        break\n",
    "        \n",
    "#     print(f\"\\nAgg Softmax Over {text_probs_altered.shape[0]} Images:\\n{vals_softmax_agg}\")\n",
    "#     print(np.array(selected_vocab)[idxs_softmax_agg.cpu()])\n",
    "    \n",
    "    vals_agg, idxs_agg = torch.topk(logit_diff_aggregate,k=10)\n",
    "    vals_least_agg, idxs_least_agg = torch.topk(logit_diff_aggregate,k=10,largest=False)\n",
    "    \n",
    "    ratios_agg, ratios_idxs_agg = torch.topk(logit_ratio_aggregate,k=10)\n",
    "    ratios_least_agg, ratios_idxs_least_agg = torch.topk(logit_ratio_aggregate,k=10,largest=False)\n",
    "    \n",
    "    vals, idxs = torch.topk(logit_diff,k=5)\n",
    "    vals_least, idxs_least = torch.topk(logit_diff,k=5,largest=False)\n",
    "    \n",
    "    ratios, ratios_idxs = torch.topk(logit_ratio,k=5)\n",
    "    ratios_least, ratios_idxs_least = torch.topk(logit_ratio,k=5,largest=False)\n",
    "    \n",
    "    top_concept_per_feat[random_feat_idxs[j]] = np.array(selected_vocab)[idxs_softmax.cpu()][0][0]\n",
    "    top_val_per_feat[random_feat_idxs[j]] = vals_softmax[0][0]\n",
    "    top_diff_per_feat[random_feat_idxs[j]] = vals_agg[0]\n",
    "    top_ratio_per_feat[random_feat_idxs[j]] = ratios_agg[0]\n",
    "    \n",
    "    \n",
    "    print(f\"\\nMost Changed, by Absolute Diff Over {logit_diff.shape[0]} Images:\\n{vals_agg}\")\n",
    "    print(np.array(selected_vocab)[idxs_agg.cpu()])\n",
    "    print(vals_least_agg)\n",
    "    print(np.array(selected_vocab)[idxs_least_agg.cpu()])\n",
    "    \n",
    "    print(f\"\\nMost Changed, by Ratio Over {logit_diff.shape[0]} Images:\")\n",
    "    print(ratios_agg)\n",
    "    print(np.array(selected_vocab)[ratios_idxs_agg.cpu()])\n",
    "    print(vals_least_agg)\n",
    "    print(np.array(selected_vocab)[ratios_idxs_least_agg.cpu()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_concept_per_feat,top_val_per_feat,top_ratio_per_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_strength_info = {}\n",
    "steering_strength_info[steering_strength] = (top_concept_per_feat,top_val_per_feat,top_ratio_per_feat,top_diff_per_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_strength_info[steering_strength][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steering_strength_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for feat_num in steering_strength_info[steering_strength][0].keys():\n",
    "    print(f\"=====================\\nfeat_num: {feat_num}\")\n",
    "    feat_num_concept_arr = []\n",
    "    feat_num_prob_arr = []\n",
    "    feat_num_ratio_arr = []\n",
    "    for key in steering_strength_info:\n",
    "        print(key, steering_strength_info[key][0][feat_num])\n",
    "        feat_num_concept_arr.append((key, steering_strength_info[key][0][feat_num]))\n",
    "        print(key, steering_strength_info[key][1][feat_num])\n",
    "        feat_num_prob_arr.append((key, steering_strength_info[key][1][feat_num].item()))\n",
    "        print(key, steering_strength_info[key][2][feat_num])\n",
    "        feat_num_ratio_arr.append((key, steering_strength_info[key][2][feat_num].item()))\n",
    "    i += 1\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(feat_num_concept_arr),sorted(feat_num_prob_arr),sorted(feat_num_ratio_arr),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "x = [tup[0] for tup in sorted(feat_num_concept_arr)]\n",
    "y1 = [tup[1] for tup in sorted(feat_num_concept_arr)]\n",
    "y2 = [tup[1] for tup in sorted(feat_num_prob_arr)]\n",
    "# y3 = [tup[1] for tup in sorted(feat_num_ratio_arr)]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each line with different colors and markers\n",
    "# plt.plot(x, y1, 'bo-', label='Series 1')  # Blue line with circles\n",
    "plt.plot(np.array(x), y2, 'rs-', label='Series 2')  # Red line with squares\n",
    "# plt.plot(x, y3, 'gd-', label='Series 3')  # Green line with diamonds\n",
    "\n",
    "# Label each point for all three series\n",
    "for i in range(len(x)):\n",
    "#     # Labels for series 1\n",
    "#     plt.annotate(f'({x[i]}, {y1[i]})', \n",
    "#                 (x[i], y1[i]), \n",
    "#                 textcoords=\"offset points\", \n",
    "#                 xytext=(0,10),\n",
    "#                 ha='center')\n",
    "    \n",
    "    # Labels for series 2\n",
    "    plt.annotate(f'({y2[i]:01f}, {y1[i]})', \n",
    "                (x[i], y2[i]), \n",
    "                textcoords=\"offset points\", \n",
    "                xytext=(0,-15),\n",
    "                ha='center')\n",
    "    \n",
    "#     # Labels for series 3\n",
    "#     plt.annotate(f'({x[i]}, {y3[i]})', \n",
    "#                 (x[i], y3[i]), \n",
    "#                 textcoords=\"offset points\", \n",
    "#                 xytext=(0,10),\n",
    "#                 ha='center')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title(f'Most Likely Class by Feature Steering Strength, Feature {feat_num}\\n Label at 0.0: {steering_strength_info[0.0][0][feat_num]}. Label at max steered val: {steering_strength_info[max(list(steering_strength_info.keys()))][0][feat_num]}.')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout to prevent label overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enc/Dec Clustering/Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_weights_for_math = sparse_autoencoder.W_enc\n",
    "decoder_weights_for_math = sparse_autoencoder.W_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_from_feat_0 = encoder_weights_for_math[0] - encoder_weights_for_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_from_feat_0_normalized = encoder_weights_for_math[0]/encoder_weights_for_math[0].norm(p=2) - encoder_weights_for_math/encoder_weights_for_math.norm(p=2,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists_from_feat_0.norm(p=2, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(bins[:-1], bins)\n",
    "plt.hist(dists_from_feat_0.norm(p=2, dim=0).cpu(), density=True, bins=1000, histtype='step')  # density=False would make counts\n",
    "plt.title('Encoder Dist from feat 0')\n",
    "plt.ylabel('L2 Distance')\n",
    "plt.xlabel('Density (of ~50k feats)');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(dists_from_feat_0.norm(p=2, dim=0),k=10,largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_dists_from_feat_0 = decoder_weights_for_math[0]/decoder_weights_for_math[0].norm(p=2) - decoder_weights_for_math/decoder_weights_for_math.norm(p=2)\n",
    "dec_dists_from_feat_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(bins[:-1], bins)\n",
    "plt.hist(dec_dists_from_feat_0.T.norm(p=2, dim=0).cpu(), density=True, bins=1000, histtype='step')  # density=False would make counts\n",
    "plt.title('Decoder Dist from feat 0')\n",
    "plt.ylabel('L2 Distance')\n",
    "plt.xlabel('Density (of ~50k feats)');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(dec_dists_from_feat_0.T.norm(p=2, dim=0),k=10,largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_dists_from_feat_0.T.norm(p=2, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
