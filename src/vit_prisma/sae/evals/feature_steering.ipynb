{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating your SAE\n",
    "\n",
    "Code based off Rob Graham's ([themachinefan](https://github.com/themachinefan)) SAE evaluation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/ViT-Prisma/src/vit_prisma/sae/evals'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_per_buffer (millions): 0.032\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 158691\n",
      "Total training images: 13000000\n",
      "Total wandb updates: 15869\n",
      "Expansion factor: 16\n",
      "n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 158 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Gradient clipping with max_norm=1.0\n",
      "Using SAE initialization method: encoder_transpose_decoder\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from vit_prisma.sae.config import VisionModelSAERunnerConfig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig(VisionModelSAERunnerConfig):\n",
    "    sae_path: str = '/workspace/sae_checkpoints/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-11-hook_resid_post-l1-0.0001/n_images_2600058.pt'\n",
    "    model_name: str = \"open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "    model_type: str =  \"clip\"\n",
    "    patch_size: str = 32\n",
    "\n",
    "    dataset_path = \"/workspace\"\n",
    "    dataset_train_path: str = \"/workspace/ILSVRC/Data/CLS-LOC/train\"\n",
    "    dataset_val_path: str = \"/workspace/ILSVRC/Data/CLS-LOC/val\"\n",
    "\n",
    "    verbose: bool = True\n",
    "\n",
    "    device: bool = 'cuda'\n",
    "\n",
    "    eval_max: int = 50_000 # 50_000\n",
    "    batch_size: int = 32\n",
    "\n",
    "    # make the max image output folder a subfolder of the sae path\n",
    "\n",
    "\n",
    "    @property\n",
    "    def max_image_output_folder(self) -> str:\n",
    "        # Get the base directory of sae_checkpoints\n",
    "        sae_base_dir = os.path.dirname(os.path.dirname(self.sae_path))\n",
    "        \n",
    "        # Get the name of the original SAE checkpoint folder\n",
    "        sae_folder_name = os.path.basename(os.path.dirname(self.sae_path))\n",
    "        \n",
    "        # Create a new folder path in sae_checkpoints/images with the original name\n",
    "        output_folder = os.path.join(sae_base_dir, 'max_images', sae_folder_name)\n",
    "        output_folder = os.path.join(output_folder, f\"layer_{self.hook_point_layer}\") # Add layer number\n",
    "\n",
    "        \n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        return output_folder\n",
    "\n",
    "cfg = EvalConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7ef6b8585e40>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "Official model name open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "Converting OpenCLIP weights\n",
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "visual projection shape torch.Size([768, 512])\n",
      "Setting center_writing_weights to False for OpenCLIP\n",
      "Setting fold_ln to False for OpenCLIP\n",
      "Loaded pretrained model open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from vit_prisma.models.base_vit import HookedViT\n",
    "\n",
    "model_name = \"open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "model = HookedViT.from_pretrained(model_name, is_timm=False, is_clip=True).to(cfg.device)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import vit_prisma\n",
    "# importlib.reload(vit_prisma.dataloaders.imagenet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation data length: 50000\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "import open_clip\n",
    "from vit_prisma.utils.data_utils.imagenet_utils import setup_imagenet_paths\n",
    "from vit_prisma.dataloaders.imagenet_dataset import get_imagenet_transforms_clip, ImageNetValidationDataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "og_model_name = \"hf-hub:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "og_model, _, preproc = open_clip.create_model_and_transforms(og_model_name)\n",
    "processor = preproc\n",
    "\n",
    "size=224\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                     std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "    \n",
    "imagenet_paths = setup_imagenet_paths(cfg.dataset_path)\n",
    "imagenet_paths[\"train\"] = \"/workspace/ILSVRC/Data/CLS-LOC/train\"\n",
    "imagenet_paths[\"val\"] = \"/workspace/ILSVRC/Data/CLS-LOC/val\"\n",
    "imagenet_paths[\"val_labels\"] = \"/workspace/LOC_val_solution.csv\"\n",
    "imagenet_paths[\"label_strings\"] = \"/workspace/LOC_synset_mapping.txt\"\n",
    "print()\n",
    "train_data = torchvision.datasets.ImageFolder(cfg.dataset_train_path, transform=data_transforms)\n",
    "val_data = ImageNetValidationDataset(cfg.dataset_val_path, \n",
    "                                imagenet_paths['label_strings'], \n",
    "                                imagenet_paths['val_labels'], \n",
    "                                data_transforms,\n",
    "                                return_index=True,\n",
    ")\n",
    "val_data_visualize = ImageNetValidationDataset(cfg.dataset_val_path, \n",
    "                                imagenet_paths['label_strings'], \n",
    "                                imagenet_paths['val_labels'],\n",
    "                                torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),]), return_index=True)\n",
    "\n",
    "print(f\"Validation data length: {len(val_data)}\") if cfg.verbose else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.sae.training.activations_store import VisionActivationsStore\n",
    "# import dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# activations_loader = VisionActivationsStore(cfg, model, train_data, eval_dataset=val_data)\n",
    "val_dataloader = DataLoader(val_data, batch_size=cfg.batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained SAE to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_activation_fn received: activation_fn=relu, kwargs={}\n",
      "n_tokens_per_buffer (millions): 0.032\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 158691\n",
      "Total training images: 13000000\n",
      "Total wandb updates: 1586\n",
      "Expansion factor: 64\n",
      "n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 158 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Gradient clipping with max_norm=1.0\n",
      "Using SAE initialization method: encoder_transpose_decoder\n",
      "get_activation_fn received: activation_fn=relu, kwargs={}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseAutoencoder(\n",
       "  (hook_sae_in): HookPoint()\n",
       "  (hook_hidden_pre): HookPoint()\n",
       "  (hook_hidden_post): HookPoint()\n",
       "  (hook_sae_out): HookPoint()\n",
       "  (activation_fn): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vit_prisma.sae.sae import SparseAutoencoder\n",
    "sparse_autoencoder = SparseAutoencoder(cfg).load_from_pretrained(\"/workspace/sae_checkpoints/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-11-hook_resid_post-l1-0.0001/n_images_2600058.pt\")\n",
    "sparse_autoencoder.to(cfg.device)\n",
    "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Labeling AutoInterp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_imagenet_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.dataloaders.imagenet_dataset import get_imagenet_index_to_name\n",
    "ind_to_name = get_imagenet_index_to_name()\n",
    "\n",
    "all_imagenet_class_names = []\n",
    "for i in range(len(ind_to_name)):\n",
    "    all_imagenet_class_names.append(ind_to_name[str(i)][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/sae_checkpoints/max_images/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-11-hook_resid_post-l1-0.0001/layer_9'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.max_image_output_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_replacement_hook_curry(feat_idx: int = 0, feat_activ: float = 1.0):\n",
    "    def standard_replacement_hook(activations: torch.Tensor, hook):\n",
    "        activations = sparse_autoencoder.forward(activations)[0].to(activations.dtype)\n",
    "        feature_acts = sparse_autoencoder.encode_standard(activations)\n",
    "\n",
    "        # in all batches and patches, set feature w idx idx to 0\n",
    "        print(f\"feature_acts[:,:,idx].shape: {feature_acts[:,:,feat_idx].shape}\")\n",
    "        print(f\"feat activ: {feature_acts[:,:,feat_idx]}\")\n",
    "        feature_acts[:,:,feat_idx] *= feat_activ\n",
    "        print(f\"feat activ: {feature_acts[:,:,feat_idx]}\")\n",
    "        print(f\"feat activ: {feature_acts.shape}\")\n",
    "        print(f\"feat activ: {feature_acts}\")\n",
    "        print(\"feature_acts[:,:,idx].sum(): (should be batch size x len seq x feat val)\", feature_acts[:,:,feat_idx].sum())\n",
    "        sae_out = sparse_autoencoder.hook_sae_out(\n",
    "            einops.einsum(\n",
    "                feature_acts,\n",
    "                sparse_autoencoder.W_dec,\n",
    "                \"... d_sae, d_sae d_in -> ... d_in\",\n",
    "            )\n",
    "            + sparse_autoencoder.b_dec\n",
    "        )\n",
    "        \n",
    "        print(f\"sae_out.shape: {sae_out.shape}\")\n",
    "        print(f\"sae_out: {sae_out}\")\n",
    "\n",
    "        # allows normalization. Possibly identity if no normalization\n",
    "        sae_out = sparse_autoencoder.run_time_activation_norm_fn_out(sae_out)\n",
    "        return sae_out\n",
    "    return standard_replacement_hook\n",
    "\n",
    "\n",
    "def steering_hook_fn(\n",
    "    activations, cfg, hook, sae, steering_indices, steering_strength=1.0, mean_ablation_values=None, include_error=False\n",
    "\n",
    "):\n",
    "    sae.to(activations.device)\n",
    "\n",
    "\n",
    "    sae_input = activations.clone()\n",
    "    sae_output, feature_activations, *data = sae(sae_input)\n",
    "    \n",
    "    steered_feature_activations = feature_activations.clone()\n",
    "    \n",
    "    steered_feature_activations[:, :, steering_indices] = steering_strength\n",
    "\n",
    "    steered_sae_out = einops.einsum(\n",
    "                steered_feature_activations,\n",
    "                sae.W_dec,\n",
    "                \"... d_sae, d_sae d_in -> ... d_in\",\n",
    "            ) + sae.b_dec\n",
    "\n",
    "    steered_sae_out = sae.run_time_activation_norm_fn_out(steered_sae_out)\n",
    "    \n",
    "    print(steered_sae_out.shape)\n",
    "    print(steered_sae_out.shape)\n",
    "    print(f\"steering norm: {(steered_sae_out - sae_output).norm()}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    if include_error:\n",
    "        error = sae_input - sae_output\n",
    "        print(f\"error.norm(): {error.norm()}\")\n",
    "        return steered_sae_out + error\n",
    "    return steered_sae_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_feat_idxs = np.random.randint(0, high=3000, size=(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given feature, set it high/low on maxim activ. imgs and high/low on non-activ images\n",
    "# hook SAE and replace desired feature with 0 or 1 \n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_feature_activations_set_feat(\n",
    "    images: torch.Tensor,\n",
    "    model: torch.nn.Module,\n",
    "    sparse_autoencoder: torch.nn.Module,\n",
    "    encoder_weights: torch.Tensor,\n",
    "    encoder_biases: torch.Tensor,\n",
    "    feature_ids: List[int],\n",
    "    feature_categories: List[str],\n",
    "    top_k: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the highest activating tokens for given features in a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        images: Input images\n",
    "        model: The main model\n",
    "        sparse_autoencoder: The sparse autoencoder\n",
    "        encoder_weights: Encoder weights for selected features\n",
    "        encoder_biases: Encoder biases for selected features\n",
    "        feature_ids: List of feature IDs to analyze\n",
    "        feature_categories: Categories of the features\n",
    "        top_k: Number of top activations to return per feature\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping feature IDs to tuples of (top_indices, top_values)\n",
    "    \"\"\"\n",
    "    _, cache = model.run_with_cache(images, names_filter=[sparse_autoencoder.cfg.hook_point])\n",
    "#     recons_image_embeddings_feat_altered = model.run_with_hooks(\n",
    "#         images,\n",
    "#         fwd_hooks=[(\"blocks.9.hook_mlp_out\", standard_replacement_hook)],\n",
    "#     )\n",
    "    recons_image_embeddings_feat_altered_list = []\n",
    "    for idx in np.array(range(sparse_autoencoder.W_dec.shape[0]))[random_feat_idxs]:\n",
    "        print(f\"Feature: {idx} ====================\")\n",
    "        \n",
    "        steering_hook = partial(\n",
    "            steering_hook_fn,\n",
    "            cfg=cfg,\n",
    "            sae=sparse_autoencoder,\n",
    "            steering_indices=[idx],\n",
    "            steering_strength=50.0,\n",
    "            mean_ablation_values = [1.0],\n",
    "            include_error=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "        recons_image_embeddings_feat_altered = model.run_with_hooks(\n",
    "            images,\n",
    "#             fwd_hooks=[(\"blocks.9.hook_mlp_out\", standard_replacement_hook_curry(idx, 10.0))],\n",
    "            fwd_hooks=[(\"blocks.9.hook_mlp_out\", steering_hook)],\n",
    "        )\n",
    "        recons_image_embeddings_feat_altered_list.append(recons_image_embeddings_feat_altered)\n",
    "\n",
    "    \n",
    "    # output is in clip embedding space\n",
    "    recons_image_embeddings_default = model.run_with_hooks(\n",
    "        images,\n",
    "        fwd_hooks=[(\"blocks.9.hook_mlp_out\", lambda x, hook: x)],\n",
    "    )\n",
    "    \n",
    "    print(f\"recons_image_embeddings_default: {recons_image_embeddings_default}\")\n",
    "    print(f\"recons_image_embeddings_default.shape: {recons_image_embeddings_default.shape}\")\n",
    "    print(f\"recons_image_embeddings_default: {recons_image_embeddings_default.shape}\")\n",
    "\n",
    "    print(f\"recons_image_embeddings_feat_altered: {recons_image_embeddings_feat_altered}\")\n",
    "    print(f\"recons_image_embeddings_feat_altered.shape: {recons_image_embeddings_feat_altered.shape}\")\n",
    "\n",
    "    return recons_image_embeddings_feat_altered_list, recons_image_embeddings_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                            | 0/1562 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 919 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.88671875\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 258 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1266 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 995 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1706 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 996 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 391 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.000244140625\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2750 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2807 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1757 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 747 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.000244140625\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2681 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 912 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 164 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.000244140625\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2021 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1331 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1763 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 146 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2769 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.989013671875\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 929 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 949 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.962890625\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 283 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1780 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2180 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                    | 1/1562 [00:01<49:29,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1323 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 561.3306884765625\n",
      "recons_image_embeddings_default: tensor([[ 0.0352,  0.0083, -0.0740,  ..., -0.0311,  0.0275,  0.0019],\n",
      "        [-0.0101, -0.0539, -0.0622,  ...,  0.0199, -0.0555, -0.0743],\n",
      "        [-0.0206,  0.0059, -0.0366,  ..., -0.0307,  0.0756, -0.0016],\n",
      "        ...,\n",
      "        [ 0.0099, -0.0045, -0.0059,  ..., -0.0521,  0.0647, -0.0225],\n",
      "        [-0.0422,  0.0518, -0.0482,  ...,  0.0098,  0.0418,  0.0290],\n",
      "        [-0.0411, -0.0590,  0.0014,  ..., -0.0432, -0.0089, -0.0449]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[ 0.0528, -0.0427, -0.0113,  ..., -0.0205,  0.0527,  0.0599],\n",
      "        [ 0.0414, -0.0500, -0.0162,  ..., -0.0282,  0.0233,  0.0511],\n",
      "        [ 0.0241, -0.0562, -0.0119,  ..., -0.0366,  0.0649,  0.0651],\n",
      "        ...,\n",
      "        [ 0.0437, -0.0352,  0.0116,  ..., -0.0349,  0.0624,  0.0464],\n",
      "        [ 0.0219, -0.0344, -0.0135,  ..., -0.0146,  0.0478,  0.0651],\n",
      "        [ 0.0276, -0.0688,  0.0048,  ..., -0.0300,  0.0426,  0.0417]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 919 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.9759521484375\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 258 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1266 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 995 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1706 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 996 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 391 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.000244140625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2750 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2807 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1757 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 747 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2681 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 912 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 164 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.000244140625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2021 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1331 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1763 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 146 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2769 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.950927734375\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 929 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 949 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.9879150390625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 283 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1780 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2180 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1323 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 560.2279052734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                                                   | 2/1562 [00:03<43:04,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recons_image_embeddings_default: tensor([[ 0.0146, -0.0148, -0.0460,  ...,  0.0118,  0.0082,  0.0083],\n",
      "        [-0.0018,  0.0212, -0.0113,  ...,  0.0519, -0.0585, -0.0361],\n",
      "        [-0.0171, -0.0393, -0.0432,  ...,  0.0160,  0.0028,  0.0136],\n",
      "        ...,\n",
      "        [-0.0224, -0.0082, -0.0361,  ..., -0.0352,  0.0784,  0.0265],\n",
      "        [-0.0062,  0.0247, -0.0572,  ...,  0.0121, -0.0083,  0.0222],\n",
      "        [-0.0130,  0.0321, -0.0363,  ...,  0.0437,  0.0279, -0.0109]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[ 0.0335, -0.0481, -0.0024,  ..., -0.0238,  0.0495,  0.0592],\n",
      "        [ 0.0450, -0.0409,  0.0037,  ..., -0.0079,  0.0234,  0.0526],\n",
      "        [ 0.0535, -0.0529, -0.0222,  ..., -0.0185,  0.0463,  0.0485],\n",
      "        ...,\n",
      "        [ 0.0366, -0.0527, -0.0098,  ..., -0.0252,  0.0559,  0.0742],\n",
      "        [ 0.0292, -0.0445, -0.0072,  ..., -0.0206,  0.0475,  0.0647],\n",
      "        [ 0.0336, -0.0450, -0.0062,  ..., -0.0130,  0.0526,  0.0605]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 919 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.994873046875\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 258 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1266 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 995 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1706 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 996 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 391 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.000244140625\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2750 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2807 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1757 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 747 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2681 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 912 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 164 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.000244140625\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2021 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1331 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1763 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 146 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2769 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.8021240234375\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 929 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 949 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.9930419921875\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 283 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1780 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                                                   | 3/1562 [00:04<40:56,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2180 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1323 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 559.0997314453125\n",
      "recons_image_embeddings_default: tensor([[ 0.0294,  0.0383,  0.0048,  ..., -0.0036,  0.0256,  0.0279],\n",
      "        [ 0.0004,  0.0353, -0.0868,  ..., -0.0146,  0.0002,  0.0059],\n",
      "        [ 0.0709, -0.0185, -0.0175,  ...,  0.0050,  0.0293,  0.0257],\n",
      "        ...,\n",
      "        [-0.0168, -0.0003, -0.0274,  ..., -0.0302,  0.0601, -0.0477],\n",
      "        [ 0.0075,  0.0213, -0.0235,  ..., -0.0346,  0.0216,  0.0487],\n",
      "        [ 0.0059, -0.0119, -0.0019,  ...,  0.0249, -0.0424,  0.0157]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[ 4.6092e-02, -4.3605e-02,  1.1148e-02,  ..., -1.6019e-02,\n",
      "          5.2664e-02,  5.7953e-02],\n",
      "        [ 4.1208e-02, -3.6856e-02, -3.0512e-02,  ..., -2.3538e-02,\n",
      "          3.8977e-02,  5.8172e-02],\n",
      "        [ 4.8620e-02, -4.6181e-02, -1.3117e-02,  ..., -1.1668e-02,\n",
      "          5.4477e-02,  6.9493e-02],\n",
      "        ...,\n",
      "        [ 4.2759e-02, -5.8006e-02, -9.4116e-03,  ...,  5.5850e-03,\n",
      "          5.9993e-02,  4.3127e-02],\n",
      "        [ 4.2409e-02, -3.8732e-02,  1.1817e-02,  ..., -3.3076e-02,\n",
      "          4.9963e-02,  7.3160e-02],\n",
      "        [ 4.7143e-02, -5.8313e-02, -1.8104e-05,  ..., -8.9693e-03,\n",
      "          4.2517e-02,  5.9426e-02]], device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 919 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.9725341796875\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 258 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1266 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.99462890625\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 995 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1706 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 996 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 391 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.000244140625\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2750 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2807 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1757 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 747 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2681 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 912 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 164 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.000244140625\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2021 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1331 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1763 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 146 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2769 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.9447021484375\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 929 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 949 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.9427490234375\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 283 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1780 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2180 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1323 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                                                   | 4/1562 [00:06<39:56,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 564.427001953125\n",
      "recons_image_embeddings_default: tensor([[ 0.0495,  0.0061, -0.0375,  ..., -0.0073, -0.0049,  0.0464],\n",
      "        [ 0.0656,  0.0185, -0.0169,  ..., -0.0542,  0.0806,  0.0280],\n",
      "        [ 0.0439,  0.0136,  0.0194,  ..., -0.0279,  0.0640, -0.0370],\n",
      "        ...,\n",
      "        [ 0.0259,  0.0402, -0.0065,  ..., -0.0289,  0.0129,  0.0450],\n",
      "        [ 0.0245,  0.0248, -0.0074,  ..., -0.0344,  0.0273, -0.0038],\n",
      "        [ 0.0167,  0.0346, -0.0975,  ...,  0.0074,  0.0849, -0.0346]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[ 0.0520, -0.0430, -0.0186,  ..., -0.0307,  0.0407,  0.0708],\n",
      "        [ 0.0430, -0.0385, -0.0082,  ..., -0.0425,  0.0528,  0.0652],\n",
      "        [ 0.0313, -0.0452,  0.0096,  ..., -0.0211,  0.0687,  0.0537],\n",
      "        ...,\n",
      "        [ 0.0332, -0.0364, -0.0071,  ..., -0.0378,  0.0446,  0.0665],\n",
      "        [ 0.0471, -0.0416, -0.0234,  ..., -0.0132,  0.0539,  0.0634],\n",
      "        [ 0.0365, -0.0465, -0.0303,  ..., -0.0174,  0.0560,  0.0617]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 919 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.9444580078125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 258 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1266 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 995 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1706 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 996 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 391 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2750 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2807 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1757 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 747 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2681 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 912 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 164 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.000244140625\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2021 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1331 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1763 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 146 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2769 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.973388671875\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 929 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 949 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 1999.92578125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 283 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1780 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2180 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1323 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 2000.0001220703125\n",
      "error.norm(): 563.367431640625\n",
      "recons_image_embeddings_default: tensor([[-0.0659, -0.0776, -0.0139,  ..., -0.0386,  0.0279,  0.0018],\n",
      "        [ 0.0148, -0.0243,  0.0026,  ..., -0.0218,  0.0321,  0.0376],\n",
      "        [-0.0408, -0.0001, -0.0266,  ..., -0.0062,  0.0039, -0.0037],\n",
      "        ...,\n",
      "        [-0.0287,  0.0508, -0.0474,  ...,  0.0316,  0.0009,  0.0108],\n",
      "        [-0.0258, -0.0096,  0.0075,  ..., -0.0291, -0.0626, -0.0089],\n",
      "        [ 0.0060, -0.0028, -0.0319,  ..., -0.0128,  0.0170, -0.0358]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[ 0.0280, -0.0614,  0.0014,  ..., -0.0395,  0.0535,  0.0635],\n",
      "        [ 0.0390, -0.0549,  0.0096,  ..., -0.0415,  0.0498,  0.0667],\n",
      "        [ 0.0423, -0.0534, -0.0101,  ..., -0.0264,  0.0463,  0.0620],\n",
      "        ...,\n",
      "        [ 0.0245, -0.0390, -0.0258,  ..., -0.0249,  0.0541,  0.0486],\n",
      "        [ 0.0332, -0.0412,  0.0024,  ..., -0.0279,  0.0376,  0.0550],\n",
      "        [ 0.0449, -0.0428, -0.0134,  ..., -0.0205,  0.0440,  0.0548]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                   | 4/1562 [00:07<51:40,  1.99s/it]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "max_samples = cfg.eval_max\n",
    "\n",
    "# top_activations = {i: (None, None) for i in interesting_features_indices}\n",
    "encoder_biases = sparse_autoencoder.b_enc#[interesting_features_indices]\n",
    "encoder_weights = sparse_autoencoder.W_enc#[:, interesting_features_indices]\n",
    "\n",
    "top_k=10\n",
    "processed_samples = 0\n",
    "default_embeds_list = []\n",
    "feature_steered_embeds = defaultdict(list)\n",
    "l = 0\n",
    "for batch_images, _, batch_indices in tqdm(val_dataloader, total=max_samples // cfg.batch_size):\n",
    "    batch_images = batch_images.to(cfg.device)\n",
    "    batch_indices = batch_indices.to(cfg.device)\n",
    "    batch_size = batch_images.shape[0]\n",
    "\n",
    "    altered_embeds_list, default_embeds = compute_feature_activations_set_feat(\n",
    "        batch_images, model, sparse_autoencoder, encoder_weights, encoder_biases,\n",
    "        None, None, top_k\n",
    "    )\n",
    "    default_embeds_list.append(default_embeds)\n",
    "    for j, altered_embeds in enumerate(altered_embeds_list):\n",
    "        feature_steered_embeds[random_feat_idxs[j]].extend(altered_embeds)\n",
    "    # either label embeds or optimize to maximal token in text transformer embedding face\n",
    "    l += 1\n",
    "    if l >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_steered_embeds[random_feat_idxs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([160, 512])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_embeds.shape\n",
    "len(default_embeds_list)\n",
    "default_embeds = torch.cat(default_embeds_list)\n",
    "default_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, torch.Size([32, 512]), torch.Size([160, 512]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(altered_embeds_list), altered_embeds_list[0].shape, default_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/clip_dissect_raw.txt\", \"r\") as f:\n",
    "    larger_vocab = [line[:-1] for line in f.readlines()][:5000]\n",
    "\n",
    "# with open(\"/workspace/better_img_desc.txt\", \"r\") as f:\n",
    "#     larger_vocab = [line[:-1] for line in f.readlines()][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_features_normed.shape: torch.Size([5000, 512])\n",
      "919\n",
      "258\n",
      "1266\n",
      "995\n",
      "1706\n",
      "996\n",
      "391\n",
      "2750\n",
      "2807\n",
      "1757\n",
      "747\n",
      "2681\n",
      "912\n",
      "164\n",
      "2021\n",
      "1331\n",
      "1763\n",
      "146\n",
      "2769\n",
      "929\n",
      "949\n",
      "283\n",
      "1780\n",
      "2180\n",
      "1323\n",
      "Label probs altered: torch.Size([160, 5000])\n",
      "Label probs default: torch.Size([160, 5000])\n"
     ]
    }
   ],
   "source": [
    "# use clip vocab here and compare embeds\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "text = tokenizer(larger_vocab)\n",
    "text_features = og_model.encode_text(text.cuda())\n",
    "text_features_normed = text_features/text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "print(f\"text_features_normed.shape: {text_features_normed.shape}\")\n",
    "text_probs_altered_list = []\n",
    "# can probs make this one tensor \n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    for key in feature_steered_embeds:\n",
    "        print(key)\n",
    "        # embeds already have L2 norm of 1\n",
    "        text_probs_altered = (100.0 * torch.stack(feature_steered_embeds[key]) @ text_features_normed.T).softmax(dim=-1)\n",
    "        text_probs_altered_list.append(text_probs_altered)\n",
    "    text_probs_default = (100.0 * default_embeds @ text_features_normed.T).softmax(dim=-1)\n",
    "\n",
    "\n",
    "# for altered_embeds in altered_embeds_list:\n",
    "#     with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "#         # might want to still normalize\n",
    "        \n",
    "#         # already normalized\n",
    "#         # altered_embeds /= altered_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         text_probs_altered = (100.0 * altered_embeds @ text_features_normed.T).softmax(dim=-1)\n",
    "#         text_probs_altered_list.append(text_probs_altered)\n",
    "#     # default_embds_norm = default_embeds.norm(dim=-1, keepdim=True)\n",
    "#     text_probs_default = (100.0 * default_embeds @ text_features_normed.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs altered:\", text_probs_altered.shape)  # prints: [[1., 0., 0.]]\n",
    "print(\"Label probs default:\", text_probs_default.shape)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([919, 258, 1266, 995, 1706, 996, 391, 2750, 2807, 1757, 747, 2681, 912, 164, 2021, 1331, 1763, 146, 2769, 929, 949, 283, 1780, 2180, 1323])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_steered_embeds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0703e-03, 3.9035e-04, 2.0367e-05,  ..., 5.0409e-05, 2.4376e-05,\n",
       "         1.5616e-05],\n",
       "        [3.1685e-03, 2.6834e-04, 2.2027e-05,  ..., 1.1451e-04, 2.5353e-05,\n",
       "         1.5139e-05],\n",
       "        [1.8051e-03, 2.0253e-04, 8.9683e-06,  ..., 7.5091e-05, 3.8054e-05,\n",
       "         6.0683e-06],\n",
       "        ...,\n",
       "        [2.2572e-03, 1.3556e-04, 5.7952e-06,  ..., 5.1451e-05, 2.1115e-05,\n",
       "         3.1451e-05],\n",
       "        [3.1749e-03, 1.4169e-04, 8.5761e-06,  ..., 1.8480e-04, 4.1559e-05,\n",
       "         1.0105e-05],\n",
       "        [4.2913e-03, 3.5502e-04, 9.9926e-06,  ..., 1.8855e-04, 2.1488e-05,\n",
       "         9.4608e-06]], device='cuda:0')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_probs_altered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summed Logit Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "\n",
      "For Feature 919\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0004, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['parts' 'coverage' 'kit' ... 'layout' 'dodge' 'seat']\n",
      " ['parts' 'coverage' 'headlines' ... 'set' 'trailers' 'shown']\n",
      " ['parts' 'kit' 'wing' ... 'dodge' 'trailers' 'paint']\n",
      " ...\n",
      " ['parts' 'templates' 'custom' ... 'trailers' 'modified' 'automotive']\n",
      " ['parts' 'kit' 'set' ... 'wing' 'seat' 'shown']\n",
      " ['parts' 'coverage' 'kit' ... 'headlines' 'dodge' 'shown']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['parts' 'coverage' 'kit' 'templates' 'set' 'headlines' 'trailers'\n",
      " 'layout' 'dodge' 'seat']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([47.3456,  6.0059,  5.7569,  4.6167,  3.1223,  2.5825,  2.4684,  1.8153,\n",
      "         1.3686,  1.3016], device='cuda:0')\n",
      "['parts' 'kit' 'coverage' 'templates' 'headlines' 'wing' 'trailers' 'set'\n",
      " 'shown' 'custom']\n",
      "tensor([-2.3839, -2.0354, -1.7564, -1.5694, -1.4884, -1.3134, -1.1959, -1.1532,\n",
      "        -1.1254, -1.1200], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'detail' 'male' 'seal'\n",
      " 'fox']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([2663290.7500,  330466.6250,  277706.0938,  190053.2188,  160110.4688,\n",
      "         156875.1406,  155854.6719,  136060.4375,   95112.6797,   81987.0156],\n",
      "       device='cuda:0')\n",
      "['parts' 'sponsors' 'templates' 'papers' 'trailers' 'vehicles' 'headlines'\n",
      " 'pittsburgh' 'boats' 'trucks']\n",
      "tensor([-2.3839, -2.0354, -1.7564, -1.5694, -1.4884, -1.3134, -1.1959, -1.1532,\n",
      "        -1.1254, -1.1200], device='cuda:0')\n",
      "['outlet' 'plug' 'waiting' 'offering' 'unix' 'virtual' 'walking' 'plaza'\n",
      " 'mpeg' 'festival']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 258\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['kiss' 'proposal' 'jul' ... 'rental' 'hundred' 'lay']\n",
      " ['kiss' 'proposal' 'jul' ... 'lay' 'tradition' 'proposals']\n",
      " ['proposal' 'kiss' 'southern' ... 'charlotte' 'commission' 'hundred']\n",
      " ...\n",
      " ['gun' 'kiss' 'proposal' ... 'rental' 'resistance' 'western']\n",
      " ['kiss' 'rental' 'proposal' ... 'outdoor' 'eligible' 'sit']\n",
      " ['proposal' 'kiss' 'outdoor' ... 'parks' 'event' 'philadelphia']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['kiss' 'proposal' 'jul' 'commission' 'western' 'proposals' 'outdoor'\n",
      " 'rental' 'hundred' 'lay']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([10.5247,  8.3760,  2.3807,  1.8258,  1.7361,  1.4205,  1.3442,  1.0785,\n",
      "         0.9774,  0.8206], device='cuda:0')\n",
      "['kiss' 'proposal' 'rental' 'outdoor' 'southern' 'rentals' 'commission'\n",
      " 'jul' 'eligible' 'event']\n",
      "tensor([-2.3870, -2.0288, -1.7501, -1.5889, -1.4871, -1.3125, -1.2822, -1.1977,\n",
      "        -1.1958, -1.1580], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([4603743.5000, 3343809.7500,  229270.1719,  196738.0781,  185107.7812,\n",
      "         161582.2969,  157339.3594,  142789.1250,  136509.8438,  115436.0547],\n",
      "       device='cuda:0')\n",
      "['nashville' 'philadelphia' 'pittsburgh' 'nebraska' 'colleges'\n",
      " 'cincinnati' 'dallas' 'ministry' 'virginia' 'lesbian']\n",
      "tensor([-2.3870, -2.0288, -1.7501, -1.5889, -1.4871, -1.3125, -1.2822, -1.1977,\n",
      "        -1.1958, -1.1580], device='cuda:0')\n",
      "['bureau' 'shaved' 'desk' 'comparison' 'ipod' 'scale' 'sole' 'rear'\n",
      " 'clock' 'documents']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1266\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0003, 0.0003, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0003, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0003, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0004, 0.0003, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0003, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0003, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['gnu' 'ram' 'turkey' ... 'mass' 'asp' 'jewish']\n",
      " ['ram' 'gnu' 'turkey' ... 'charger' 'shall' 'canon']\n",
      " ['ram' 'gnu' 'turkey' ... 'shall' 'holy' 'hearing']\n",
      " ...\n",
      " ['gnu' 'ram' 'gun' ... 'rf' 'dodge' 'mass']\n",
      " ['ram' 'gnu' 'turkey' ... 'charger' 'rf' 'jesus']\n",
      " ['ram' 'gnu' 'turkey' ... 'shall' 'hearing' 'holy']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0003, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['gnu' 'ram' 'turkey' 'charger' 'buffalo' 'holy' 'dodge' 'mass' 'asp'\n",
      " 'jewish']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([65.2224, 59.0624,  6.0599,  1.1601,  0.8830,  0.4953,  0.4082,  0.3351,\n",
      "         0.3321,  0.3301], device='cuda:0')\n",
      "['ram' 'gnu' 'turkey' 'charger' 'buffalo' 'holy' 'jesus' 'shall' 'mass'\n",
      " 'hearing']\n",
      "tensor([-2.3568, -2.0122, -1.7592, -1.5886, -1.4884, -1.3153, -1.2854, -1.1950,\n",
      "        -1.1914, -1.1522], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'detail' 'eagle'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1.0068e+07, 7.3624e+05, 8.1333e+04, 1.4820e+04, 1.0505e+04, 1.0492e+04,\n",
      "        9.9557e+03, 9.4337e+03, 6.9750e+03, 6.7381e+03], device='cuda:0')\n",
      "['gnu' 'ram' 'buffalo' 'turkey' 'charger' 'churches' 'temple' 'logic'\n",
      " 'jesus' 'mysql']\n",
      "tensor([-2.3568, -2.0122, -1.7592, -1.5886, -1.4884, -1.3153, -1.2854, -1.1950,\n",
      "        -1.1914, -1.1522], device='cuda:0')\n",
      "['injury' 'jennifer' 'anywhere' 'exception' 'appointed' 'intended'\n",
      " 'movement' 'sequence' 'incident' 'upskirt']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 995\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['advertisement' 'ads' 'magazine' ... 'contest' 'beer' 'monthly']\n",
      " ['advertisement' 'contest' 'magazine' ... 'milf' 'kim' 'upskirt']\n",
      " ['magazine' 'advertisement' 'contest' ... 'upskirt' 'ads' 'postal']\n",
      " ...\n",
      " ['advertisement' 'magazine' 'ads' ... 'poster' 'shorts' 'pipe']\n",
      " ['advertisement' 'pub' 'magazine' ... 'shorts' 'ad' 'paperback']\n",
      " ['magazine' 'advertisement' 'contest' ... 'ad' 'tour' 'paperback']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['advertisement' 'ads' 'magazine' 'pub' 'poster' 'shorts' 'ad' 'contest'\n",
      " 'beer' 'monthly']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([9.7549, 7.6453, 4.2635, 4.2455, 3.9032, 3.1837, 2.3183, 2.1446, 2.1386,\n",
      "        2.1156], device='cuda:0')\n",
      "['advertisement' 'magazine' 'contest' 'pub' 'ads' 'poster' 'shorts' 'milf'\n",
      " 'beer' 'ad']\n",
      "tensor([-2.3855, -1.8236, -1.7554, -1.5900, -1.4883, -1.3147, -1.2328, -1.1964,\n",
      "        -1.1963, -1.1356], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1140451.3750,  521662.7500,  346037.7500,  243360.6094,  231874.4531,\n",
      "         151458.6094,  111899.0234,   95696.5234,   86836.5625,   84987.3281],\n",
      "       device='cuda:0')\n",
      "['shorts' 'catalogue' 'catalog' 'poster' 'pub' 'quotes' 'advertisement'\n",
      " 'posters' 'ads' 'upskirt']\n",
      "tensor([-2.3855, -1.8236, -1.7554, -1.5900, -1.4883, -1.3147, -1.2328, -1.1964,\n",
      "        -1.1963, -1.1356], device='cuda:0')\n",
      "['detail' 'receiving' 'simon' 'joseph' 'details' 'partial' 'offering'\n",
      " 'screening' 'glance' 'david']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1706\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['nearest' 'passed' 'wall' ... 'sending' 'tracking' 'street']\n",
      " ['rf' 'sending' 'ski' ... 'board' 'sw' 'tracking']\n",
      " ['pic' 'nearest' 'logic' ... 'sending' 'stupid' 'mc']\n",
      " ...\n",
      " ['rf' 'wall' 'nearest' ... 'search' 'ss' 'logic']\n",
      " ['interior' 'rf' 'nearest' ... 'rss' 'board' 'deal']\n",
      " ['rf' 'wall' 'rss' ... 'sending' 'nearby' 'stupid']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['nearest' 'passed' 'wall' 'spent' 'rss' 'rf' 'deal' 'sending' 'tracking'\n",
      " 'street']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([2.8256, 2.6030, 1.8233, 1.5928, 1.5799, 1.5188, 1.0964, 1.0383, 0.9422,\n",
      "        0.9173], device='cuda:0')\n",
      "['nearest' 'rf' 'wall' 'sending' 'logic' 'rss' 'spent' 'passed' 'deal'\n",
      " 'stupid']\n",
      "tensor([-2.3869, -2.0244, -1.7520, -1.4859, -1.4314, -1.3150, -1.2817, -1.1941,\n",
      "        -1.1730, -1.1568], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'python' 'ray' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([829310.9375, 243557.6094, 148107.9219, 130457.8750, 106594.1016,\n",
      "         96273.0859,  66482.9297,  58124.1836,  56610.5000,  46014.9883],\n",
      "       device='cuda:0')\n",
      "['street' 'logic' 'philadelphia' 'streets' 'rss' 'guestbook' 'road'\n",
      " 'citysearch' 'disclosure' 'pittsburgh']\n",
      "tensor([-2.3869, -2.0244, -1.7520, -1.4859, -1.4314, -1.3150, -1.2817, -1.1941,\n",
      "        -1.1730, -1.1568], device='cuda:0')\n",
      "['balance' 'desktops' 'differences' 'hole' 'monitors' 'shoe' 'supplements'\n",
      " 'administrative' 'cells' 'matrix']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 996\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['epson' 'panasonic' 'dell' ... 'size' 'philips' 'pioneer']\n",
      " ['epson' 'mens' 'dell' ... 'shown' 'giant' 'size']\n",
      " ['epson' 'panasonic' 'mac' ... 'mens' 'dog' 'size']\n",
      " ...\n",
      " ['panasonic' 'epson' 'canon' ... 'mens' 'philips' 'shown']\n",
      " ['epson' 'panasonic' 'mens' ... 'canon' 'philips' 'nec']\n",
      " ['epson' 'canon' 'dell' ... 'bluetooth' 'shown' 'mens']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['epson' 'panasonic' 'dell' 'canon' 'mens' 'thongs' 'mac' 'size' 'philips'\n",
      " 'pioneer']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([38.4323, 10.4409,  4.4323,  3.6278,  3.3626,  3.3206,  2.7508,  2.6618,\n",
      "         2.1594,  1.9750], device='cuda:0')\n",
      "['epson' 'panasonic' 'canon' 'mens' 'mac' 'dell' 'pioneer' 'philips'\n",
      " 'bluetooth' 'size']\n",
      "tensor([-2.3869, -1.7436, -1.5868, -1.5578, -1.4837, -1.3139, -1.2490, -1.1953,\n",
      "        -1.1672, -1.1333], device='cuda:0')\n",
      "['guinea' 'fucking' 'ray' 'dog' 'python' 'fish' 'pair' 'detail' 'eagle'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1109839.2500,  234122.8750,  105275.1016,   37807.4453,   31664.2441,\n",
      "          29626.1504,   27858.3164,   24288.3086,   22058.1816,   18256.0039],\n",
      "       device='cuda:0')\n",
      "['bluetooth' 'epson' 'panasonic' 'mens' 'canon' 'electronics' 'recipes'\n",
      " 'philips' 'toyota' 'honda']\n",
      "tensor([-2.3869, -1.7436, -1.5868, -1.5578, -1.4837, -1.3139, -1.2490, -1.1953,\n",
      "        -1.1672, -1.1333], device='cuda:0')\n",
      "['baltimore' 'ak' 'forgotten' 'november' 'cyprus' 'authorities' 'night'\n",
      " 'fort' 'down' 'fighting']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 391\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['golf' 'ball' 'dance' ... 'pan' 'test' 'sand']\n",
      " ['golf' 'dance' 'pan' ... 'games' 'dancing' 'game']\n",
      " ['golf' 'pet' 'ball' ... 'online' 'pub' 'gaming']\n",
      " ...\n",
      " ['golf' 'games' 'jason' ... 'searching' 'ball' 'laser']\n",
      " ['golf' 'dance' 'pan' ... 'ball' 'fitness' 'jason']\n",
      " ['golf' 'tennis' 'games' ... 'course' 'gaming' 'balls']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['golf' 'ball' 'dance' 'tennis' 'game' 'balls' 'games' 'pan' 'test' 'sand']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([40.4366,  2.5112,  2.3558,  2.3029,  1.9255,  1.8399,  1.5266,  1.4156,\n",
      "         1.1679,  1.0563], device='cuda:0')\n",
      "['golf' 'dance' 'pet' 'games' 'pan' 'jason' 'ball' 'tennis' 'game'\n",
      " 'online']\n",
      "tensor([-2.3691, -1.9946, -1.7581, -1.5866, -1.4865, -1.2664, -1.2061, -1.1962,\n",
      "        -1.1458, -1.1348], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'detail' 'male'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([784473.3125, 466476.7188, 207414.4219, 149320.2188, 136091.1406,\n",
      "        101370.4141,  84541.7812,  73070.1328,  72631.8516,  68598.3125],\n",
      "       device='cuda:0')\n",
      "['jason' 'golf' 'restaurants' 'bedroom' 'crafts' 'movies' 'anime' 'pink'\n",
      " 'games' 'dance']\n",
      "tensor([-2.3691, -1.9946, -1.7581, -1.5866, -1.4865, -1.2664, -1.2061, -1.1962,\n",
      "        -1.1458, -1.1348], device='cuda:0')\n",
      "['elected' 'charge' 'side' 'glance' 'column' 'proposal' 'photo' 'charged'\n",
      " 'proposals' 'usd']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2750\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['or' 'op' 'epson' ... 'u' 'a' 'cr']\n",
      " ['epson' 'our' 'oh' ... 'wa' 'guides' 'al']\n",
      " ['epson' 'op' 'oh' ... 'ha' 'our' 'wa']\n",
      " ...\n",
      " ['epson' 'banner' 'or' ... 'clip' 'french' 'rhode']\n",
      " ['epson' 'or' 'ron' ... 'op' 'banner' 'our']\n",
      " ['epson' 'or' 'our' ... 'u' 'al' 'du']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['or' 'op' 'epson' 'oh' 'our' 'ah' 'banner' 'u' 'a' 'cr']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([6.6414, 3.8060, 3.6737, 3.3291, 3.2002, 2.7042, 2.1671, 1.9034, 1.8238,\n",
      "        1.8139], device='cuda:0')\n",
      "['epson' 'or' 'oh' 'ah' 'banner' 'our' 'op' 'cr' 'up' 'episode']\n",
      "tensor([-2.3857, -2.0326, -1.7571, -1.5787, -1.4887, -1.3135, -1.2767, -1.2060,\n",
      "        -1.1831, -1.1623], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([178304.3281, 151722.2812, 110901.4141,  85432.0234,  81107.0938,\n",
      "         66211.6016,  59963.0000,  37336.4492,  35083.6680,  33826.2070],\n",
      "       device='cuda:0')\n",
      "['colleges' 'banner' 'quotes' 'episode' 'rhode' 'recipes' 'houses' 'epson'\n",
      " 'catalog' 'ron']\n",
      "tensor([-2.3857, -2.0326, -1.7571, -1.5787, -1.4887, -1.3135, -1.2767, -1.2060,\n",
      "        -1.1831, -1.1623], device='cuda:0')\n",
      "['spyware' 'remote' 'mouth' 'temperature' 'television' 'composition'\n",
      " 'birmingham' 'viewing' 'inspection' 'infection']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2807\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['dust' 'goods' 'sand' ... 'troops' 'mix' 'teachers']\n",
      " ['winter' 'mt' 'holidays' ... 'mount' 'staff' 'pull']\n",
      " ['hair' 'stick' 'staff' ... 'photo' 'teachers' 'holder']\n",
      " ...\n",
      " ['holder' 'pin' 'card' ... 'pull' 'accessory' 'staff']\n",
      " ['inn' 'teachers' 'cleaning' ... 'lodge' 'holder' 'launch']\n",
      " ['gardens' 'square' 'mt' ... 'garden' 'min' 'parks']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['dust' 'goods' 'sand' 'cleaning' 'clean' 'pull' 'prices' 'troops' 'mix'\n",
      " 'teachers']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([1.8723, 1.8105, 1.5028, 1.4921, 1.4266, 1.3423, 1.2889, 1.2553, 1.2427,\n",
      "        1.1929], device='cuda:0')\n",
      "['teachers' 'holder' 'pull' 'card' 'cleaning' 'staff' 'prices' 'clean'\n",
      " 'goods' 'offer']\n",
      "tensor([-2.3249, -2.0068, -1.5897, -1.4886, -1.3080, -1.1962, -1.1928, -1.1454,\n",
      "        -1.1339, -1.1187], device='cuda:0')\n",
      "['guinea' 'dog' 'ray' 'python' 'fish' 'detail' 'eagle' 'male' 'seal' 'fox']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([481915.3125, 435639.3438, 422831.2188, 136680.9844, 121263.4375,\n",
      "        118181.6484, 115703.6797, 105086.5000,  83561.9531,  82452.2891],\n",
      "       device='cuda:0')\n",
      "['teachers' 'farmers' 'volunteers' 'scientists' 'houses' 'plants'\n",
      " 'students' 'prices' 'hair' 'farm']\n",
      "tensor([-2.3249, -2.0068, -1.5897, -1.4886, -1.3080, -1.1962, -1.1928, -1.1454,\n",
      "        -1.1339, -1.1187], device='cuda:0')\n",
      "['pubmed' 'denver' 'ea' 'birmingham' 'privacy' 'identified' 'apache'\n",
      " 'detailed' 'colorado' 'dallas']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1757\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0003, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['pet' 'toy' 'seat' ... 'section' 'photo' 'pack']\n",
      " ['photo' 'section' 'pet' ... 'pack' 'toy' 'rear']\n",
      " ['pet' 'dog' 'pets' ... 'seat' 'dogs' 'stick']\n",
      " ...\n",
      " ['toy' 'gun' 'thompson' ... 'rear' 'milfhunter' 'layout']\n",
      " ['chair' 'seat' 'section' ... 'photo' 'furniture' 'pack']\n",
      " ['section' 'photo' 'chair' ... 'opened' 'font' 'po']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['pet' 'toy' 'seat' 'layout' 'chair' 'dog' 'opened' 'section' 'photo'\n",
      " 'pack']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([17.0372,  6.1612,  4.8192,  4.3788,  3.7739,  3.7040,  3.2929,  2.0715,\n",
      "         1.6375,  1.5129], device='cuda:0')\n",
      "['pet' 'toy' 'dog' 'chair' 'photo' 'section' 'seat' 'opened' 'pack'\n",
      " 'layout']\n",
      "tensor([-2.3787, -1.7206, -1.5477, -1.4837, -1.3134, -1.1796, -1.1549, -1.1251,\n",
      "        -1.0505, -0.9925], device='cuda:0')\n",
      "['guinea' 'fucking' 'ray' 'python' 'fish' 'eagle' 'detail' 'seal' 'fox'\n",
      " 'playing']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([189381.1094, 148054.7812, 115596.2891,  70551.7734,  66663.4219,\n",
      "         65654.9688,  53360.6953,  40551.3867,  39013.1211,  34500.7461],\n",
      "       device='cuda:0')\n",
      "['recipes' 'chair' 'label' 'jacket' 'foods' 'dog' 'pet' 'bear' 'nashville'\n",
      " 'pets']\n",
      "tensor([-2.3787, -1.7206, -1.5477, -1.4837, -1.3134, -1.1796, -1.1549, -1.1251,\n",
      "        -1.0505, -0.9925], device='cuda:0')\n",
      "['remaining' 'chronic' 'ended' 'identified' 'shaved' 'plasma' 'clicking'\n",
      " 'blood' 'finds' 'regarding']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 747\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['sheet' 'printable' 'template' ... 'qty' 'trailer' 'h']\n",
      " ['mens' 'sheet' 'printable' ... 'womens' 'mt' 'rt']\n",
      " ['printable' 'sheet' 'outdoor' ... 'from' 'womens' 'h']\n",
      " ...\n",
      " ['printable' 'sheet' 'mens' ... 'cs' 'womens' 't']\n",
      " ['printable' 'sheet' 'shown' ... 'outdoor' 'trailer' 'womens']\n",
      " ['outdoor' 'sheet' 'printable' ... 'screen' 'template' 'rt']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['sheet' 'printable' 'template' 'outdoor' 'mens' 'pst' 'womens' 'qty'\n",
      " 'trailer' 'h']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([14.2127,  7.5085,  4.1904,  4.0874,  2.6615,  2.6443,  2.5043,  2.2529,\n",
      "         1.5696,  1.4030], device='cuda:0')\n",
      "['printable' 'sheet' 'mens' 'outdoor' 'pst' 'shown' 'template' 'womens'\n",
      " 'trailers' 'rt']\n",
      "tensor([-2.3720, -2.0134, -1.7568, -1.4786, -1.3086, -1.3005, -1.2014, -1.1936,\n",
      "        -1.1353, -1.0921], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'python' 'ray' 'fish' 'eagle' 'detail' 'seal'\n",
      " 'fox']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([533037.3125, 132286.7344,  95451.9922,  67354.8984,  63127.8633,\n",
      "         56653.7305,  53173.8750,  48527.3398,  40363.5117,  32896.1836],\n",
      "       device='cuda:0')\n",
      "['printable' 'speakers' 'trailers' 'hotel' 'fans' 'mens' 'outdoor' 'sheet'\n",
      " 'it' 'womens']\n",
      "tensor([-2.3720, -2.0134, -1.7568, -1.4786, -1.3086, -1.3005, -1.2014, -1.1936,\n",
      "        -1.1353, -1.0921], device='cuda:0')\n",
      "['miller' 'exhibit' 'aged' 'model' 'exposure' 'sight' 'exhibition' 'eight'\n",
      " 'instructor' 'commissioner']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2681\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['madison' 'proposals' 'python' ... 'addition' 'amendment' 'addressed']\n",
      " ['madison' 'proposals' 'edge' ... 'lodging' 'protected' 'undergraduate']\n",
      " ['madison' 'undergraduate' 'elementary' ... 'addition' 'douglas'\n",
      "  'degree']\n",
      " ...\n",
      " ['madison' 'elementary' 'progressive' ... 'admission' 'warranty'\n",
      "  'degree']\n",
      " ['madison' 'elementary' 'undergraduate' ... 'addressed' 'proposals'\n",
      "  'provision']\n",
      " ['madison' 'admission' 'proposals' ... 'progressive' 'addition'\n",
      "  'elementary']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['madison' 'proposals' 'python' 'elementary' 'progressive' 'provision'\n",
      " 'admission' 'addition' 'amendment' 'addressed']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([26.9430,  3.8866,  2.7849,  2.6191,  2.0778,  2.0616,  1.9073,  1.4432,\n",
      "         1.2835,  1.2434], device='cuda:0')\n",
      "['madison' 'elementary' 'undergraduate' 'progressive' 'proposals'\n",
      " 'provision' 'admission' 'amendment' 'radiation' 'protected']\n",
      "tensor([-2.3867, -2.0212, -1.7404, -1.5133, -1.2851, -1.2435, -1.1961, -1.1907,\n",
      "        -1.1626, -1.1109], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'fish' 'pair' 'detail' 'eagle' 'male'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([5347134.0000, 1008468.9375, 1001665.1875,  768721.1875,  412865.3438,\n",
      "         119288.3281,  109937.7656,   97804.7422,   74841.1484,   71823.1641],\n",
      "       device='cuda:0')\n",
      "['madison' 'nebraska' 'colleges' 'elementary' 'cincinnati' 'warranty'\n",
      " 'admission' 'pubmed' 'announcements' 'addresses']\n",
      "tensor([-2.3867, -2.0212, -1.7404, -1.5133, -1.2851, -1.2435, -1.1961, -1.1907,\n",
      "        -1.1626, -1.1109], device='cuda:0')\n",
      "['remote' 'stand' 'rear' 'bowl' 'iii' 'gate' 'ken' 'various' 'match'\n",
      " 'opened']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 912\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['configuration' 'installation' 'business' ... 'setting' 'systems'\n",
      "  'scottish']\n",
      " ['scottish' 'turkey' 'displays' ... 'setting' 'degrees' 'alaska']\n",
      " ['installed' 'scottish' 'custom' ... 'curriculum' 'customer' 'pic']\n",
      " ...\n",
      " ['weapons' 'custom' 'customize' ... 'business' 'corrections' 'gun']\n",
      " ['displays' 'installed' 'installation' ... 'lessons' 'catering'\n",
      "  'traditional']\n",
      " ['farm' 'gardens' 'scottish' ... 'curriculum' 'listings' 'displays']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['configuration' 'installation' 'business' 'turkey' 'installed' 'versions'\n",
      " 'customize' 'setting' 'systems' 'scottish']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([2.3353, 2.2925, 2.1246, 1.9291, 1.4064, 1.4034, 1.2940, 1.2536, 1.2182,\n",
      "        1.1398], device='cuda:0')\n",
      "['business' 'scottish' 'customize' 'turkey' 'catering' 'custom'\n",
      " 'collections' 'filters' 'installed' 'configuration']\n",
      "tensor([-2.2444, -2.0325, -1.7586, -1.5908, -1.4860, -1.2862, -1.2647, -1.1929,\n",
      "        -1.1632, -1.1357], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'male'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([737233.5000, 536410.0000, 521727.7188, 305616.5000, 243876.3594,\n",
      "        189010.3438, 160443.3281, 158456.1719, 126012.1484, 115134.0547],\n",
      "       device='cuda:0')\n",
      "['recipes' 'catering' 'philadelphia' 'farm' 'businesses' 'displays'\n",
      " 'bedroom' 'catalog' 'plants' 'customer']\n",
      "tensor([-2.2444, -2.0325, -1.7586, -1.5908, -1.4860, -1.2862, -1.2647, -1.1929,\n",
      "        -1.1632, -1.1357], device='cuda:0')\n",
      "['ray' 'msn' 'blackjack' 'wake' 'loop' 'miles' 'switch' 'miller' 'rape'\n",
      " 'basis']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 164\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['pipe' 'cast' 'epinions' ... 'seat' 'p' 'drawing']\n",
      " ['shown' 'pipe' 'cast' ... 'recent' 'generated' 'piece']\n",
      " ['pipe' 'pa' 'shown' ... 'cotton' 'generated' 'little']\n",
      " ...\n",
      " ['pipe' 'shown' 'holder' ... 'p' 'thumbnail' 'brass']\n",
      " ['shown' 'pipe' 'seat' ... 'epinions' 'piece' 'set']\n",
      " ['pipe' 'pa' 'shown' ... 'section' 'paperback' 'qty']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['pipe' 'cast' 'epinions' 'shown' 'qty' 'sand' 'd' 'seat' 'p' 'drawing']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([8.5625, 6.0462, 2.8417, 2.7785, 2.3951, 1.7335, 1.6889, 1.6670, 1.4791,\n",
      "        1.2983], device='cuda:0')\n",
      "['pipe' 'shown' 'pa' 'cast' 'thumbnail' 'generated' 'cotton' 'epinions'\n",
      " 'paperback' 'seat']\n",
      "tensor([-2.3768, -1.9104, -1.5865, -1.4361, -1.4268, -1.3025, -1.1986, -1.1080,\n",
      "        -1.0930, -1.0843], device='cuda:0')\n",
      "['guinea' 'dog' 'ray' 'python' 'fucking' 'fish' 'eagle' 'fox' 'seal'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([136608.2344,  68895.9766,  65066.2461,  59226.5195,  45323.3594,\n",
      "         30815.9531,  30525.6094,  28848.9512,  23811.7617,  21757.2617],\n",
      "       device='cuda:0')\n",
      "['bears' 'boats' 'cincinnati' 'pipe' 'dates' 'sheets' 'cotton' 'boat'\n",
      " 'eating' 'estimate']\n",
      "tensor([-2.3768, -1.9104, -1.5865, -1.4361, -1.4268, -1.3025, -1.1986, -1.1080,\n",
      "        -1.0930, -1.0843], device='cuda:0')\n",
      "['jump' 'finder' 'aol' 'drop' 'air' 'playstation' 'museum' 'searching'\n",
      " 'ibm' 'dancing']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2021\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['pair' 'jersey' 'front' ... 'shoe' 'skip' 'these']\n",
      " ['pair' 'jersey' 'front' ... 'mount' 'these' 'australian']\n",
      " ['jersey' 'pair' 'front' ... 'jerry' 'pets' 'small']\n",
      " ...\n",
      " ['jersey' 'front' 'pair' ... 'card' 'magazines' 'oregon']\n",
      " ['pair' 'front' 'paperback' ... 'still' 'size' 'shoe']\n",
      " ['front' 'pair' 'jersey' ... 'these' 'oregon' 'estate']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['pair' 'jersey' 'front' 'still' 'paperback' 'set' 'size' 'shoe' 'skip'\n",
      " 'these']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([13.4448, 12.6732,  6.8392,  3.9027,  2.8727,  1.6912,  1.6861,  1.6370,\n",
      "         1.5635,  1.5480], device='cuda:0')\n",
      "['pair' 'jersey' 'front' 'paperback' 'wa' 'printer' 'size' 'card' 'still'\n",
      " 'these']\n",
      "tensor([-2.3861, -1.9487, -1.7442, -1.5876, -1.4885, -1.2922, -1.1963, -1.1961,\n",
      "        -1.1215, -1.1182], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'detail' 'seal'\n",
      " 'fox']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([251043.4844, 225391.0469, 172041.7344,  81681.6016,  79229.2734,\n",
      "         72054.1172,  69223.7109,  68546.8047,  60965.8945,  56207.1250],\n",
      "       device='cuda:0')\n",
      "['nj' 'counties' 'shirts' 'states' 'cincinnati' 'jersey' 'idaho' 'oregon'\n",
      " 'pennsylvania' 'maine']\n",
      "tensor([-2.3861, -1.9487, -1.7442, -1.5876, -1.4885, -1.2922, -1.1963, -1.1961,\n",
      "        -1.1215, -1.1182], device='cuda:0')\n",
      "['arrival' 'focused' 'thoughts' 'thinking' 'feedback' 'searching'\n",
      " 'something' 'prayer' 'examination' 'tutorial']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1331\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['card' 'thumbnail' 'mac' ... 'bear' 'patch' 'wallpaper']\n",
      " ['card' 'mac' 'shown' ... 'pair' 'amazon' 'rw']\n",
      " ['card' 'mac' 'thumbnail' ... 'wallpaper' 'cat' 'mi']\n",
      " ...\n",
      " ['card' 'mac' 'thumbnail' ... 'wallpaper' 'amazon' 'terry']\n",
      " ['card' 'screen' 'mac' ... 'amazon' 'panel' 'lg']\n",
      " ['card' 'screen' 'mac' ... 'cover' 'patch' 'oh']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['card' 'thumbnail' 'mac' 'screen' 'shown' 'amazon' 'recipe' 'bear'\n",
      " 'patch' 'wallpaper']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([41.4175,  8.1178,  7.2345,  6.0328,  3.5752,  2.5491,  1.9623,  1.7392,\n",
      "         1.7267,  1.5933], device='cuda:0')\n",
      "['card' 'mac' 'thumbnail' 'screen' 'shown' 'bear' 'amazon' 'wallpaper'\n",
      " 'recipe' 'printable']\n",
      "tensor([-2.2654, -1.8865, -1.7259, -1.4851, -1.3027, -1.2867, -1.1846, -1.1437,\n",
      "        -1.1110, -1.0993], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'python' 'fish' 'ray' 'detail' 'male' 'eagle'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([385422.5000, 163108.0469,  89819.8672,  62905.2500,  61925.5508,\n",
      "         48009.8398,  47559.3398,  46770.0078,  44836.4023,  44707.3945],\n",
      "       device='cuda:0')\n",
      "['card' 'bear' 'recipe' 'catalogue' 'printable' 'certificate' 'wallpaper'\n",
      " 'bears' 'wallpapers' 'cincinnati']\n",
      "tensor([-2.2654, -1.8865, -1.7259, -1.4851, -1.3027, -1.2867, -1.1846, -1.1437,\n",
      "        -1.1110, -1.0993], device='cuda:0')\n",
      "['bulgaria' 'becomes' 'scope' 'degrees' 'copies' 'execution' 'presence'\n",
      " 'chile' 'decisions' 'vacations']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1763\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['photo' 'drug' 'bag' ... 'euro' 'mail' 'email']\n",
      " ['hockey' 'photo' 'cooperation' ... 'television' 'email' 'australian']\n",
      " ['photo' 'mail' 'email' ... 'leading' 'blogs' 'networking']\n",
      " ...\n",
      " ['trademarks' 'photo' 'www' ... 'aluminum' 'email' 'mailing']\n",
      " ['television' 'window' 'photo' ... 'quantity' 'leading' 'medication']\n",
      " ['photo' 'singapore' 'networking' ... 'trademarks' 'australia' 'email']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['photo' 'drug' 'bag' 'television' 'trademarks' 'singapore' 'hockey'\n",
      " 'euro' 'mail' 'email']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([4.2571, 2.5589, 2.4232, 2.0451, 1.9769, 1.9367, 1.7449, 1.5928, 1.5510,\n",
      "        1.4467], device='cuda:0')\n",
      "['photo' 'television' 'trademarks' 'email' 'drug' 'bag' 'mail' 'betting'\n",
      " 'emails' 'networking']\n",
      "tensor([-2.3577, -2.0009, -1.7519, -1.5670, -1.4703, -1.2968, -1.2688, -1.1846,\n",
      "        -1.1623, -1.1551], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'detail' 'eagle'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([2715882.0000,  505427.8125,  455479.4688,  380724.1875,  364698.6250,\n",
      "         308282.9688,  275643.8438,  248935.6562,  243055.6562,  212633.7031],\n",
      "       device='cuda:0')\n",
      "['supplements' 'connecticut' 'philadelphia' 'phentermine' 'pittsburgh'\n",
      " 'pills' 'singapore' 'nebraska' 'pennsylvania' 'betting']\n",
      "tensor([-2.3577, -2.0009, -1.7519, -1.5670, -1.4703, -1.2968, -1.2688, -1.1846,\n",
      "        -1.1623, -1.1551], device='cuda:0')\n",
      "['pan' 'naked' 'tiffany' 'native' 'master' 'harris' 'ken' 'milfhunter'\n",
      " 'campbell' 'stupid']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 146\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['oh' 'and' 'by' ... 'ads' 'sign' 'advertisement']\n",
      " ['oh' 'and' 'front' ... 'ad' 'ha' 'these']\n",
      " ['oh' 'and' 'front' ... 'ha' 'netherlands' 'except']\n",
      " ...\n",
      " ['front' 'oh' 'gun' ... 'proposed' 'shown' 'card']\n",
      " ['oh' 'front' 'and' ... 'ad' 'ah' 'shown']\n",
      " ['oh' 'front' 'and' ... 'nsw' 'properly' 'ad']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['oh' 'and' 'by' 'properly' 'ad' 'ah' 'front' 'ads' 'sign' 'advertisement']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([11.9434,  5.0026,  3.7228,  2.8819,  2.3825,  2.2712,  1.7817,  1.3353,\n",
      "         1.2666,  1.1833], device='cuda:0')\n",
      "['oh' 'and' 'front' 'properly' 'ah' 'by' 'ad' 'except' 'advertisement'\n",
      " 'ha']\n",
      "tensor([-2.3779, -2.0282, -1.7474, -1.5905, -1.4857, -1.3107, -1.1971, -1.1737,\n",
      "        -1.1627, -1.1334], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'detail' 'male'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1579005.8750,  393285.2188,  345783.9062,  320389.0938,  300956.6250,\n",
      "         270681.4375,  225466.0000,  209988.6094,  204780.9531,  203267.7969],\n",
      "       device='cuda:0')\n",
      "['philadelphia' 'nj' 'netherlands' 'amsterdam' 'cincinnati' 'pittsburgh'\n",
      " 'melbourne' 'laptops' 'colleges' 'dublin']\n",
      "tensor([-2.3779, -2.0282, -1.7474, -1.5905, -1.4857, -1.3107, -1.1971, -1.1737,\n",
      "        -1.1627, -1.1334], device='cuda:0')\n",
      "['solo' 'maria' 'array' 'storm' 'messenger' 'diego' 'discovery'\n",
      " 'milfhunter' 'simon' 'mine']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2769\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['birth' 'john' 'tom' ... 'asian' 'lingerie' 'matt']\n",
      " ['tom' 'want' 'john' ... 'feet' 'ski' 'jon']\n",
      " ['tom' 'birth' 'want' ... 'jennifer' 'princess' 'womens']\n",
      " ...\n",
      " ['want' 'tom' 'gun' ... 'launched' 'these' 'matt']\n",
      " ['tom' 'want' 'birth' ... 'matt' 'these' 'walker']\n",
      " ['want' 'tom' 'john' ... 'korea' 'these' 'oh']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['birth' 'john' 'tom' 'feet' 'want' 'womens' 'jon' 'asian' 'lingerie'\n",
      " 'matt']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([5.5265, 4.8511, 4.5071, 3.7108, 2.7749, 1.9962, 1.8398, 1.7403, 1.1241,\n",
      "        1.0388], device='cuda:0')\n",
      "['birth' 'tom' 'want' 'john' 'womens' 'jon' 'asian' 'matt' 'these'\n",
      " 'jennifer']\n",
      "tensor([-2.3426, -2.0341, -1.7574, -1.5353, -1.4877, -1.3078, -1.2057, -1.1950,\n",
      "        -1.1465, -1.1212], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'detail' 'pair'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([443557.4062, 325493.0000, 249351.1250, 243643.3594, 192486.4219,\n",
      "        174208.5938, 155223.0938, 136372.3750, 131901.0156, 131873.5938],\n",
      "       device='cuda:0')\n",
      "['countries' 'lingerie' 'italy' 'recipes' 'cincinnati' 'bedroom' 'hawaii'\n",
      " 'trees' 'plants' 'rob']\n",
      "tensor([-2.3426, -2.0341, -1.7574, -1.5353, -1.4877, -1.3078, -1.2057, -1.1950,\n",
      "        -1.1465, -1.1212], device='cuda:0')\n",
      "['cartoon' 'debian' 'scope' 'photo' 'perspective' 'gcc' 'animation' 'foto'\n",
      " 'transmission' 'corresponding']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 929\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['ads' 'magazine' 'for' ... 'advertisement' 'tr' 'willing']\n",
      " ['guides' 'charter' 'magazine' ... 'magazines' 'for' 'mens']\n",
      " ['magazine' 'for' 'magazines' ... 'charter' 'bookmark' 'issue']\n",
      " ...\n",
      " ['magazine' 'magazines' 'customize' ... 'for' 'ads' 'cs']\n",
      " ['plates' 'address' 'plate' ... 'tr' 'magazines' 'korean']\n",
      " ['address' 'magazine' 'series' ... 'issue' 'plate' 'korean']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['ads' 'magazine' 'for' 'address' 'plates' 'plate' 'magazines'\n",
      " 'advertisement' 'tr' 'willing']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([5.6093, 3.2388, 3.2177, 3.1525, 2.2565, 1.9898, 1.9310, 1.8162, 1.6897,\n",
      "        1.6455], device='cuda:0')\n",
      "['magazine' 'magazines' 'for' 'address' 'plates' 'tr' 'plate' 'charter'\n",
      " 'issue' 'willing']\n",
      "tensor([-2.3437, -1.9870, -1.7314, -1.5311, -1.4880, -1.3149, -1.2783, -1.2021,\n",
      "        -1.1961, -1.1350], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([161132.0000, 161018.9062,  93269.7266,  77429.2109,  76872.6016,\n",
      "         54187.5312,  38794.8008,  35622.1992,  33416.1680,  29663.9316],\n",
      "       device='cuda:0')\n",
      "['ea' 'magazines' 'bookmark' 'plates' 'address' 'pittsburgh' 'plate'\n",
      " 'magazine' 'xbox' 'customize']\n",
      "tensor([-2.3437, -1.9870, -1.7314, -1.5311, -1.4880, -1.3149, -1.2783, -1.2021,\n",
      "        -1.1961, -1.1350], device='cuda:0')\n",
      "['proposal' 'proposals' 'crack' 'afternoon' 'foto' 'nevada' 'terminal'\n",
      " 'empty' 'diego' 'photo']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 949\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['for' 'from' 'coastal' ... 'with' 'or' 'by']\n",
      " ['for' 'from' 'with' ... 'top' 'coastal' 'qty']\n",
      " ['for' 'from' 'tiny' ... 'misc' 'dog' 'ma']\n",
      " ...\n",
      " ['for' 'gun' 'qty' ... 'guard' 'misc' 'stock']\n",
      " ['for' 'from' 'qty' ... 'with' 'misc' 'ft']\n",
      " ['for' 'from' 'qty' ... 'pic' 'coastal' 'misc']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['for' 'from' 'coastal' 'qty' 'top' 'cm' 'tiny' 'with' 'or' 'by']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([19.8041,  7.8632,  2.5382,  2.2953,  2.1285,  1.8646,  1.5577,  1.4832,\n",
      "         1.2939,  1.1511], device='cuda:0')\n",
      "['for' 'from' 'tiny' 'with' 'qty' 'coastal' 'top' 'cm' 'misc' 'cute']\n",
      "tensor([-2.3769, -1.7494, -1.7309, -1.5519, -1.4839, -1.3041, -1.2785, -1.1973,\n",
      "        -1.1961, -1.1188], device='cuda:0')\n",
      "['guinea' 'fucking' 'dog' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([43572.8633, 26367.6504, 25161.8691, 24242.4316, 22903.7695, 22578.3398,\n",
      "        22181.2539, 19027.2891, 12804.8154, 12320.2383], device='cuda:0')\n",
      "['navy' 'for' 'ocean' 'coastal' 'farm' 'edge' 'sea' 'ear' 'cute' 'from']\n",
      "tensor([-2.3769, -1.7494, -1.7309, -1.5519, -1.4839, -1.3041, -1.2785, -1.1973,\n",
      "        -1.1961, -1.1188], device='cuda:0')\n",
      "['phones' 'hands' 'ipod' 'plastic' 'heating' 'portfolio' 'phone' 'nokia'\n",
      " 'ringtones' 'chairs']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 283\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['pair' 'winners' 'double' ... 'wealth' 'sexual' 'worth']\n",
      " ['winners' 'pair' 'twin' ... 'standing' 'months' 'comparison']\n",
      " ['pair' 'winners' 'serious' ... 'sexual' 'sweet' 'lawrence']\n",
      " ...\n",
      " ['gun' 'sexcam' 'winners' ... 'serial' 'expensive' 'good']\n",
      " ['pair' 'twin' 'differences' ... 'worth' 'comparison' 'show']\n",
      " ['winners' 'pair' 'twin' ... 'worth' 'offering' 'standing']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['pair' 'winners' 'double' 'twin' 'several' 'looked' 'standing' 'wealth'\n",
      " 'sexual' 'worth']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([4.2284, 1.7286, 1.2000, 1.1287, 1.0995, 1.0238, 0.9704, 0.9623, 0.8775,\n",
      "        0.8701], device='cuda:0')\n",
      "['winners' 'pair' 'wealth' 'winning' 'twin' 'severe' 'serious' 'worth'\n",
      " 'standing' 'sexual']\n",
      "tensor([-2.3637, -2.0072, -1.7554, -1.5908, -1.4883, -1.3127, -1.2052, -1.1951,\n",
      "        -1.1354, -1.1349], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'detail' 'seal'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([90052.7734, 74712.6484, 70053.0156, 45813.3047, 41533.6484, 36056.3203,\n",
      "        33601.0117, 31353.4570, 30020.9121, 29363.3027], device='cuda:0')\n",
      "['winners' 'dates' 'cincinnati' 'parties' 'roses' 'eating' 'recipes'\n",
      " 'pittsburgh' 'foods' 'phentermine']\n",
      "tensor([-2.3637, -2.0072, -1.7554, -1.5908, -1.4883, -1.3127, -1.2052, -1.1951,\n",
      "        -1.1354, -1.1349], device='cuda:0')\n",
      "['pst' 'line' 'unix' 'css' 'ray' 'xml' 'asp' 'plug' 'met' 'chip']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1780\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['construction' 'case' 'external' ... 'housing' 'electrical' 'finished']\n",
      " ['engineers' 'case' 'couples' ... 'construction' 'cartridge' 'employees']\n",
      " ['l' 'finished' 'case' ... 'electrical' 'residential' 'ear']\n",
      " ...\n",
      " ['cartridge' 'gun' 'case' ... 'used' 'replacement' 'motor']\n",
      " ['case' 'housing' 'apartments' ... 'l' 'electrical' 'cartridge']\n",
      " ['factory' 'match' 'housing' ... 'campus' 'manufacturer' 'cartridge']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['construction' 'case' 'external' 'factory' 'route' 'match' 'l' 'housing'\n",
      " 'electrical' 'finished']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([2.8638, 2.3837, 2.0794, 2.0601, 1.7441, 1.5654, 1.5180, 1.4589, 1.1800,\n",
      "        1.1743], device='cuda:0')\n",
      "['case' 'cartridge' 'l' 'external' 'electrical' 'factory' 'match'\n",
      " 'housing' 'manufacturer' 'cases']\n",
      "tensor([-2.3540, -1.9670, -1.7290, -1.5901, -1.4831, -1.3112, -1.1825, -1.1807,\n",
      "        -1.1341, -1.1219], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'detail' 'seal'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1228981.7500,  777239.6875,  410640.7188,  350674.0000,  244193.6562,\n",
      "         240016.0469,  218877.5469,  218075.2969,  172985.1094,  165707.7031],\n",
      "       device='cuda:0')\n",
      "['apartments' 'buildings' 'houses' 'factory' 'colleges' 'engineers'\n",
      " 'mississippi' 'churches' 'universities' 'homes']\n",
      "tensor([-2.3540, -1.9670, -1.7290, -1.5901, -1.4831, -1.3112, -1.1825, -1.1807,\n",
      "        -1.1341, -1.1219], device='cuda:0')\n",
      "['disclosure' 'him' 'formal' 'target' 'common' 'newsletter' 'tranny'\n",
      " 'organic' 'thompson' 'scope']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2180\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['oh' 'size' 'dvd' ... 'for' 'epson' 'length']\n",
      " ['german' 'oh' 'polish' ... 'dvd' 'length' 'swiss']\n",
      " ['german' 'dog' 'pets' ... 'dogs' 'size' 'ha']\n",
      " ...\n",
      " ['german' 'size' 'for' ... 'full' 'includes' 'epson']\n",
      " ['size' 'length' 'oh' ... 'longer' 'ha' 'german']\n",
      " ['oh' 'size' 'dvd' ... 'epson' 'polish' 'bush']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['oh' 'size' 'dvd' 'foundation' 'cd' 'longer' 'german' 'for' 'epson'\n",
      " 'length']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([2.6566, 2.5668, 2.1798, 1.6407, 1.6028, 1.3789, 1.3770, 1.3539, 1.2366,\n",
      "        1.1662], device='cuda:0')\n",
      "['oh' 'size' 'german' 'polish' 'dvd' 'for' 'ha' 'length' 'printable' 'er']\n",
      "tensor([-2.2788, -1.7474, -1.5898, -1.4664, -1.3032, -1.2596, -1.2038, -1.2002,\n",
      "        -1.1962, -1.1143], device='cuda:0')\n",
      "['guinea' 'fucking' 'ray' 'python' 'fish' 'pair' 'dog' 'eagle' 'detail'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([88006.9531, 81386.2656, 75396.9062, 62036.5430, 58733.7500, 52885.7812,\n",
      "        50868.1758, 48740.1367, 47666.1250, 45577.6094], device='cuda:0')\n",
      "['dvds' 'attorneys' 'bedroom' 'pittsburgh' 'pants' 'lesbian' 'catalog'\n",
      " 'printable' 'recipes' 'supplements']\n",
      "tensor([-2.2788, -1.7474, -1.5898, -1.4664, -1.3032, -1.2596, -1.2038, -1.2002,\n",
      "        -1.1962, -1.1143], device='cuda:0')\n",
      "['offering' 'recording' 'banner' 'christ' 'wind' 'joy' 'angeles' 'header'\n",
      " 'queen' 'planet']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1323\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002]],\n",
      "       device='cuda:0')\n",
      "[['try' 'tel' 'still' ... 'te' 'long' 'huge']\n",
      " ['inch' 'try' 'huge' ... 'ski' 'trembl' 'giant']\n",
      " ['inch' 'still' 'try' ... 'long' 'penis' 'tell']\n",
      " ...\n",
      " ['inch' 'tel' 'still' ... 'const' 'tip' 'auctions']\n",
      " ['inch' 'try' 'still' ... 'tell' 'itself' 'total']\n",
      " ['try' 'tel' 'inch' ... 'itself' 'wells' 'total']]\n",
      "torch.Size([10]) (10,)\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002], device='cuda:0') ['try' 'tel' 'still' 'inch' 'itself' 'tell' 'total' 'te' 'long' 'huge']\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([4.7293, 3.8820, 3.2250, 3.1327, 2.2627, 2.1677, 2.1365, 1.8776, 1.7964,\n",
      "        1.4417], device='cuda:0')\n",
      "['try' 'inch' 'still' 'tel' 'te' 'itself' 'total' 'huge' 'tell' 'long']\n",
      "tensor([-2.3851, -2.0289, -1.7537, -1.5710, -1.4886, -1.3145, -1.2062, -1.1939,\n",
      "        -1.1485, -1.1355], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'detail' 'pair'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([88045.5469, 81992.3281, 80245.0547, 68190.8438, 52778.8086, 45591.5273,\n",
      "        31708.9531, 28308.5996, 27118.3438, 25409.7012], device='cuda:0')\n",
      "['usc' 'community' 'teens' 'pittsburgh' 'toronto' 'philadelphia'\n",
      " 'spanking' 'try' 'ea' 'citysearch']\n",
      "tensor([-2.3851, -2.0289, -1.7537, -1.5710, -1.4886, -1.3145, -1.2062, -1.1939,\n",
      "        -1.1485, -1.1355], device='cuda:0')\n",
      "['shaved' 'eye' 'reception' 'battery' 'preview' 'face' 'eyes' 'packaging'\n",
      " 'shopping' 'folder']\n"
     ]
    }
   ],
   "source": [
    "# subtract from default, label, and print trends\n",
    "text_probs_altered.shape\n",
    "\n",
    "# selected_vocab = all_imagenet_class_names\n",
    "selected_vocab = larger_vocab\n",
    "\n",
    "# switch to datacomp eval??\n",
    "# we run this for each feature over all of imagenet eval and create average absolute/ratio\n",
    "# difference vectors\n",
    "for j, text_probs_altered in enumerate(text_probs_altered_list):\n",
    "    print(f\"{'============================================'*2}\\n\\nFor Feature {random_feat_idxs[j]}\")\n",
    "    logit_diff = text_probs_altered - text_probs_default\n",
    "#     print(f\"logit_diff.shape: {logit_diff.shape}\")\n",
    "    logit_diff_aggregate = logit_diff.sum(dim=0)\n",
    "#     print(f\"logit_diff_aggregate.shape: {logit_diff_aggregate.shape}\")\n",
    "    \n",
    "    logit_ratio = text_probs_altered/text_probs_default\n",
    "    logit_ratio_aggregate = logit_ratio.mean(dim=0)\n",
    "    \n",
    "    print(f\"text_probs_altered.softmax(): {text_probs_altered.softmax(1).shape}\")\n",
    "    text_probs_altered_softmax = text_probs_altered.softmax(1)\n",
    "    vals_softmax, idxs_softmax = torch.topk(text_probs_altered_softmax,k=10)\n",
    "    \n",
    "    print(f\"\\nSoftmax Over {text_probs_altered.shape[0]} Images:\\n{vals_softmax}\")\n",
    "    print(np.array(selected_vocab)[idxs_softmax.cpu()])\n",
    "    for i in range(vals_softmax.shape[0]):\n",
    "        print(vals_softmax[i], \"\\n\", np.array(selected_vocab)[idxs_softmax.cpu()][i])\n",
    "        break\n",
    "    \n",
    "    vals_agg, idxs_agg = torch.topk(logit_diff_aggregate,k=10)\n",
    "    vals_least_agg, idxs_least_agg = torch.topk(logit_diff_aggregate,k=10,largest=False)\n",
    "    \n",
    "    ratios_agg, ratios_idxs_agg = torch.topk(logit_ratio_aggregate,k=10)\n",
    "    ratios_least_agg, ratios_idxs_least_agg = torch.topk(logit_ratio_aggregate,k=10,largest=False)\n",
    "    \n",
    "    vals, idxs = torch.topk(logit_diff,k=5)\n",
    "    vals_least, idxs_least = torch.topk(logit_diff,k=5,largest=False)\n",
    "    \n",
    "    ratios, ratios_idxs = torch.topk(logit_ratio,k=5)\n",
    "    ratios_least, ratios_idxs_least = torch.topk(logit_ratio,k=5,largest=False)\n",
    "    \n",
    "    print(f\"\\nMost Changed, by Absolute Diff Over {logit_diff.shape[0]} Images:\\n{vals_agg}\")\n",
    "    print(np.array(selected_vocab)[idxs_agg.cpu()])\n",
    "    print(vals_least_agg)\n",
    "    print(np.array(selected_vocab)[idxs_least_agg.cpu()])\n",
    "    \n",
    "    print(f\"\\nMost Changed, by Ratio Over {logit_diff.shape[0]} Images:\")\n",
    "    print(ratios_agg)\n",
    "    print(np.array(selected_vocab)[ratios_idxs_agg.cpu()])\n",
    "    print(vals_least_agg)\n",
    "    print(np.array(selected_vocab)[ratios_idxs_least_agg.cpu()])\n",
    "        \n",
    "    \n",
    "#     for i in range(logit_diff.shape[0]):\n",
    "# #         print(f\"\\nImage {i} ========================\\nMost Changed, by Absolute Diff\\n:{vals[i]}\")\n",
    "# #         print(np.array(selected_vocab)[idxs.cpu()][i])\n",
    "# #         print(vals_least[i])\n",
    "# #         print(np.array(selected_vocab)[idxs_least.cpu()][i])\n",
    "#         print(f\"\\nImage {i} Most Changed, by Ratio:\")\n",
    "#         print(ratios[i])\n",
    "#         print(np.array(selected_vocab)[ratios_idxs.cpu()][i])\n",
    "#         print(ratios_least[i])\n",
    "#         print(np.array(selected_vocab)[ratios_idxs_least.cpu()][i])\n",
    "        \n",
    "# #         text = tokenizer(np.array(selected_vocab)[ratios_idxs.cpu()][i])\n",
    "# #         text_least = tokenizer(np.array(selected_vocab)[ratios_idxs_least.cpu()][i])\n",
    "# #         text_features = og_model.encode_text(text.cuda())\n",
    "# #         text_features_least = og_model.encode_text(text_least.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract from default, label, and print trends\n",
    "text_probs_altered.shape\n",
    "\n",
    "# selected_vocab = all_imagenet_class_names\n",
    "selected_vocab = larger_vocab\n",
    "\n",
    "cov_stuff_avgs = []\n",
    "\n",
    "# switch to datacomp eval??\n",
    "# we run this for each feature over all of imagenet eval and create average absolute/ratio\n",
    "# difference vectors\n",
    "per_feat_avg_vectors = []\n",
    "for j, text_probs_altered in enumerate(text_probs_altered_list):\n",
    "    print(f\"\\n\\nFor Feature {random_feat_idxs[j]}\")\n",
    "    print(f\"\\n\\nFor Feature {random_feat_idxs[j]}\")\n",
    "    logit_diff = text_probs_altered - text_probs_default\n",
    "    logit_ratio = text_probs_altered/text_probs_default\n",
    "    \n",
    "    vals, idxs = torch.topk(logit_diff,k=5)\n",
    "    vals_least, idxs_least = torch.topk(logit_diff,k=5,largest=False)\n",
    "    \n",
    "    ratios, ratios_idxs = torch.topk(logit_ratio,k=5)\n",
    "    ratios_least, ratios_idxs_least = torch.topk(logit_ratio,k=5,largest=False)\n",
    "    \n",
    "    feat_avg_vectors = []\n",
    "    for i in range(logit_diff.shape[0]):\n",
    "#         print(f\"\\nImage {i} ========================\\nMost Changed, by Absolute Diff\\n:{vals[i]}\")\n",
    "#         print(np.array(all_imagenet_class_names)[idxs.cpu()][i])\n",
    "#         print(vals_least[i])\n",
    "#         print(np.array(all_imagenet_class_names)[idxs_least.cpu()][i])\n",
    "        \n",
    "        print(\"\\nMost Changed, by Ratio:\")\n",
    "        print(ratios[i])\n",
    "        print(np.array(selected_vocab)[ratios_idxs.cpu()][i])\n",
    "#         print(ratios_least[i])\n",
    "#         print(np.array(selected_vocab)[ratios_idxs_least.cpu()][i])\n",
    "        \n",
    "        text = tokenizer(np.array(selected_vocab)[ratios_idxs.cpu()][i])\n",
    "#         text_least = tokenizer(np.array(selected_vocab)[ratios_idxs_least.cpu()][i])\n",
    "        text_features = og_model.encode_text(text.cuda())\n",
    "        cov_over_images.append(text_features)\n",
    "#         text_features_least = og_model.encode_text(text_least.cuda())\n",
    "        print(torch.tril(torch.cov(text_features), diagonal=-1))\n",
    "        print(torch.tril(torch.cov(text_features), diagonal=-1).sum()/10)\n",
    "#         cov_stuff = torch.tril(torch.cov(text_features), diagonal=-1).sum()/10\n",
    "#         cov_stuff_least = torch.tril(torch.cov(text_features_least), diagonal=-1).sum()/10\n",
    "    print(torch.tril(torch.cov(torch.cat(cov_over_images)), diagonal=-1).shape)\n",
    "    n = torch.tril(torch.cov(torch.cat(cov_over_images)), diagonal=-1).shape[0]\n",
    "    num_elements = (n**2)/2 - n\n",
    "    cov_stuff = torch.tril(torch.cov(torch.cat(cov_over_images)), diagonal=-1).sum()/num_elements\n",
    "    cov_stuff_avgs.append(cov_stuff)\n",
    "#         cov_stuff_avgs_least.append(cov_stuff_least)\n",
    "    if j > 10:\n",
    "        break\n",
    "print(torch.tensor(cov_stuff_avgs).mean())\n",
    "# print(torch.tensor(cov_stuff_avgs_least).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_imgnet_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "feat_autolabels_default = defaultdict(Counter)\n",
    "for i in range(text_probs_default.shape[0]):\n",
    "    vals, idxs = torch.topk(text_probs_default[i],k=top_k_imgnet_labels)\n",
    "    print(i)\n",
    "    for k, idx in enumerate(idxs):\n",
    "        feat_autolabels_default[i][all_imagenet_class_names[idx]] += vals[k]\n",
    "        print(\"\\t\", all_imagenet_class_names[idx])\n",
    "feat_autolabels_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract from default, label, and print trends\n",
    "text_probs_altered.shape\n",
    "\n",
    "for text_probs_altered in text_probs_altered_list:\n",
    "    logit_diff = text_probs_altered - text_probs_default\n",
    "    print(logit_diff)\n",
    "    vals, idxs = torch.topk(logit_diff,k=5)\n",
    "    print(vals, np.array(all_imagenet_class_names[idxs])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imagenet_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "feat_autolabels_altered_list = []\n",
    "for text_probs_altered in text_probs_altered_list:\n",
    "    feat_autolabels_altered = defaultdict(Counter)\n",
    "    for i in range(text_probs_altered.shape[0]):\n",
    "        vals, idxs = torch.topk(text_probs_altered[i],k=top_k_imgnet_labels)\n",
    "#         print(i)\n",
    "        for k, idx in enumerate(idxs):\n",
    "            feat_autolabels_altered[i][all_imagenet_class_names[idx]] += vals[k]\n",
    "#             print(\"\\t\", all_imagenet_class_names[idx])\n",
    "    feat_autolabels_altered_list.append(feat_autolabels_altered)\n",
    "\n",
    "start_idx = 9\n",
    "end_idx = 10\n",
    "    \n",
    "h = 0\n",
    "for key in feat_autolabels_default:\n",
    "    print(f\"\\nfeat_autolabels_default img {key}:\\n {feat_autolabels_default[key]}\\n\")\n",
    "    h += 1\n",
    "    if h > end_idx:\n",
    "        break\n",
    "for i, f_a_a in enumerate(feat_autolabels_altered_list):\n",
    "    print(\"============= feature number \", i, \"====================\")\n",
    "    h = 0\n",
    "    for key in range(start_idx, end_idx):\n",
    "#         print(\"\\n\", key)\n",
    "#         for item in f_a_a[key]:\n",
    "#             print(\"\\t\", item, f_a_a[key][item].cpu().item())\n",
    "        print(f\"\\nf_a_a img {key}:\\n {f_a_a[key]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(text_probs_default.shape[0]):\n",
    "    vals, idxs = torch.topk(text_probs_default[i],k=1000)\n",
    "    print(i, ind_to_name[str(idxs[0].cpu().item())][1])\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "#     ax.xaxis.set_ticks((1000))\n",
    "#     ax.set_xticks(list(range(1000)), [ind_to_name[str(idxs[idx].cpu().item())][1] for idx in idxs])\n",
    "    plt.bar(idxs.cpu(), vals.cpu(), width=5)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch_images[2].cpu().permute((1,2,0)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def get_heatmap(\n",
    "          image,\n",
    "          model,\n",
    "          sparse_autoencoder,\n",
    "          feature_id,\n",
    "): \n",
    "    image = image.to(cfg.device)\n",
    "    _, cache = model.run_with_cache(image.unsqueeze(0))\n",
    "\n",
    "    post_reshaped = einops.rearrange(cache[sparse_autoencoder.cfg.hook_point], \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "    # Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n",
    "    sae_in =  post_reshaped - sparse_autoencoder.b_dec # Remove decoder bias as per Anthropic\n",
    "    print(f\"sae_in.shape: {sae_in.shape}\")\n",
    "    acts = einops.einsum(\n",
    "            sae_in,\n",
    "            sparse_autoencoder.W_enc[:, feature_id],\n",
    "            \"x d_in, d_in -> x\",\n",
    "        )\n",
    "    return acts \n",
    "     \n",
    "def image_patch_heatmap(activation_values,image_size=224, pixel_num=14):\n",
    "    activation_values = activation_values.detach().cpu().numpy()\n",
    "    activation_values = activation_values[1:]\n",
    "    activation_values = activation_values.reshape(pixel_num, pixel_num)\n",
    "\n",
    "    # Create a heatmap overlay\n",
    "    heatmap = np.zeros((image_size, image_size))\n",
    "    patch_size = image_size // pixel_num\n",
    "\n",
    "    for i in range(pixel_num):\n",
    "        for j in range(pixel_num):\n",
    "            heatmap[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size] = activation_values[i, j]\n",
    "\n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "grid_size = 1\n",
    "fig, axs = plt.subplots(int(np.ceil(len(images)/grid_size)), grid_size, figsize=(15, 15))\n",
    "name=  f\"Category: uhh,  Feature: {0}\"\n",
    "fig.suptitle(name)#, y=0.95)\n",
    "for ax in axs.flatten():\n",
    "    ax.axis('off')\n",
    "complete_bid = []\n",
    "\n",
    "heatmap = get_heatmap(batch_images[2], model,sparse_autoencoder, 10000)\n",
    "heatmap = image_patch_heatmap(heatmap, pixel_num=224//cfg.patch_size)\n",
    "\n",
    "display = batch_images[2].cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "has_zero = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(display)\n",
    "plt.imshow(heatmap, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(display)\n",
    "plt.imshow(heatmap, alpha=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
