{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating your SAE\n",
    "\n",
    "Code based off Rob Graham's ([themachinefan](https://github.com/themachinefan)) SAE evaluation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/ViT-Prisma/src/vit_prisma/sae/evals'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_per_buffer (millions): 0.032\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 158691\n",
      "Total training images: 13000000\n",
      "Total wandb updates: 15869\n",
      "Expansion factor: 16\n",
      "n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 158 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Gradient clipping with max_norm=1.0\n",
      "Using SAE initialization method: encoder_transpose_decoder\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from vit_prisma.sae.config import VisionModelSAERunnerConfig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig(VisionModelSAERunnerConfig):\n",
    "    sae_path: str = '/workspace/sae_checkpoints/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-0.0001/n_images_2600058.pt'\n",
    "    model_name: str = \"open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "    model_type: str =  \"clip\"\n",
    "    patch_size: str = 32\n",
    "\n",
    "    dataset_path = \"/workspace\"\n",
    "    dataset_train_path: str = \"/workspace/ILSVRC/Data/CLS-LOC/train\"\n",
    "    dataset_val_path: str = \"/workspace/ILSVRC/Data/CLS-LOC/val\"\n",
    "\n",
    "    verbose: bool = True\n",
    "\n",
    "    device: bool = 'cuda'\n",
    "\n",
    "    eval_max: int = 50_000 # 50_000\n",
    "    batch_size: int = 32\n",
    "\n",
    "    # make the max image output folder a subfolder of the sae path\n",
    "\n",
    "\n",
    "    @property\n",
    "    def max_image_output_folder(self) -> str:\n",
    "        # Get the base directory of sae_checkpoints\n",
    "        sae_base_dir = os.path.dirname(os.path.dirname(self.sae_path))\n",
    "        \n",
    "        # Get the name of the original SAE checkpoint folder\n",
    "        sae_folder_name = os.path.basename(os.path.dirname(self.sae_path))\n",
    "        \n",
    "        # Create a new folder path in sae_checkpoints/images with the original name\n",
    "        output_folder = os.path.join(sae_base_dir, 'max_images', sae_folder_name)\n",
    "        output_folder = os.path.join(output_folder, f\"layer_{self.hook_point_layer}\") # Add layer number\n",
    "\n",
    "        \n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        return output_folder\n",
    "\n",
    "cfg = EvalConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7ae7318a3d30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "Official model name open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "Converting OpenCLIP weights\n",
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "visual projection shape torch.Size([768, 512])\n",
      "Setting center_writing_weights to False for OpenCLIP\n",
      "Setting fold_ln to False for OpenCLIP\n",
      "Loaded pretrained model open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from vit_prisma.models.base_vit import HookedViT\n",
    "\n",
    "model_name = \"open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "model = HookedViT.from_pretrained(model_name, is_timm=False, is_clip=True).to(cfg.device)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import vit_prisma\n",
    "# importlib.reload(vit_prisma.dataloaders.imagenet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation data length: 50000\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "import open_clip\n",
    "from vit_prisma.utils.data_utils.imagenet_utils import setup_imagenet_paths\n",
    "from vit_prisma.dataloaders.imagenet_dataset import get_imagenet_transforms_clip, ImageNetValidationDataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "og_model_name = \"hf-hub:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "og_model, _, preproc = open_clip.create_model_and_transforms(og_model_name)\n",
    "processor = preproc\n",
    "\n",
    "size=224\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                     std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "    \n",
    "imagenet_paths = setup_imagenet_paths(cfg.dataset_path)\n",
    "imagenet_paths[\"train\"] = \"/workspace/ILSVRC/Data/CLS-LOC/train\"\n",
    "imagenet_paths[\"val\"] = \"/workspace/ILSVRC/Data/CLS-LOC/val\"\n",
    "imagenet_paths[\"val_labels\"] = \"/workspace/LOC_val_solution.csv\"\n",
    "imagenet_paths[\"label_strings\"] = \"/workspace/LOC_synset_mapping.txt\"\n",
    "print()\n",
    "train_data = torchvision.datasets.ImageFolder(cfg.dataset_train_path, transform=data_transforms)\n",
    "val_data = ImageNetValidationDataset(cfg.dataset_val_path, \n",
    "                                imagenet_paths['label_strings'], \n",
    "                                imagenet_paths['val_labels'], \n",
    "                                data_transforms,\n",
    "                                return_index=True,\n",
    ")\n",
    "val_data_visualize = ImageNetValidationDataset(cfg.dataset_val_path, \n",
    "                                imagenet_paths['label_strings'], \n",
    "                                imagenet_paths['val_labels'],\n",
    "                                torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),]), return_index=True)\n",
    "\n",
    "print(f\"Validation data length: {len(val_data)}\") if cfg.verbose else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.sae.training.activations_store import VisionActivationsStore\n",
    "# import dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# activations_loader = VisionActivationsStore(cfg, model, train_data, eval_dataset=val_data)\n",
    "val_dataloader = DataLoader(val_data, batch_size=cfg.batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained SAE to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_activation_fn received: activation_fn=relu, kwargs={}\n",
      "n_tokens_per_buffer (millions): 0.032\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 158691\n",
      "Total training images: 13000000\n",
      "Total wandb updates: 1586\n",
      "Expansion factor: 64\n",
      "n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 158 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Gradient clipping with max_norm=1.0\n",
      "Using SAE initialization method: encoder_transpose_decoder\n",
      "get_activation_fn received: activation_fn=relu, kwargs={}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseAutoencoder(\n",
       "  (hook_sae_in): HookPoint()\n",
       "  (hook_hidden_pre): HookPoint()\n",
       "  (hook_hidden_post): HookPoint()\n",
       "  (hook_sae_out): HookPoint()\n",
       "  (activation_fn): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vit_prisma.sae.sae import SparseAutoencoder\n",
    "sparse_autoencoder = SparseAutoencoder(cfg).load_from_pretrained(\"/workspace/sae_checkpoints/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-0.0001/n_images_2600058.pt\")\n",
    "sparse_autoencoder.to(cfg.device)\n",
    "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Labeling AutoInterp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_imagenet_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.dataloaders.imagenet_dataset import get_imagenet_index_to_name\n",
    "ind_to_name = get_imagenet_index_to_name()\n",
    "\n",
    "all_imagenet_class_names = []\n",
    "for i in range(len(ind_to_name)):\n",
    "    all_imagenet_class_names.append(ind_to_name[str(i)][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/sae_checkpoints/max_images/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-0.0001/layer_9'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.max_image_output_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_replacement_hook_curry(feat_idx: int = 0, feat_activ: float = 1.0):\n",
    "    def standard_replacement_hook(activations: torch.Tensor, hook):\n",
    "        activations = sparse_autoencoder.forward(activations)[0].to(activations.dtype)\n",
    "        feature_acts = sparse_autoencoder.encode_standard(activations)\n",
    "\n",
    "        # in all batches and patches, set feature w idx idx to 0\n",
    "        print(f\"feature_acts[:,:,idx].shape: {feature_acts[:,:,feat_idx].shape}\")\n",
    "        print(f\"feat activ: {feature_acts[:,:,feat_idx]}\")\n",
    "        feature_acts[:,:,feat_idx] *= feat_activ\n",
    "        print(f\"feat activ: {feature_acts[:,:,feat_idx]}\")\n",
    "        print(f\"feat activ: {feature_acts.shape}\")\n",
    "        print(f\"feat activ: {feature_acts}\")\n",
    "        print(\"feature_acts[:,:,idx].sum(): (should be batch size x len seq x feat val)\", feature_acts[:,:,feat_idx].sum())\n",
    "        sae_out = sparse_autoencoder.hook_sae_out(\n",
    "            einops.einsum(\n",
    "                feature_acts,\n",
    "                sparse_autoencoder.W_dec,\n",
    "                \"... d_sae, d_sae d_in -> ... d_in\",\n",
    "            )\n",
    "            + sparse_autoencoder.b_dec\n",
    "        )\n",
    "        \n",
    "        print(f\"sae_out.shape: {sae_out.shape}\")\n",
    "        print(f\"sae_out: {sae_out}\")\n",
    "\n",
    "        # allows normalization. Possibly identity if no normalization\n",
    "        sae_out = sparse_autoencoder.run_time_activation_norm_fn_out(sae_out)\n",
    "        return sae_out\n",
    "    return standard_replacement_hook\n",
    "\n",
    "\n",
    "def steering_hook_fn(\n",
    "    activations, cfg, hook, sae, steering_indices, steering_strength=1.0, mean_ablation_values=None, include_error=False\n",
    "\n",
    "):\n",
    "    sae.to(activations.device)\n",
    "\n",
    "\n",
    "    sae_input = activations.clone()\n",
    "    sae_output, feature_activations, *data = sae(sae_input)\n",
    "    \n",
    "    steered_feature_activations = feature_activations.clone()\n",
    "    \n",
    "    steered_feature_activations[:, :, steering_indices] = steering_strength\n",
    "\n",
    "    steered_sae_out = einops.einsum(\n",
    "                steered_feature_activations,\n",
    "                sae.W_dec,\n",
    "                \"... d_sae, d_sae d_in -> ... d_in\",\n",
    "            ) + sae.b_dec\n",
    "\n",
    "    steered_sae_out = sae.run_time_activation_norm_fn_out(steered_sae_out)\n",
    "    \n",
    "    print(steered_sae_out.shape)\n",
    "    print(steered_sae_out.shape)\n",
    "    print(f\"steering norm: {(steered_sae_out - sae_output).norm()}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    if include_error:\n",
    "        error = sae_input - sae_output\n",
    "        print(f\"error.norm(): {error.norm()}\")\n",
    "        return steered_sae_out + error\n",
    "    return steered_sae_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_feat_idxs = np.random.randint(0, high=3000, size=(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given feature, set it high/low on maxim activ. imgs and high/low on non-activ images\n",
    "# hook SAE and replace desired feature with 0 or 1 \n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_feature_activations_set_feat(\n",
    "    images: torch.Tensor,\n",
    "    model: torch.nn.Module,\n",
    "    sparse_autoencoder: torch.nn.Module,\n",
    "    encoder_weights: torch.Tensor,\n",
    "    encoder_biases: torch.Tensor,\n",
    "    feature_ids: List[int],\n",
    "    feature_categories: List[str],\n",
    "    top_k: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the highest activating tokens for given features in a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        images: Input images\n",
    "        model: The main model\n",
    "        sparse_autoencoder: The sparse autoencoder\n",
    "        encoder_weights: Encoder weights for selected features\n",
    "        encoder_biases: Encoder biases for selected features\n",
    "        feature_ids: List of feature IDs to analyze\n",
    "        feature_categories: Categories of the features\n",
    "        top_k: Number of top activations to return per feature\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping feature IDs to tuples of (top_indices, top_values)\n",
    "    \"\"\"\n",
    "    _, cache = model.run_with_cache(images, names_filter=[sparse_autoencoder.cfg.hook_point])\n",
    "#     recons_image_embeddings_feat_altered = model.run_with_hooks(\n",
    "#         images,\n",
    "#         fwd_hooks=[(\"blocks.9.hook_mlp_out\", standard_replacement_hook)],\n",
    "#     )\n",
    "    recons_image_embeddings_feat_altered_list = []\n",
    "    for idx in np.array(range(sparse_autoencoder.W_dec.shape[0]))[random_feat_idxs]:\n",
    "        print(f\"Feature: {idx} ====================\")\n",
    "        \n",
    "        steering_hook = partial(\n",
    "            steering_hook_fn,\n",
    "            cfg=cfg,\n",
    "            sae=sparse_autoencoder,\n",
    "            steering_indices=[idx],\n",
    "            steering_strength=10.0,\n",
    "            mean_ablation_values = [1.0],\n",
    "            include_error=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "        recons_image_embeddings_feat_altered = model.run_with_hooks(\n",
    "            images,\n",
    "#             fwd_hooks=[(\"blocks.9.hook_mlp_out\", standard_replacement_hook_curry(idx, 10.0))],\n",
    "            fwd_hooks=[(\"blocks.9.hook_mlp_out\", steering_hook)],\n",
    "        )\n",
    "        recons_image_embeddings_feat_altered_list.append(recons_image_embeddings_feat_altered)\n",
    "\n",
    "    \n",
    "    # output is in clip embedding space\n",
    "    recons_image_embeddings_default = model.run_with_hooks(\n",
    "        images,\n",
    "        fwd_hooks=[(\"blocks.9.hook_mlp_out\", lambda x, hook: x)],\n",
    "    )\n",
    "    \n",
    "    print(f\"recons_image_embeddings_default: {recons_image_embeddings_default}\")\n",
    "    print(f\"recons_image_embeddings_default.shape: {recons_image_embeddings_default.shape}\")\n",
    "    print(f\"recons_image_embeddings_default: {recons_image_embeddings_default.shape}\")\n",
    "\n",
    "    print(f\"recons_image_embeddings_feat_altered: {recons_image_embeddings_feat_altered}\")\n",
    "    print(f\"recons_image_embeddings_feat_altered.shape: {recons_image_embeddings_feat_altered.shape}\")\n",
    "\n",
    "    return recons_image_embeddings_feat_altered_list, recons_image_embeddings_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                            | 0/1562 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 926 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.5616455078125\n",
      "error.norm(): 3214.9658203125\n",
      "Feature: 302 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.6670227050781\n",
      "error.norm(): 3214.9658203125\n",
      "Feature: 1670 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.4660949707031\n",
      "error.norm(): 3214.9658203125\n",
      "Feature: 894 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.6549377441406\n",
      "error.norm(): 3214.9658203125\n",
      "Feature: 2293 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.53839111328125\n",
      "error.norm(): 3214.9658203125\n",
      "Feature: 2680 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.04730224609375\n",
      "error.norm(): 3214.9658203125\n",
      "Feature: 161 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.310546875\n",
      "error.norm(): 3214.9658203125\n",
      "Feature: 649 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                    | 1/1562 [00:01<28:59,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.8880310058594\n",
      "error.norm(): 3214.9658203125\n",
      "Feature: 1530 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.58062744140625\n",
      "error.norm(): 3214.9658203125\n",
      "Feature: 2816 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.75042724609375\n",
      "error.norm(): 3214.9658203125\n",
      "recons_image_embeddings_default: tensor([[ 0.0352,  0.0083, -0.0740,  ..., -0.0311,  0.0275,  0.0019],\n",
      "        [-0.0101, -0.0539, -0.0622,  ...,  0.0199, -0.0555, -0.0743],\n",
      "        [-0.0206,  0.0059, -0.0366,  ..., -0.0307,  0.0756, -0.0016],\n",
      "        ...,\n",
      "        [ 0.0099, -0.0045, -0.0059,  ..., -0.0521,  0.0647, -0.0225],\n",
      "        [-0.0422,  0.0518, -0.0482,  ...,  0.0098,  0.0418,  0.0290],\n",
      "        [-0.0411, -0.0590,  0.0014,  ..., -0.0432, -0.0089, -0.0449]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[ 0.0274,  0.0304, -0.0387,  ..., -0.0682,  0.0374, -0.0156],\n",
      "        [ 0.0089, -0.0307, -0.0381,  ..., -0.0450, -0.0857, -0.0783],\n",
      "        [-0.0216, -0.0062, -0.0078,  ..., -0.0536,  0.0547, -0.0182],\n",
      "        ...,\n",
      "        [ 0.0144, -0.0009, -0.0078,  ..., -0.0652,  0.0742, -0.0500],\n",
      "        [-0.0381,  0.0762, -0.0383,  ..., -0.0262,  0.0159,  0.0017],\n",
      "        [-0.0476, -0.0381,  0.0228,  ..., -0.0727, -0.0255, -0.0713]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 926 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.74261474609375\n",
      "error.norm(): 2310.85302734375\n",
      "Feature: 302 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.6986083984375\n",
      "error.norm(): 2310.85302734375\n",
      "Feature: 1670 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.5472106933594\n",
      "error.norm(): 2310.85302734375\n",
      "Feature: 894 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.8796691894531\n",
      "error.norm(): 2310.85302734375\n",
      "Feature: 2293 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.9096984863281\n",
      "error.norm(): 2310.85302734375\n",
      "Feature: 2680 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.21380615234375\n",
      "error.norm(): 2310.85302734375\n",
      "Feature: 161 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.25970458984375\n",
      "error.norm(): 2310.85302734375\n",
      "Feature: 649 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                                                   | 2/1562 [00:01<22:27,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.50457763671875\n",
      "error.norm(): 2310.85302734375\n",
      "Feature: 1530 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.7679748535156\n",
      "error.norm(): 2310.85302734375\n",
      "Feature: 2816 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.0284729003906\n",
      "error.norm(): 2310.85302734375\n",
      "recons_image_embeddings_default: tensor([[ 0.0146, -0.0148, -0.0460,  ...,  0.0118,  0.0082,  0.0083],\n",
      "        [-0.0018,  0.0212, -0.0113,  ...,  0.0519, -0.0585, -0.0361],\n",
      "        [-0.0171, -0.0393, -0.0432,  ...,  0.0160,  0.0028,  0.0136],\n",
      "        ...,\n",
      "        [-0.0224, -0.0082, -0.0361,  ..., -0.0352,  0.0784,  0.0265],\n",
      "        [-0.0062,  0.0247, -0.0572,  ...,  0.0121, -0.0083,  0.0222],\n",
      "        [-0.0130,  0.0321, -0.0363,  ...,  0.0437,  0.0279, -0.0109]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[ 0.0222, -0.0071, -0.0306,  ..., -0.0070,  0.0003, -0.0244],\n",
      "        [-0.0141,  0.0418, -0.0232,  ...,  0.0027, -0.0756, -0.0567],\n",
      "        [-0.0147,  0.0095, -0.0316,  ...,  0.0033, -0.0127, -0.0333],\n",
      "        ...,\n",
      "        [-0.0089,  0.0047, -0.0207,  ..., -0.0672,  0.0471,  0.0165],\n",
      "        [ 0.0140,  0.0319, -0.0600,  ..., -0.0010, -0.0151, -0.0003],\n",
      "        [ 0.0082,  0.0446, -0.0364,  ...,  0.0130,  0.0283, -0.0426]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 926 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.3521423339844\n",
      "error.norm(): 2365.204345703125\n",
      "Feature: 302 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.6650695800781\n",
      "error.norm(): 2365.204345703125\n",
      "Feature: 1670 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.5699768066406\n",
      "error.norm(): 2365.204345703125\n",
      "Feature: 894 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.7852478027344\n",
      "error.norm(): 2365.204345703125\n",
      "Feature: 2293 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.8359375\n",
      "error.norm(): 2365.204345703125\n",
      "Feature: 2680 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.1692199707031\n",
      "error.norm(): 2365.204345703125\n",
      "Feature: 161 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 397.95245361328125\n",
      "error.norm(): 2365.204345703125\n",
      "Feature: 649 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.607421875\n",
      "error.norm(): 2365.204345703125\n",
      "Feature: 1530 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                                                   | 3/1562 [00:02<19:54,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.5960388183594\n",
      "error.norm(): 2365.204345703125\n",
      "Feature: 2816 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.3450622558594\n",
      "error.norm(): 2365.204345703125\n",
      "recons_image_embeddings_default: tensor([[ 0.0294,  0.0383,  0.0048,  ..., -0.0036,  0.0256,  0.0279],\n",
      "        [ 0.0004,  0.0353, -0.0868,  ..., -0.0146,  0.0002,  0.0059],\n",
      "        [ 0.0709, -0.0185, -0.0175,  ...,  0.0050,  0.0293,  0.0257],\n",
      "        ...,\n",
      "        [-0.0168, -0.0003, -0.0274,  ..., -0.0302,  0.0601, -0.0477],\n",
      "        [ 0.0075,  0.0213, -0.0235,  ..., -0.0346,  0.0216,  0.0487],\n",
      "        [ 0.0059, -0.0119, -0.0019,  ...,  0.0249, -0.0424,  0.0157]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[ 0.0086,  0.0529,  0.0399,  ..., -0.0419,  0.0089, -0.0033],\n",
      "        [ 0.0145,  0.0432, -0.0715,  ..., -0.0379, -0.0095, -0.0315],\n",
      "        [ 0.0717,  0.0002, -0.0088,  ..., -0.0109,  0.0313, -0.0249],\n",
      "        ...,\n",
      "        [-0.0172, -0.0057, -0.0155,  ..., -0.0320,  0.0451, -0.0702],\n",
      "        [ 0.0148,  0.0544, -0.0056,  ..., -0.0422,  0.0176,  0.0102],\n",
      "        [-0.0011, -0.0181, -0.0024,  ...,  0.0243, -0.0535, -0.0065]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 926 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.6653137207031\n",
      "error.norm(): 2469.7783203125\n",
      "Feature: 302 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.7342834472656\n",
      "error.norm(): 2469.7783203125\n",
      "Feature: 1670 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.5897216796875\n",
      "error.norm(): 2469.7783203125\n",
      "Feature: 894 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.7880554199219\n",
      "error.norm(): 2469.7783203125\n",
      "Feature: 2293 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.8424072265625\n",
      "error.norm(): 2469.7783203125\n",
      "Feature: 2680 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.19219970703125\n",
      "error.norm(): 2469.7783203125\n",
      "Feature: 161 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.01483154296875\n",
      "error.norm(): 2469.7783203125\n",
      "Feature: 649 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.7655029296875\n",
      "error.norm(): 2469.7783203125\n",
      "Feature: 1530 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.7540588378906\n",
      "error.norm(): 2469.7783203125\n",
      "Feature: 2816 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                                                   | 4/1562 [00:03<18:37,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.78082275390625\n",
      "error.norm(): 2469.7783203125\n",
      "recons_image_embeddings_default: tensor([[ 0.0495,  0.0061, -0.0375,  ..., -0.0073, -0.0049,  0.0464],\n",
      "        [ 0.0656,  0.0185, -0.0169,  ..., -0.0542,  0.0806,  0.0280],\n",
      "        [ 0.0439,  0.0136,  0.0194,  ..., -0.0279,  0.0640, -0.0370],\n",
      "        ...,\n",
      "        [ 0.0259,  0.0402, -0.0065,  ..., -0.0289,  0.0129,  0.0450],\n",
      "        [ 0.0245,  0.0248, -0.0074,  ..., -0.0344,  0.0273, -0.0038],\n",
      "        [ 0.0167,  0.0346, -0.0975,  ...,  0.0074,  0.0849, -0.0346]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[ 0.0561,  0.0094, -0.0136,  ..., -0.0054,  0.0058,  0.0109],\n",
      "        [ 0.0664,  0.0435,  0.0039,  ..., -0.0612,  0.0621, -0.0128],\n",
      "        [ 0.0471,  0.0375,  0.0340,  ..., -0.0452,  0.0574, -0.0603],\n",
      "        ...,\n",
      "        [ 0.0138,  0.0437, -0.0035,  ..., -0.0495, -0.0058,  0.0223],\n",
      "        [ 0.0222,  0.0488, -0.0032,  ..., -0.0482,  0.0240, -0.0404],\n",
      "        [ 0.0277,  0.0533, -0.0660,  ..., -0.0090,  0.0571, -0.0354]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 926 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.5784606933594\n",
      "error.norm(): 2564.51220703125\n",
      "Feature: 302 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.6171569824219\n",
      "error.norm(): 2564.51220703125\n",
      "Feature: 1670 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.6531066894531\n",
      "error.norm(): 2564.51220703125\n",
      "Feature: 894 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.7791748046875\n",
      "error.norm(): 2564.51220703125\n",
      "Feature: 2293 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.7481384277344\n",
      "error.norm(): 2564.51220703125\n",
      "Feature: 2680 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.260009765625\n",
      "error.norm(): 2564.51220703125\n",
      "Feature: 161 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.1645202636719\n",
      "error.norm(): 2564.51220703125\n",
      "Feature: 649 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.6534423828125\n",
      "error.norm(): 2564.51220703125\n",
      "Feature: 1530 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.5770568847656\n",
      "error.norm(): 2564.51220703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                                                   | 5/1562 [00:03<18:28,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 2816 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.4371032714844\n",
      "error.norm(): 2564.51220703125\n",
      "recons_image_embeddings_default: tensor([[-0.0659, -0.0776, -0.0139,  ..., -0.0386,  0.0279,  0.0018],\n",
      "        [ 0.0148, -0.0243,  0.0026,  ..., -0.0218,  0.0321,  0.0376],\n",
      "        [-0.0408, -0.0001, -0.0266,  ..., -0.0062,  0.0039, -0.0037],\n",
      "        ...,\n",
      "        [-0.0287,  0.0508, -0.0474,  ...,  0.0316,  0.0009,  0.0108],\n",
      "        [-0.0258, -0.0096,  0.0075,  ..., -0.0291, -0.0626, -0.0089],\n",
      "        [ 0.0060, -0.0028, -0.0319,  ..., -0.0128,  0.0170, -0.0358]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[-0.0455, -0.0539,  0.0307,  ..., -0.0800,  0.0275, -0.0213],\n",
      "        [ 0.0182, -0.0212,  0.0214,  ..., -0.0278, -0.0036,  0.0367],\n",
      "        [-0.0091, -0.0196, -0.0200,  ..., -0.0316, -0.0004,  0.0059],\n",
      "        ...,\n",
      "        [-0.0410,  0.0416, -0.0564,  ...,  0.0466, -0.0198, -0.0205],\n",
      "        [-0.0062, -0.0019,  0.0048,  ..., -0.0456, -0.0640, -0.0389],\n",
      "        [ 0.0334,  0.0069, -0.0305,  ..., -0.0277,  0.0016, -0.0282]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 926 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.4043273925781\n",
      "error.norm(): 3598.17431640625\n",
      "Feature: 302 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.64483642578125\n",
      "error.norm(): 3598.17431640625\n",
      "Feature: 1670 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.1455383300781\n",
      "error.norm(): 3598.17431640625\n",
      "Feature: 894 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.69464111328125\n",
      "error.norm(): 3598.17431640625\n",
      "Feature: 2293 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.52850341796875\n",
      "error.norm(): 3598.17431640625\n",
      "Feature: 2680 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.00335693359375\n",
      "error.norm(): 3598.17431640625\n",
      "Feature: 161 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 397.6912536621094\n",
      "error.norm(): 3598.17431640625\n",
      "Feature: 649 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.3600769042969\n",
      "error.norm(): 3598.17431640625\n",
      "Feature: 1530 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.603271484375\n",
      "error.norm(): 3598.17431640625\n",
      "Feature: 2816 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▍                                                                                                   | 6/1562 [00:04<18:14,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.7709655761719\n",
      "error.norm(): 3598.17431640625\n",
      "recons_image_embeddings_default: tensor([[-0.0091, -0.0222,  0.0052,  ..., -0.0296,  0.0704, -0.0185],\n",
      "        [-0.0131, -0.0249, -0.0013,  ...,  0.0340, -0.0035,  0.0514],\n",
      "        [-0.0462,  0.0257, -0.0337,  ..., -0.0528, -0.0081, -0.0102],\n",
      "        ...,\n",
      "        [-0.0051, -0.0604, -0.0089,  ..., -0.0775, -0.0198, -0.0302],\n",
      "        [-0.0528, -0.0173,  0.0297,  ..., -0.0252,  0.0219,  0.0388],\n",
      "        [ 0.0022,  0.0004, -0.0324,  ..., -0.0625,  0.0004,  0.0154]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[-8.2808e-03, -2.3835e-02,  1.5856e-02,  ..., -7.5381e-02,\n",
      "          5.1499e-02, -5.4940e-02],\n",
      "        [ 1.1095e-02, -1.0313e-02,  5.7277e-05,  ...,  3.5189e-02,\n",
      "          3.6864e-03,  3.2079e-02],\n",
      "        [-5.1130e-02,  3.4042e-02, -4.3143e-02,  ..., -7.4632e-02,\n",
      "         -8.9801e-03, -2.9197e-02],\n",
      "        ...,\n",
      "        [-2.1917e-02, -2.1570e-02,  9.3216e-03,  ..., -1.0023e-01,\n",
      "         -1.5465e-02, -5.9362e-02],\n",
      "        [-4.4141e-02,  2.4029e-03,  5.1155e-02,  ..., -3.8464e-02,\n",
      "          1.0334e-02,  3.6968e-03],\n",
      "        [-9.8227e-03, -6.6791e-03, -3.4215e-02,  ..., -5.9345e-02,\n",
      "         -6.5057e-03, -1.1347e-02]], device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 926 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.60333251953125\n",
      "error.norm(): 2667.07763671875\n",
      "Feature: 302 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.6353759765625\n",
      "error.norm(): 2667.07763671875\n",
      "Feature: 1670 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.61712646484375\n",
      "error.norm(): 2667.07763671875\n",
      "Feature: 894 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.7904968261719\n",
      "error.norm(): 2667.07763671875\n",
      "Feature: 2293 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.67852783203125\n",
      "error.norm(): 2667.07763671875\n",
      "Feature: 2680 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.12469482421875\n",
      "error.norm(): 2667.07763671875\n",
      "Feature: 161 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.2742004394531\n",
      "error.norm(): 2667.07763671875\n",
      "Feature: 649 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.0686950683594\n",
      "error.norm(): 2667.07763671875\n",
      "Feature: 1530 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.6607971191406\n",
      "error.norm(): 2667.07763671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▍                                                                                                   | 7/1562 [00:05<18:10,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 2816 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.4515075683594\n",
      "error.norm(): 2667.07763671875\n",
      "recons_image_embeddings_default: tensor([[ 0.0136,  0.0643,  0.0395,  ..., -0.0361, -0.0052, -0.0236],\n",
      "        [ 0.0247, -0.0430,  0.0023,  ...,  0.0616,  0.0209,  0.0213],\n",
      "        [ 0.0044,  0.0217, -0.0404,  ..., -0.0586,  0.0506,  0.0444],\n",
      "        ...,\n",
      "        [-0.0156, -0.0046, -0.0264,  ..., -0.0680, -0.0048,  0.0319],\n",
      "        [ 0.0188, -0.0514, -0.0120,  ...,  0.0095, -0.0219, -0.0319],\n",
      "        [-0.0656,  0.0328,  0.0056,  ..., -0.0298,  0.0554, -0.0084]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[ 0.0185,  0.0363,  0.0478,  ..., -0.0441, -0.0143, -0.0541],\n",
      "        [ 0.0081, -0.0343,  0.0197,  ...,  0.0440, -0.0069, -0.0034],\n",
      "        [-0.0103,  0.0146, -0.0345,  ..., -0.0767,  0.0423,  0.0272],\n",
      "        ...,\n",
      "        [ 0.0030,  0.0150, -0.0357,  ..., -0.0684,  0.0164,  0.0272],\n",
      "        [ 0.0074, -0.0188, -0.0415,  ..., -0.0181, -0.0266, -0.0583],\n",
      "        [-0.0568,  0.0346,  0.0180,  ..., -0.0638,  0.0464, -0.0593]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 926 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.6501159667969\n",
      "error.norm(): 2228.6064453125\n",
      "Feature: 302 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.70330810546875\n",
      "error.norm(): 2228.6064453125\n",
      "Feature: 1670 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.6688537597656\n",
      "error.norm(): 2228.6064453125\n",
      "Feature: 894 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.8249206542969\n",
      "error.norm(): 2228.6064453125\n",
      "Feature: 2293 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.85247802734375\n",
      "error.norm(): 2228.6064453125\n",
      "Feature: 2680 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.1766052246094\n",
      "error.norm(): 2228.6064453125\n",
      "Feature: 161 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 397.9896240234375\n",
      "error.norm(): 2228.6064453125\n",
      "Feature: 649 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.810546875\n",
      "error.norm(): 2228.6064453125\n",
      "Feature: 1530 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.7647399902344\n",
      "error.norm(): 2228.6064453125\n",
      "Feature: 2816 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                                                   | 8/1562 [00:05<18:03,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.9732666015625\n",
      "error.norm(): 2228.6064453125\n",
      "recons_image_embeddings_default: tensor([[-0.0251,  0.0197, -0.1125,  ...,  0.0378, -0.0042, -0.0264],\n",
      "        [ 0.0071,  0.0089,  0.0063,  ...,  0.0080, -0.0004,  0.0045],\n",
      "        [-0.0034, -0.0238, -0.0226,  ..., -0.0305,  0.0216,  0.0113],\n",
      "        ...,\n",
      "        [-0.0507, -0.0230,  0.0469,  ...,  0.0239,  0.0170,  0.0040],\n",
      "        [-0.0284,  0.0204,  0.0041,  ..., -0.0395, -0.0102, -0.0003],\n",
      "        [-0.0714,  0.0389,  0.0543,  ..., -0.0956,  0.0180,  0.0454]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[-0.0093,  0.0500, -0.1025,  ...,  0.0222,  0.0031, -0.0412],\n",
      "        [-0.0049,  0.0034,  0.0029,  ..., -0.0084,  0.0022, -0.0227],\n",
      "        [-0.0178, -0.0209,  0.0144,  ..., -0.0836,  0.0125, -0.0039],\n",
      "        ...,\n",
      "        [-0.0352, -0.0084,  0.0368,  ..., -0.0119, -0.0022, -0.0284],\n",
      "        [-0.0338,  0.0488,  0.0156,  ..., -0.0390, -0.0320, -0.0313],\n",
      "        [-0.0657,  0.0552,  0.0495,  ..., -0.1019,  0.0260,  0.0323]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 926 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.38153076171875\n",
      "error.norm(): 3044.28955078125\n",
      "Feature: 302 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.5688171386719\n",
      "error.norm(): 3044.28955078125\n",
      "Feature: 1670 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.3962097167969\n",
      "error.norm(): 3044.28955078125\n",
      "Feature: 894 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.6978454589844\n",
      "error.norm(): 3044.28955078125\n",
      "Feature: 2293 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.7167663574219\n",
      "error.norm(): 3044.28955078125\n",
      "Feature: 2680 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.0541687011719\n",
      "error.norm(): 3044.28955078125\n",
      "Feature: 161 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 397.779541015625\n",
      "error.norm(): 3044.28955078125\n",
      "Feature: 649 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.5611572265625\n",
      "error.norm(): 3044.28955078125\n",
      "Feature: 1530 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.5939025878906\n",
      "error.norm(): 3044.28955078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                                                   | 9/1562 [00:06<18:02,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 2816 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.7094421386719\n",
      "error.norm(): 3044.28955078125\n",
      "recons_image_embeddings_default: tensor([[ 0.0040, -0.0179, -0.0437,  ...,  0.0046,  0.0661,  0.0756],\n",
      "        [ 0.0060,  0.0323, -0.0683,  ..., -0.0043, -0.0139, -0.0015],\n",
      "        [ 0.0149, -0.0036, -0.0349,  ...,  0.0409,  0.0110,  0.0132],\n",
      "        ...,\n",
      "        [ 0.0022,  0.0096,  0.0305,  ..., -0.0361,  0.0311,  0.0256],\n",
      "        [ 0.0050, -0.0871, -0.0127,  ..., -0.0107,  0.0622,  0.0261],\n",
      "        [-0.0142, -0.0271,  0.0240,  ..., -0.0353,  0.0251, -0.0124]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[ 0.0130, -0.0033, -0.0534,  ..., -0.0175,  0.0305,  0.0481],\n",
      "        [-0.0018,  0.0397, -0.0487,  ..., -0.0298,  0.0007, -0.0154],\n",
      "        [ 0.0108,  0.0213, -0.0404,  ...,  0.0211, -0.0133,  0.0095],\n",
      "        ...,\n",
      "        [-0.0162,  0.0191,  0.0255,  ..., -0.0347,  0.0008, -0.0034],\n",
      "        [ 0.0242, -0.0830, -0.0086,  ..., -0.0120,  0.0515, -0.0033],\n",
      "        [-0.0080, -0.0448,  0.0046,  ..., -0.0410,  0.0088, -0.0403]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 926 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.77899169921875\n",
      "error.norm(): 2149.607177734375\n",
      "Feature: 302 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.729248046875\n",
      "error.norm(): 2149.607177734375\n",
      "Feature: 1670 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.66485595703125\n",
      "error.norm(): 2149.607177734375\n",
      "Feature: 894 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.7366638183594\n",
      "error.norm(): 2149.607177734375\n",
      "Feature: 2293 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.8100280761719\n",
      "error.norm(): 2149.607177734375\n",
      "Feature: 2680 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.328125\n",
      "error.norm(): 2149.607177734375\n",
      "Feature: 161 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.2706604003906\n",
      "error.norm(): 2149.607177734375\n",
      "Feature: 649 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 398.2381591796875\n",
      "error.norm(): 2149.607177734375\n",
      "Feature: 1530 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.7051696777344\n",
      "error.norm(): 2149.607177734375\n",
      "Feature: 2816 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 399.1852111816406\n",
      "error.norm(): 2149.607177734375\n",
      "recons_image_embeddings_default: tensor([[ 0.0080,  0.0201, -0.0240,  ..., -0.0847,  0.0018, -0.1048],\n",
      "        [ 0.0137,  0.0007, -0.0480,  ..., -0.0553,  0.0555,  0.0146],\n",
      "        [-0.0104, -0.0148,  0.0434,  ...,  0.0549,  0.0064, -0.0264],\n",
      "        ...,\n",
      "        [-0.0120, -0.0477, -0.0498,  ..., -0.0423,  0.0376,  0.0224],\n",
      "        [ 0.0618,  0.0314, -0.0301,  ..., -0.0508,  0.0342, -0.0084],\n",
      "        [-0.0197, -0.0276, -0.0188,  ..., -0.0247,  0.0591,  0.0142]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[ 0.0065,  0.0297, -0.0004,  ..., -0.0819, -0.0066, -0.1083],\n",
      "        [ 0.0026,  0.0127, -0.0415,  ..., -0.0699,  0.0419,  0.0069],\n",
      "        [-0.0330,  0.0019,  0.0312,  ...,  0.0575, -0.0118, -0.0338],\n",
      "        ...,\n",
      "        [ 0.0148, -0.0166, -0.0272,  ..., -0.0392,  0.0245, -0.0027],\n",
      "        [ 0.0379,  0.0353, -0.0252,  ..., -0.0729,  0.0223, -0.0424],\n",
      "        [-0.0188,  0.0008, -0.0034,  ..., -0.0613,  0.0213, -0.0131]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                                   | 9/1562 [00:07<21:15,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "max_samples = cfg.eval_max\n",
    "\n",
    "# top_activations = {i: (None, None) for i in interesting_features_indices}\n",
    "encoder_biases = sparse_autoencoder.b_enc#[interesting_features_indices]\n",
    "encoder_weights = sparse_autoencoder.W_enc#[:, interesting_features_indices]\n",
    "\n",
    "top_k=10\n",
    "processed_samples = 0\n",
    "default_embeds_list = []\n",
    "feature_steered_embeds = defaultdict(list)\n",
    "l = 0\n",
    "for batch_images, _, batch_indices in tqdm(val_dataloader, total=max_samples // cfg.batch_size):\n",
    "    batch_images = batch_images.to(cfg.device)\n",
    "    batch_indices = batch_indices.to(cfg.device)\n",
    "    batch_size = batch_images.shape[0]\n",
    "\n",
    "    altered_embeds_list, default_embeds = compute_feature_activations_set_feat(\n",
    "        batch_images, model, sparse_autoencoder, encoder_weights, encoder_biases,\n",
    "        None, None, top_k\n",
    "    )\n",
    "    default_embeds_list.append(default_embeds)\n",
    "    for j, altered_embeds in enumerate(altered_embeds_list):\n",
    "        feature_steered_embeds[random_feat_idxs[j]].extend(altered_embeds)\n",
    "    # either label embeds or optimize to maximal token in text transformer embedding face\n",
    "    l += 1\n",
    "    if l >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_steered_embeds[926])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320, 512])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(feature_steered_embeds[926]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320, 512])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_embeds.shape\n",
    "len(default_embeds_list)\n",
    "default_embeds = torch.cat(default_embeds_list)\n",
    "default_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, torch.Size([32, 512]), torch.Size([320, 512]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(altered_embeds_list), altered_embeds_list[0].shape, default_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/clip_dissect_raw.txt\", \"r\") as f:\n",
    "    larger_vocab = [line[:-1] for line in f.readlines()][:5000]\n",
    "\n",
    "# with open(\"/workspace/better_img_desc.txt\", \"r\") as f:\n",
    "#     larger_vocab = [line[:-1] for line in f.readlines()][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_features_normed.shape: torch.Size([5000, 512])\n",
      "Label probs altered: torch.Size([320, 5000])\n",
      "Label probs default: torch.Size([320, 5000])\n"
     ]
    }
   ],
   "source": [
    "# use clip vocab here and compare embeds\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "text = tokenizer(larger_vocab)\n",
    "text_features = og_model.encode_text(text.cuda())\n",
    "text_features_normed = text_features/text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "print(f\"text_features_normed.shape: {text_features_normed.shape}\")\n",
    "text_probs_altered_list = []\n",
    "# can probs make this one tensor \n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    for key in feature_steered_embeds:\n",
    "        text_probs_altered = (100.0 * torch.stack(feature_steered_embeds[key]) @ text_features_normed.T).softmax(dim=-1)\n",
    "        text_probs_altered_list.append(text_probs_altered)\n",
    "    text_probs_default = (100.0 * default_embeds @ text_features_normed.T).softmax(dim=-1)\n",
    "\n",
    "\n",
    "# for altered_embeds in altered_embeds_list:\n",
    "#     with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "#         # might want to still normalize\n",
    "        \n",
    "#         # already normalized\n",
    "#         # altered_embeds /= altered_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         text_probs_altered = (100.0 * altered_embeds @ text_features_normed.T).softmax(dim=-1)\n",
    "#         text_probs_altered_list.append(text_probs_altered)\n",
    "#     # default_embds_norm = default_embeds.norm(dim=-1, keepdim=True)\n",
    "#     text_probs_default = (100.0 * default_embeds @ text_features_normed.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs altered:\", text_probs_altered.shape)  # prints: [[1., 0., 0.]]\n",
    "print(\"Label probs default:\", text_probs_default.shape)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6635e-06, 5.4659e-05, 4.5314e-05,  ..., 4.0766e-07, 4.7290e-07,\n",
       "         9.8560e-07],\n",
       "        [3.6405e-06, 2.0305e-05, 3.0481e-05,  ..., 1.0761e-06, 1.0908e-07,\n",
       "         4.6020e-06],\n",
       "        [2.1308e-06, 2.4964e-05, 1.1885e-05,  ..., 1.8513e-06, 1.4305e-06,\n",
       "         6.2252e-07],\n",
       "        ...,\n",
       "        [3.9568e-06, 9.6617e-05, 4.1792e-06,  ..., 3.9879e-06, 1.8985e-06,\n",
       "         8.9329e-07],\n",
       "        [2.1700e-05, 1.4830e-04, 1.0850e-04,  ..., 1.4425e-06, 8.2539e-05,\n",
       "         1.2173e-05],\n",
       "        [7.6223e-07, 2.1256e-05, 3.4501e-05,  ..., 1.2086e-06, 9.5404e-08,\n",
       "         1.0337e-06]], device='cuda:0')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_probs_altered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summed Logit Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "\n",
      "For Feature 926\n",
      "logit_diff.shape: torch.Size([320, 5000])\n",
      "logit_diff_aggregate.shape: torch.Size([5000])\n",
      "\n",
      "Most Changed, by Ratio Over 320 Images:\n",
      "tensor([42163.8789, 26816.3516, 22668.0391, 22433.6445, 14714.5049, 14317.5713,\n",
      "        13733.7852, 13698.5605, 13281.7041, 12903.8779], device='cuda:0')\n",
      "['xxx' 'incorporated' 'farmers' 'neck' 'amd' 'nutrition' 'specifications'\n",
      " 'breakfast' 'watch' 'dildo']\n",
      "tensor([-2.1841, -2.1333, -1.9458, -1.7038, -1.5040, -1.4054, -1.1393, -1.0497,\n",
      "        -1.0242, -0.9965], device='cuda:0')\n",
      "['evening' 'writes' 'partners' 'night' 'jay' 'jet' 'paris' 'vintage'\n",
      " 'writing' 'pocket']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 302\n",
      "logit_diff.shape: torch.Size([320, 5000])\n",
      "logit_diff_aggregate.shape: torch.Size([5000])\n",
      "\n",
      "Most Changed, by Ratio Over 320 Images:\n",
      "tensor([64779.4219, 62509.1250, 48409.2070, 44266.4688, 37498.0000, 36858.1992,\n",
      "        34276.8125, 32407.2441, 30894.9961, 28420.1484], device='cuda:0')\n",
      "['denver' 'apache' 'cookies' 'poster' 'citysearch' 'atlanta' 'debian'\n",
      " 'recipes' 'menu' 'cialis']\n",
      "tensor([-2.0666, -2.0565, -1.3239, -1.0853, -1.0626, -1.0507, -0.9681, -0.8606,\n",
      "        -0.8474, -0.8405], device='cuda:0')\n",
      "['sw' 'receiving' 'ray' 'section' 'morning' 'successfully' 'plot' 'nd'\n",
      " 'subsection' 'surrounding']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1670\n",
      "logit_diff.shape: torch.Size([320, 5000])\n",
      "logit_diff_aggregate.shape: torch.Size([5000])\n",
      "\n",
      "Most Changed, by Ratio Over 320 Images:\n",
      "tensor([57111.5312, 38550.5000, 28975.8398, 28646.3301, 28263.5703, 27966.3438,\n",
      "        25333.2969, 24644.5020, 23966.0078, 23268.6719], device='cuda:0')\n",
      "['painting' 'roof' 'specs' 'directors' 'collectables' 'art' 'damages'\n",
      " 'studies' 'rush' 'catalog']\n",
      "tensor([-2.2554, -2.1873, -2.0726, -1.8746, -1.3104, -1.2436, -1.1977, -0.9322,\n",
      "        -0.9049, -0.7924], device='cuda:0')\n",
      "['l' 'navigation' 'k' 'lab' 's' 'z' 'j' 'm' 'iv' 'pocket']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 894\n",
      "logit_diff.shape: torch.Size([320, 5000])\n",
      "logit_diff_aggregate.shape: torch.Size([5000])\n",
      "\n",
      "Most Changed, by Ratio Over 320 Images:\n",
      "tensor([31244.9414, 27707.0664, 24950.9414, 24861.7578, 24540.0391, 22950.8320,\n",
      "        22750.6602, 20777.7363, 18251.2070, 17382.8809], device='cuda:0')\n",
      "['posters' 'baby' 'frank' 'parties' 'fans' 'guestbook' 'belgium'\n",
      " 'drinking' 'zealand' 'dinner']\n",
      "tensor([-2.2293, -1.9742, -1.6199, -1.3496, -1.0830, -1.0575, -1.0043, -0.9821,\n",
      "        -0.9389, -0.8015], device='cuda:0')\n",
      "['rose' 'link' 'edge' 'holds' 'sealed' 'detailed' 'detail' 'modeling'\n",
      " 'boots' 'configure']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2293\n",
      "logit_diff.shape: torch.Size([320, 5000])\n",
      "logit_diff_aggregate.shape: torch.Size([5000])\n",
      "\n",
      "Most Changed, by Ratio Over 320 Images:\n",
      "tensor([60983.4688, 28973.9961, 26343.6836, 24542.1680, 21880.5938, 20745.7305,\n",
      "        20297.5547, 19367.5254, 18707.4805, 18387.3145], device='cuda:0')\n",
      "['apartments' 'clothes' 'shirts' 'shoes' 'shopping' 'shopper' 'retail'\n",
      " 'guestbook' 'casinos' 'shops']\n",
      "tensor([-2.1586, -1.9039, -1.8540, -1.5847, -1.3072, -1.2142, -1.0575, -1.0377,\n",
      "        -1.0048, -0.9205], device='cuda:0')\n",
      "['pipeline' 'up' 'orleans' 'signal' 'over' 'antique' 'argument'\n",
      " 'concerning' 'oral' 'columbus']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2680\n",
      "logit_diff.shape: torch.Size([320, 5000])\n",
      "logit_diff_aggregate.shape: torch.Size([5000])\n",
      "\n",
      "Most Changed, by Ratio Over 320 Images:\n",
      "tensor([76194.5469, 48990.5156, 37552.4375, 21386.2969, 19917.4512, 18178.0664,\n",
      "        16062.8291, 15573.5020, 15053.8457, 14943.5508], device='cuda:0')\n",
      "['lingerie' 'belt' 'chain' 'piano' 'labels' 'outlet' 'rings' 'wings'\n",
      " 'leather' 'shoes']\n",
      "tensor([-2.3158, -1.6949, -1.6214, -1.4525, -1.3019, -1.1423, -1.0528, -0.9398,\n",
      "        -0.9174, -0.8664], device='cuda:0')\n",
      "['afternoon' 'arizona' 'evening' 'nevada' 'attempt' 'georgia' 'forgot'\n",
      " 'colorado' 'utah' 'plasma']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 161\n",
      "logit_diff.shape: torch.Size([320, 5000])\n",
      "logit_diff_aggregate.shape: torch.Size([5000])\n",
      "\n",
      "Most Changed, by Ratio Over 320 Images:\n",
      "tensor([85114.7422, 61466.6719, 55738.4609, 49116.9727, 36337.7656, 35095.7344,\n",
      "        35006.2344, 34595.5664, 32868.4844, 32522.7930], device='cuda:0')\n",
      "['telecommunications' 'broadband' 'administrator' 'address' 'services'\n",
      " 'apartments' 'addresses' 'voip' 'telecom' 'census']\n",
      "tensor([-3.1091, -2.1123, -1.8685, -1.6598, -1.1775, -1.1625, -1.1268, -1.1173,\n",
      "        -1.0160, -1.0027], device='cuda:0')\n",
      "['pull' 'fighting' 'butt' 'boobs' 'both' 'fight' 'potter' 'muscle'\n",
      " 'evening' 'rear']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 649\n",
      "logit_diff.shape: torch.Size([320, 5000])\n",
      "logit_diff_aggregate.shape: torch.Size([5000])\n",
      "\n",
      "Most Changed, by Ratio Over 320 Images:\n",
      "tensor([90052.8594, 43268.7344, 31181.1445, 29588.6602, 29500.7715, 29412.0820,\n",
      "        28822.0352, 28436.2539, 27148.7402, 26261.8320], device='cuda:0')\n",
      "['wedding' 'restaurants' 'catering' 'restaurant' 'supplements' 'tea'\n",
      " 'phentermine' 'guestbook' 'bars' 'chairs']\n",
      "tensor([-1.9896, -1.9742, -1.5997, -1.4940, -1.3672, -1.2465, -1.1600, -1.0875,\n",
      "        -0.9839, -0.9187], device='cuda:0')\n",
      "['oxford' 'nancy' 'orleans' 'ranking' 'sterling' 'ericsson' 'france' 'die'\n",
      " 'rom' 'bureau']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1530\n",
      "logit_diff.shape: torch.Size([320, 5000])\n",
      "logit_diff_aggregate.shape: torch.Size([5000])\n",
      "\n",
      "Most Changed, by Ratio Over 320 Images:\n",
      "tensor([44462.4180, 30542.9883, 24678.0703, 21567.2070, 20566.9805, 20114.3438,\n",
      "        19479.3125, 17654.4512, 17616.9316, 16926.7773], device='cuda:0')\n",
      "['watches' 'bugs' 'inter' 'brown' 'gcc' 'specs' 'size' 'after' 'completed'\n",
      " 'sizes']\n",
      "tensor([-1.3089, -1.1974, -1.1874, -1.0371, -0.9547, -0.9159, -0.8716, -0.8348,\n",
      "        -0.8278, -0.8215], device='cuda:0')\n",
      "['worship' 'seattle' 'residence' 'wales' 'zealand' 'toronto' 'vancouver'\n",
      " 'architecture' 'workplace' 'apartment']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2816\n",
      "logit_diff.shape: torch.Size([320, 5000])\n",
      "logit_diff_aggregate.shape: torch.Size([5000])\n",
      "\n",
      "Most Changed, by Ratio Over 320 Images:\n",
      "tensor([80019.6953, 28757.6289, 28413.5195, 18493.3672, 17665.8047, 16134.0312,\n",
      "        15636.3369, 14970.9658, 12461.8535, 11997.1211], device='cuda:0')\n",
      "['mississippi' 'pakistan' 'vintage' 'antique' 'item' 'benefit' 'tennessee'\n",
      " 'miami' 'seller' 'nursing']\n",
      "tensor([-3.4509, -1.6263, -1.4712, -1.4179, -1.3359, -1.2927, -0.9668, -0.9224,\n",
      "        -0.8152, -0.7678], device='cuda:0')\n",
      "['fucking' 'launched' 'logos' 'lock' 'joined' 'strong' 'launch' 'fighting'\n",
      " 'fight' 'opening']\n"
     ]
    }
   ],
   "source": [
    "# subtract from default, label, and print trends\n",
    "text_probs_altered.shape\n",
    "\n",
    "# selected_vocab = all_imagenet_class_names\n",
    "selected_vocab = larger_vocab\n",
    "\n",
    "# switch to datacomp eval??\n",
    "# we run this for each feature over all of imagenet eval and create average absolute/ratio\n",
    "# difference vectors\n",
    "for j, text_probs_altered in enumerate(text_probs_altered_list):\n",
    "    print(f\"{'============================================'*2}\\n\\nFor Feature {random_feat_idxs[j]}\")\n",
    "    logit_diff = text_probs_altered - text_probs_default\n",
    "    print(f\"logit_diff.shape: {logit_diff.shape}\")\n",
    "    logit_diff_aggregate = logit_diff.sum(dim=0)\n",
    "    print(f\"logit_diff_aggregate.shape: {logit_diff_aggregate.shape}\")\n",
    "    \n",
    "    logit_ratio = text_probs_altered/text_probs_default\n",
    "    logit_ratio_aggregate = logit_ratio.sum(dim=0)\n",
    "    \n",
    "    vals_agg, idxs_agg = torch.topk(logit_diff_aggregate,k=10)\n",
    "    vals_least_agg, idxs_least_agg = torch.topk(logit_diff_aggregate,k=10,largest=False)\n",
    "    \n",
    "    ratios_agg, ratios_idxs_agg = torch.topk(logit_ratio_aggregate,k=10)\n",
    "    ratios_least_agg, ratios_idxs_least_agg = torch.topk(logit_ratio_aggregate,k=10,largest=False)\n",
    "    \n",
    "    vals, idxs = torch.topk(logit_diff,k=5)\n",
    "    vals_least, idxs_least = torch.topk(logit_diff,k=5,largest=False)\n",
    "    \n",
    "    ratios, ratios_idxs = torch.topk(logit_ratio,k=5)\n",
    "    ratios_least, ratios_idxs_least = torch.topk(logit_ratio,k=5,largest=False)\n",
    "    \n",
    "#     print(f\"\\nMost Changed, by Absolute Diff Over {logit_diff.shape[0]} Images:\\n{vals_agg}\")\n",
    "#     print(np.array(selected_vocab)[idxs_agg.cpu()])\n",
    "#     print(vals_least_agg)\n",
    "#     print(np.array(selected_vocab)[idxs_least_agg.cpu()])\n",
    "    \n",
    "    print(f\"\\nMost Changed, by Ratio Over {logit_diff.shape[0]} Images:\")\n",
    "    print(ratios_agg)\n",
    "    print(np.array(selected_vocab)[ratios_idxs_agg.cpu()])\n",
    "    print(vals_least_agg)\n",
    "    print(np.array(selected_vocab)[ratios_idxs_least_agg.cpu()])\n",
    "    \n",
    "    \n",
    "#     for i in range(logit_diff.shape[0]):\n",
    "# #         print(f\"\\nImage {i} ========================\\nMost Changed, by Absolute Diff\\n:{vals[i]}\")\n",
    "# #         print(np.array(selected_vocab)[idxs.cpu()][i])\n",
    "# #         print(vals_least[i])\n",
    "# #         print(np.array(selected_vocab)[idxs_least.cpu()][i])\n",
    "#         print(f\"\\nImage {i} Most Changed, by Ratio:\")\n",
    "#         print(ratios[i])\n",
    "#         print(np.array(selected_vocab)[ratios_idxs.cpu()][i])\n",
    "#         print(ratios_least[i])\n",
    "#         print(np.array(selected_vocab)[ratios_idxs_least.cpu()][i])\n",
    "        \n",
    "# #         text = tokenizer(np.array(selected_vocab)[ratios_idxs.cpu()][i])\n",
    "# #         text_least = tokenizer(np.array(selected_vocab)[ratios_idxs_least.cpu()][i])\n",
    "# #         text_features = og_model.encode_text(text.cuda())\n",
    "# #         text_features_least = og_model.encode_text(text_least.cuda())\n",
    "\n",
    "    if j > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract from default, label, and print trends\n",
    "text_probs_altered.shape\n",
    "\n",
    "# selected_vocab = all_imagenet_class_names\n",
    "selected_vocab = larger_vocab\n",
    "\n",
    "cov_stuff_avgs = []\n",
    "\n",
    "# switch to datacomp eval??\n",
    "# we run this for each feature over all of imagenet eval and create average absolute/ratio\n",
    "# difference vectors\n",
    "per_feat_avg_vectors = []\n",
    "for j, text_probs_altered in enumerate(text_probs_altered_list):\n",
    "    print(f\"\\n\\nFor Feature {random_feat_idxs[j]}\")\n",
    "    print(f\"\\n\\nFor Feature {random_feat_idxs[j]}\")\n",
    "    logit_diff = text_probs_altered - text_probs_default\n",
    "    logit_ratio = text_probs_altered/text_probs_default\n",
    "    \n",
    "    vals, idxs = torch.topk(logit_diff,k=5)\n",
    "    vals_least, idxs_least = torch.topk(logit_diff,k=5,largest=False)\n",
    "    \n",
    "    ratios, ratios_idxs = torch.topk(logit_ratio,k=5)\n",
    "    ratios_least, ratios_idxs_least = torch.topk(logit_ratio,k=5,largest=False)\n",
    "    \n",
    "    feat_avg_vectors = []\n",
    "    for i in range(logit_diff.shape[0]):\n",
    "#         print(f\"\\nImage {i} ========================\\nMost Changed, by Absolute Diff\\n:{vals[i]}\")\n",
    "#         print(np.array(all_imagenet_class_names)[idxs.cpu()][i])\n",
    "#         print(vals_least[i])\n",
    "#         print(np.array(all_imagenet_class_names)[idxs_least.cpu()][i])\n",
    "        \n",
    "        print(\"\\nMost Changed, by Ratio:\")\n",
    "        print(ratios[i])\n",
    "        print(np.array(selected_vocab)[ratios_idxs.cpu()][i])\n",
    "#         print(ratios_least[i])\n",
    "#         print(np.array(selected_vocab)[ratios_idxs_least.cpu()][i])\n",
    "        \n",
    "        text = tokenizer(np.array(selected_vocab)[ratios_idxs.cpu()][i])\n",
    "#         text_least = tokenizer(np.array(selected_vocab)[ratios_idxs_least.cpu()][i])\n",
    "        text_features = og_model.encode_text(text.cuda())\n",
    "        cov_over_images.append(text_features)\n",
    "#         text_features_least = og_model.encode_text(text_least.cuda())\n",
    "        print(torch.tril(torch.cov(text_features), diagonal=-1))\n",
    "        print(torch.tril(torch.cov(text_features), diagonal=-1).sum()/10)\n",
    "#         cov_stuff = torch.tril(torch.cov(text_features), diagonal=-1).sum()/10\n",
    "#         cov_stuff_least = torch.tril(torch.cov(text_features_least), diagonal=-1).sum()/10\n",
    "    print(torch.tril(torch.cov(torch.cat(cov_over_images)), diagonal=-1).shape)\n",
    "    n = torch.tril(torch.cov(torch.cat(cov_over_images)), diagonal=-1).shape[0]\n",
    "    num_elements = (n**2)/2 - n\n",
    "    cov_stuff = torch.tril(torch.cov(torch.cat(cov_over_images)), diagonal=-1).sum()/num_elements\n",
    "    cov_stuff_avgs.append(cov_stuff)\n",
    "#         cov_stuff_avgs_least.append(cov_stuff_least)\n",
    "    if j > 10:\n",
    "        break\n",
    "print(torch.tensor(cov_stuff_avgs).mean())\n",
    "# print(torch.tensor(cov_stuff_avgs_least).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_imgnet_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "feat_autolabels_default = defaultdict(Counter)\n",
    "for i in range(text_probs_default.shape[0]):\n",
    "    vals, idxs = torch.topk(text_probs_default[i],k=top_k_imgnet_labels)\n",
    "    print(i)\n",
    "    for k, idx in enumerate(idxs):\n",
    "        feat_autolabels_default[i][all_imagenet_class_names[idx]] += vals[k]\n",
    "        print(\"\\t\", all_imagenet_class_names[idx])\n",
    "feat_autolabels_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract from default, label, and print trends\n",
    "text_probs_altered.shape\n",
    "\n",
    "for text_probs_altered in text_probs_altered_list:\n",
    "    logit_diff = text_probs_altered - text_probs_default\n",
    "    print(logit_diff)\n",
    "    vals, idxs = torch.topk(logit_diff,k=5)\n",
    "    print(vals, np.array(all_imagenet_class_names[idxs])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imagenet_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "feat_autolabels_altered_list = []\n",
    "for text_probs_altered in text_probs_altered_list:\n",
    "    feat_autolabels_altered = defaultdict(Counter)\n",
    "    for i in range(text_probs_altered.shape[0]):\n",
    "        vals, idxs = torch.topk(text_probs_altered[i],k=top_k_imgnet_labels)\n",
    "#         print(i)\n",
    "        for k, idx in enumerate(idxs):\n",
    "            feat_autolabels_altered[i][all_imagenet_class_names[idx]] += vals[k]\n",
    "#             print(\"\\t\", all_imagenet_class_names[idx])\n",
    "    feat_autolabels_altered_list.append(feat_autolabels_altered)\n",
    "\n",
    "start_idx = 9\n",
    "end_idx = 10\n",
    "    \n",
    "h = 0\n",
    "for key in feat_autolabels_default:\n",
    "    print(f\"\\nfeat_autolabels_default img {key}:\\n {feat_autolabels_default[key]}\\n\")\n",
    "    h += 1\n",
    "    if h > end_idx:\n",
    "        break\n",
    "for i, f_a_a in enumerate(feat_autolabels_altered_list):\n",
    "    print(\"============= feature number \", i, \"====================\")\n",
    "    h = 0\n",
    "    for key in range(start_idx, end_idx):\n",
    "#         print(\"\\n\", key)\n",
    "#         for item in f_a_a[key]:\n",
    "#             print(\"\\t\", item, f_a_a[key][item].cpu().item())\n",
    "        print(f\"\\nf_a_a img {key}:\\n {f_a_a[key]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(text_probs_default.shape[0]):\n",
    "    vals, idxs = torch.topk(text_probs_default[i],k=1000)\n",
    "    print(i, ind_to_name[str(idxs[0].cpu().item())][1])\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "#     ax.xaxis.set_ticks((1000))\n",
    "#     ax.set_xticks(list(range(1000)), [ind_to_name[str(idxs[idx].cpu().item())][1] for idx in idxs])\n",
    "    plt.bar(idxs.cpu(), vals.cpu(), width=5)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch_images[2].cpu().permute((1,2,0)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def get_heatmap(\n",
    "          image,\n",
    "          model,\n",
    "          sparse_autoencoder,\n",
    "          feature_id,\n",
    "): \n",
    "    image = image.to(cfg.device)\n",
    "    _, cache = model.run_with_cache(image.unsqueeze(0))\n",
    "\n",
    "    post_reshaped = einops.rearrange(cache[sparse_autoencoder.cfg.hook_point], \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "    # Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n",
    "    sae_in =  post_reshaped - sparse_autoencoder.b_dec # Remove decoder bias as per Anthropic\n",
    "    print(f\"sae_in.shape: {sae_in.shape}\")\n",
    "    acts = einops.einsum(\n",
    "            sae_in,\n",
    "            sparse_autoencoder.W_enc[:, feature_id],\n",
    "            \"x d_in, d_in -> x\",\n",
    "        )\n",
    "    return acts \n",
    "     \n",
    "def image_patch_heatmap(activation_values,image_size=224, pixel_num=14):\n",
    "    activation_values = activation_values.detach().cpu().numpy()\n",
    "    activation_values = activation_values[1:]\n",
    "    activation_values = activation_values.reshape(pixel_num, pixel_num)\n",
    "\n",
    "    # Create a heatmap overlay\n",
    "    heatmap = np.zeros((image_size, image_size))\n",
    "    patch_size = image_size // pixel_num\n",
    "\n",
    "    for i in range(pixel_num):\n",
    "        for j in range(pixel_num):\n",
    "            heatmap[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size] = activation_values[i, j]\n",
    "\n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "grid_size = 1\n",
    "fig, axs = plt.subplots(int(np.ceil(len(images)/grid_size)), grid_size, figsize=(15, 15))\n",
    "name=  f\"Category: uhh,  Feature: {0}\"\n",
    "fig.suptitle(name)#, y=0.95)\n",
    "for ax in axs.flatten():\n",
    "    ax.axis('off')\n",
    "complete_bid = []\n",
    "\n",
    "heatmap = get_heatmap(batch_images[2], model,sparse_autoencoder, 10000)\n",
    "heatmap = image_patch_heatmap(heatmap, pixel_num=224//cfg.patch_size)\n",
    "\n",
    "display = batch_images[2].cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "has_zero = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(display)\n",
    "plt.imshow(heatmap, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(display)\n",
    "plt.imshow(heatmap, alpha=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
