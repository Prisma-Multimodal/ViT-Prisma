{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating your SAE\n",
    "\n",
    "Code based off Rob Graham's ([themachinefan](https://github.com/themachinefan)) SAE evaluation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/ViT-Prisma/src/vit_prisma/sae/evals'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_per_buffer (millions): 0.032\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 158691\n",
      "Total training images: 13000000\n",
      "Total wandb updates: 15869\n",
      "Expansion factor: 16\n",
      "n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 158 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Gradient clipping with max_norm=1.0\n",
      "Using SAE initialization method: encoder_transpose_decoder\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from vit_prisma.sae.config import VisionModelSAERunnerConfig\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig(VisionModelSAERunnerConfig):\n",
    "    sae_path: str = '/workspace/sae_checkpoints/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-11-hook_resid_post-l1-0.0001/n_images_2600058.pt'\n",
    "    model_name: str = \"open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "    model_type: str =  \"clip\"\n",
    "    patch_size: str = 32\n",
    "\n",
    "    dataset_path = \"/workspace\"\n",
    "    dataset_train_path: str = \"/workspace/ILSVRC/Data/CLS-LOC/train\"\n",
    "    dataset_val_path: str = \"/workspace/ILSVRC/Data/CLS-LOC/val\"\n",
    "\n",
    "    verbose: bool = True\n",
    "\n",
    "    device: bool = 'cuda'\n",
    "\n",
    "    eval_max: int = 50_000 # 50_000\n",
    "    batch_size: int = 32\n",
    "\n",
    "    # make the max image output folder a subfolder of the sae path\n",
    "\n",
    "\n",
    "    @property\n",
    "    def max_image_output_folder(self) -> str:\n",
    "        # Get the base directory of sae_checkpoints\n",
    "        sae_base_dir = os.path.dirname(os.path.dirname(self.sae_path))\n",
    "        \n",
    "        # Get the name of the original SAE checkpoint folder\n",
    "        sae_folder_name = os.path.basename(os.path.dirname(self.sae_path))\n",
    "        \n",
    "        # Create a new folder path in sae_checkpoints/images with the original name\n",
    "        output_folder = os.path.join(sae_base_dir, 'max_images', sae_folder_name)\n",
    "        output_folder = os.path.join(output_folder, f\"layer_{self.hook_point_layer}\") # Add layer number\n",
    "\n",
    "        \n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        return output_folder\n",
    "\n",
    "cfg = EvalConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7ef7cac55450>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "Official model name open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "Converting OpenCLIP weights\n",
      "model_id download_pretrained_from_hf: laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\n",
      "visual projection shape torch.Size([768, 512])\n",
      "Setting center_writing_weights to False for OpenCLIP\n",
      "Setting fold_ln to False for OpenCLIP\n",
      "Loaded pretrained model open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from vit_prisma.models.base_vit import HookedViT\n",
    "\n",
    "model_name = \"open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "model = HookedViT.from_pretrained(model_name, is_timm=False, is_clip=True).to(cfg.device)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import vit_prisma\n",
    "# importlib.reload(vit_prisma.dataloaders.imagenet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation data length: 50000\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "import open_clip\n",
    "from vit_prisma.utils.data_utils.imagenet_utils import setup_imagenet_paths\n",
    "from vit_prisma.dataloaders.imagenet_dataset import get_imagenet_transforms_clip, ImageNetValidationDataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "og_model_name = \"hf-hub:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K\"\n",
    "og_model, _, preproc = open_clip.create_model_and_transforms(og_model_name)\n",
    "processor = preproc\n",
    "\n",
    "size=224\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                     std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "    \n",
    "imagenet_paths = setup_imagenet_paths(cfg.dataset_path)\n",
    "imagenet_paths[\"train\"] = \"/workspace/ILSVRC/Data/CLS-LOC/train\"\n",
    "imagenet_paths[\"val\"] = \"/workspace/ILSVRC/Data/CLS-LOC/val\"\n",
    "imagenet_paths[\"val_labels\"] = \"/workspace/LOC_val_solution.csv\"\n",
    "imagenet_paths[\"label_strings\"] = \"/workspace/LOC_synset_mapping.txt\"\n",
    "print()\n",
    "train_data = torchvision.datasets.ImageFolder(cfg.dataset_train_path, transform=data_transforms)\n",
    "val_data = ImageNetValidationDataset(cfg.dataset_val_path, \n",
    "                                imagenet_paths['label_strings'], \n",
    "                                imagenet_paths['val_labels'], \n",
    "                                data_transforms,\n",
    "                                return_index=True,\n",
    ")\n",
    "val_data_visualize = ImageNetValidationDataset(cfg.dataset_val_path, \n",
    "                                imagenet_paths['label_strings'], \n",
    "                                imagenet_paths['val_labels'],\n",
    "                                torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),]), return_index=True)\n",
    "\n",
    "print(f\"Validation data length: {len(val_data)}\") if cfg.verbose else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.sae.training.activations_store import VisionActivationsStore\n",
    "# import dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# activations_loader = VisionActivationsStore(cfg, model, train_data, eval_dataset=val_data)\n",
    "val_dataloader = DataLoader(val_data, batch_size=cfg.batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained SAE to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_activation_fn received: activation_fn=relu, kwargs={}\n",
      "n_tokens_per_buffer (millions): 0.032\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.00064\n",
      "Total training steps: 158691\n",
      "Total training images: 13000000\n",
      "Total wandb updates: 1586\n",
      "Expansion factor: 64\n",
      "n_tokens_per_feature_sampling_window (millions): 204.8\n",
      "n_tokens_per_dead_feature_window (millions): 1024.0\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 158 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Gradient clipping with max_norm=1.0\n",
      "Using SAE initialization method: encoder_transpose_decoder\n",
      "get_activation_fn received: activation_fn=relu, kwargs={}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SparseAutoencoder(\n",
       "  (hook_sae_in): HookPoint()\n",
       "  (hook_hidden_pre): HookPoint()\n",
       "  (hook_hidden_post): HookPoint()\n",
       "  (hook_sae_out): HookPoint()\n",
       "  (activation_fn): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vit_prisma.sae.sae import SparseAutoencoder\n",
    "sparse_autoencoder = SparseAutoencoder(cfg).load_from_pretrained(\"/workspace/sae_checkpoints/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-11-hook_resid_post-l1-0.0001/n_images_2600058.pt\")\n",
    "sparse_autoencoder.to(cfg.device)\n",
    "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Labeling AutoInterp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_imagenet_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.dataloaders.imagenet_dataset import get_imagenet_index_to_name\n",
    "ind_to_name = get_imagenet_index_to_name()\n",
    "\n",
    "all_imagenet_class_names = []\n",
    "for i in range(len(ind_to_name)):\n",
    "    all_imagenet_class_names.append(ind_to_name[str(i)][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/sae_checkpoints/max_images/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-11-hook_resid_post-l1-0.0001/layer_9'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.max_image_output_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_replacement_hook_curry(feat_idx: int = 0, feat_activ: float = 1.0):\n",
    "    def standard_replacement_hook(activations: torch.Tensor, hook):\n",
    "        activations = sparse_autoencoder.forward(activations)[0].to(activations.dtype)\n",
    "        feature_acts = sparse_autoencoder.encode_standard(activations)\n",
    "\n",
    "        # in all batches and patches, set feature w idx idx to 0\n",
    "        print(f\"feature_acts[:,:,idx].shape: {feature_acts[:,:,feat_idx].shape}\")\n",
    "        print(f\"feat activ: {feature_acts[:,:,feat_idx]}\")\n",
    "        feature_acts[:,:,feat_idx] *= feat_activ\n",
    "        print(f\"feat activ: {feature_acts[:,:,feat_idx]}\")\n",
    "        print(f\"feat activ: {feature_acts.shape}\")\n",
    "        print(f\"feat activ: {feature_acts}\")\n",
    "        print(\"feature_acts[:,:,idx].sum(): (should be batch size x len seq x feat val)\", feature_acts[:,:,feat_idx].sum())\n",
    "        sae_out = sparse_autoencoder.hook_sae_out(\n",
    "            einops.einsum(\n",
    "                feature_acts,\n",
    "                sparse_autoencoder.W_dec,\n",
    "                \"... d_sae, d_sae d_in -> ... d_in\",\n",
    "            )\n",
    "            + sparse_autoencoder.b_dec\n",
    "        )\n",
    "        \n",
    "        print(f\"sae_out.shape: {sae_out.shape}\")\n",
    "        print(f\"sae_out: {sae_out}\")\n",
    "\n",
    "        # allows normalization. Possibly identity if no normalization\n",
    "        sae_out = sparse_autoencoder.run_time_activation_norm_fn_out(sae_out)\n",
    "        return sae_out\n",
    "    return standard_replacement_hook\n",
    "\n",
    "\n",
    "def steering_hook_fn(\n",
    "    activations, cfg, hook, sae, steering_indices, steering_strength=1.0, mean_ablation_values=None, include_error=False\n",
    "\n",
    "):\n",
    "    sae.to(activations.device)\n",
    "\n",
    "\n",
    "    sae_input = activations.clone()\n",
    "    sae_output, feature_activations, *data = sae(sae_input)\n",
    "    \n",
    "    steered_feature_activations = feature_activations.clone()\n",
    "    \n",
    "    steered_feature_activations[:, :, steering_indices] = steering_strength\n",
    "\n",
    "    steered_sae_out = einops.einsum(\n",
    "                steered_feature_activations,\n",
    "                sae.W_dec,\n",
    "                \"... d_sae, d_sae d_in -> ... d_in\",\n",
    "            ) + sae.b_dec\n",
    "\n",
    "    steered_sae_out = sae.run_time_activation_norm_fn_out(steered_sae_out)\n",
    "    \n",
    "    print(steered_sae_out.shape)\n",
    "    print(steered_sae_out.shape)\n",
    "    print(f\"steering norm: {(steered_sae_out - sae_output).norm()}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    if include_error:\n",
    "        error = sae_input - sae_output\n",
    "        print(f\"error.norm(): {error.norm()}\")\n",
    "        return steered_sae_out + error\n",
    "    return steered_sae_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_feat_idxs = np.random.randint(0, high=3000, size=(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given feature, set it high/low on maxim activ. imgs and high/low on non-activ images\n",
    "# hook SAE and replace desired feature with 0 or 1 \n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_feature_activations_set_feat(\n",
    "    images: torch.Tensor,\n",
    "    model: torch.nn.Module,\n",
    "    sparse_autoencoder: torch.nn.Module,\n",
    "    encoder_weights: torch.Tensor,\n",
    "    encoder_biases: torch.Tensor,\n",
    "    feature_ids: List[int],\n",
    "    feature_categories: List[str],\n",
    "    top_k: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the highest activating tokens for given features in a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        images: Input images\n",
    "        model: The main model\n",
    "        sparse_autoencoder: The sparse autoencoder\n",
    "        encoder_weights: Encoder weights for selected features\n",
    "        encoder_biases: Encoder biases for selected features\n",
    "        feature_ids: List of feature IDs to analyze\n",
    "        feature_categories: Categories of the features\n",
    "        top_k: Number of top activations to return per feature\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping feature IDs to tuples of (top_indices, top_values)\n",
    "    \"\"\"\n",
    "    _, cache = model.run_with_cache(images, names_filter=[sparse_autoencoder.cfg.hook_point])\n",
    "#     recons_image_embeddings_feat_altered = model.run_with_hooks(\n",
    "#         images,\n",
    "#         fwd_hooks=[(\"blocks.9.hook_mlp_out\", standard_replacement_hook)],\n",
    "#     )\n",
    "    recons_image_embeddings_feat_altered_list = []\n",
    "    for idx in np.array(range(sparse_autoencoder.W_dec.shape[0]))[random_feat_idxs]:\n",
    "        print(f\"Feature: {idx} ====================\")\n",
    "        \n",
    "        steering_hook = partial(\n",
    "            steering_hook_fn,\n",
    "            cfg=cfg,\n",
    "            sae=sparse_autoencoder,\n",
    "            steering_indices=[idx],\n",
    "            steering_strength=10.0,\n",
    "            mean_ablation_values = [1.0],\n",
    "            include_error=True,\n",
    "            )\n",
    "        \n",
    "        \n",
    "        recons_image_embeddings_feat_altered = model.run_with_hooks(\n",
    "            images,\n",
    "#             fwd_hooks=[(\"blocks.9.hook_mlp_out\", standard_replacement_hook_curry(idx, 10.0))],\n",
    "            fwd_hooks=[(\"blocks.9.hook_mlp_out\", steering_hook)],\n",
    "        )\n",
    "        recons_image_embeddings_feat_altered_list.append(recons_image_embeddings_feat_altered)\n",
    "\n",
    "    \n",
    "    # output is in clip embedding space\n",
    "    recons_image_embeddings_default = model.run_with_hooks(\n",
    "        images,\n",
    "        fwd_hooks=[(\"blocks.9.hook_mlp_out\", lambda x, hook: x)],\n",
    "    )\n",
    "    \n",
    "    print(f\"recons_image_embeddings_default: {recons_image_embeddings_default}\")\n",
    "    print(f\"recons_image_embeddings_default.shape: {recons_image_embeddings_default.shape}\")\n",
    "    print(f\"recons_image_embeddings_default: {recons_image_embeddings_default.shape}\")\n",
    "\n",
    "    print(f\"recons_image_embeddings_feat_altered: {recons_image_embeddings_feat_altered}\")\n",
    "    print(f\"recons_image_embeddings_feat_altered.shape: {recons_image_embeddings_feat_altered.shape}\")\n",
    "\n",
    "    return recons_image_embeddings_feat_altered_list, recons_image_embeddings_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                            | 0/1562 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 2434 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2843 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1418 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 32 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 749 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.815673828125\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2600 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.9716796875\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1035 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2929 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.999755859375\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 690 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2789 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1346 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.995849609375\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2473 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 469 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1730 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.999755859375\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1507 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1462 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 80 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2295 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 529 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.9736328125\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 184 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 1585 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 289 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.00048828125\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2790 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2007 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                    | 1/1562 [00:02<55:10,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 561.3306884765625\n",
      "Feature: 2452 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 561.3306884765625\n",
      "recons_image_embeddings_default: tensor([[ 0.0352,  0.0083, -0.0740,  ..., -0.0311,  0.0275,  0.0019],\n",
      "        [-0.0101, -0.0539, -0.0622,  ...,  0.0199, -0.0555, -0.0743],\n",
      "        [-0.0206,  0.0059, -0.0366,  ..., -0.0307,  0.0756, -0.0016],\n",
      "        ...,\n",
      "        [ 0.0099, -0.0045, -0.0059,  ..., -0.0521,  0.0647, -0.0225],\n",
      "        [-0.0422,  0.0518, -0.0482,  ...,  0.0098,  0.0418,  0.0290],\n",
      "        [-0.0411, -0.0590,  0.0014,  ..., -0.0432, -0.0089, -0.0449]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[-0.0054,  0.0261,  0.0094,  ..., -0.0455, -0.0191, -0.0197],\n",
      "        [-0.0107,  0.0246,  0.0123,  ..., -0.0425, -0.0270, -0.0206],\n",
      "        [-0.0110,  0.0228,  0.0150,  ..., -0.0394, -0.0151, -0.0157],\n",
      "        ...,\n",
      "        [-0.0050,  0.0304,  0.0168,  ..., -0.0439, -0.0143, -0.0205],\n",
      "        [-0.0134,  0.0294,  0.0086,  ..., -0.0359, -0.0200, -0.0209],\n",
      "        [-0.0123,  0.0203,  0.0140,  ..., -0.0439, -0.0230, -0.0234]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 2434 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2843 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1418 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 32 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 749 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.95166015625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2600 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.9921875\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1035 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2929 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 690 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2789 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1346 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.99853515625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2473 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 469 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1730 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.999755859375\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1507 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1462 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 80 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2295 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 529 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.9931640625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 184 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 1585 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 289 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.00048828125\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2790 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2007 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 560.2279052734375\n",
      "Feature: 2452 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                                                   | 2/1562 [00:03<45:40,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 560.2279052734375\n",
      "recons_image_embeddings_default: tensor([[ 0.0146, -0.0148, -0.0460,  ...,  0.0118,  0.0082,  0.0083],\n",
      "        [-0.0018,  0.0212, -0.0113,  ...,  0.0519, -0.0585, -0.0361],\n",
      "        [-0.0171, -0.0393, -0.0432,  ...,  0.0160,  0.0028,  0.0136],\n",
      "        ...,\n",
      "        [-0.0224, -0.0082, -0.0361,  ..., -0.0352,  0.0784,  0.0265],\n",
      "        [-0.0062,  0.0247, -0.0572,  ...,  0.0121, -0.0083,  0.0222],\n",
      "        [-0.0130,  0.0321, -0.0363,  ...,  0.0437,  0.0279, -0.0109]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[-0.0117,  0.0219,  0.0167,  ..., -0.0393, -0.0157, -0.0238],\n",
      "        [-0.0100,  0.0287,  0.0138,  ..., -0.0336, -0.0285, -0.0187],\n",
      "        [-0.0032,  0.0236,  0.0072,  ..., -0.0408, -0.0196, -0.0208],\n",
      "        ...,\n",
      "        [-0.0108,  0.0250,  0.0119,  ..., -0.0396, -0.0201, -0.0134],\n",
      "        [-0.0115,  0.0228,  0.0148,  ..., -0.0373, -0.0203, -0.0176],\n",
      "        [-0.0094,  0.0240,  0.0103,  ..., -0.0340, -0.0188, -0.0205]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 2434 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2843 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1418 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.99853515625\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 32 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 749 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.96923828125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2600 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.986328125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1035 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.999755859375\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2929 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 690 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.999755859375\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2789 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1346 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.998046875\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2473 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 469 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1730 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.999755859375\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1507 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1462 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 80 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2295 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 529 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.995849609375\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 184 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 1585 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 289 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.00048828125\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2790 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2007 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 559.0997314453125\n",
      "Feature: 2452 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 559.0997314453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                                                   | 3/1562 [00:05<42:34,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recons_image_embeddings_default: tensor([[ 0.0294,  0.0383,  0.0048,  ..., -0.0036,  0.0256,  0.0279],\n",
      "        [ 0.0004,  0.0353, -0.0868,  ..., -0.0146,  0.0002,  0.0059],\n",
      "        [ 0.0709, -0.0185, -0.0175,  ...,  0.0050,  0.0293,  0.0257],\n",
      "        ...,\n",
      "        [-0.0168, -0.0003, -0.0274,  ..., -0.0302,  0.0601, -0.0477],\n",
      "        [ 0.0075,  0.0213, -0.0235,  ..., -0.0346,  0.0216,  0.0487],\n",
      "        [ 0.0059, -0.0119, -0.0019,  ...,  0.0249, -0.0424,  0.0157]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[-0.0084,  0.0291,  0.0134,  ..., -0.0376, -0.0192, -0.0171],\n",
      "        [-0.0068,  0.0291,  0.0043,  ..., -0.0407, -0.0195, -0.0196],\n",
      "        [-0.0044,  0.0239,  0.0132,  ..., -0.0417, -0.0180, -0.0151],\n",
      "        ...,\n",
      "        [-0.0087,  0.0200,  0.0116,  ..., -0.0372, -0.0121, -0.0241],\n",
      "        [-0.0091,  0.0282,  0.0199,  ..., -0.0431, -0.0200, -0.0136],\n",
      "        [-0.0088,  0.0225,  0.0119,  ..., -0.0340, -0.0208, -0.0180]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 2434 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2843 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1418 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.999755859375\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 32 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 749 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.95947265625\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2600 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.97998046875\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1035 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.999755859375\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2929 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.999755859375\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 690 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2789 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1346 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.988037109375\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2473 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 469 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1730 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.999755859375\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1507 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1462 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 80 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2295 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 529 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.887939453125\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 184 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 1585 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 289 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.00048828125\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2790 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                                                   | 4/1562 [00:06<40:54,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2007 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 564.427001953125\n",
      "Feature: 2452 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 564.427001953125\n",
      "recons_image_embeddings_default: tensor([[ 0.0495,  0.0061, -0.0375,  ..., -0.0073, -0.0049,  0.0464],\n",
      "        [ 0.0656,  0.0185, -0.0169,  ..., -0.0542,  0.0806,  0.0280],\n",
      "        [ 0.0439,  0.0136,  0.0194,  ..., -0.0279,  0.0640, -0.0370],\n",
      "        ...,\n",
      "        [ 0.0259,  0.0402, -0.0065,  ..., -0.0289,  0.0129,  0.0450],\n",
      "        [ 0.0245,  0.0248, -0.0074,  ..., -0.0344,  0.0273, -0.0038],\n",
      "        [ 0.0167,  0.0346, -0.0975,  ...,  0.0074,  0.0849, -0.0346]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[-0.0037,  0.0273,  0.0129,  ..., -0.0443, -0.0217, -0.0148],\n",
      "        [-0.0055,  0.0269,  0.0152,  ..., -0.0459, -0.0169, -0.0194],\n",
      "        [-0.0131,  0.0242,  0.0157,  ..., -0.0382, -0.0093, -0.0207],\n",
      "        ...,\n",
      "        [-0.0114,  0.0318,  0.0174,  ..., -0.0428, -0.0226, -0.0163],\n",
      "        [-0.0043,  0.0276,  0.0078,  ..., -0.0419, -0.0178, -0.0165],\n",
      "        [-0.0107,  0.0265,  0.0102,  ..., -0.0428, -0.0167, -0.0158]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n",
      "Feature: 2434 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2843 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1418 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 32 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 749 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.94775390625\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2600 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.973388671875\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1035 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.999755859375\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2929 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 690 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2789 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.9970703125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1346 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.99755859375\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2473 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 469 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1730 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.999755859375\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1507 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1462 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 80 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.00048828125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2295 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 529 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 3999.972900390625\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 184 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 1585 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 289 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.00048828125\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2790 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.0\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2007 ====================\n",
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 563.367431640625\n",
      "Feature: 2452 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                                                   | 4/1562 [00:08<53:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 768])\n",
      "torch.Size([32, 50, 768])\n",
      "steering norm: 4000.000244140625\n",
      "error.norm(): 563.367431640625\n",
      "recons_image_embeddings_default: tensor([[-0.0659, -0.0776, -0.0139,  ..., -0.0386,  0.0279,  0.0018],\n",
      "        [ 0.0148, -0.0243,  0.0026,  ..., -0.0218,  0.0321,  0.0376],\n",
      "        [-0.0408, -0.0001, -0.0266,  ..., -0.0062,  0.0039, -0.0037],\n",
      "        ...,\n",
      "        [-0.0287,  0.0508, -0.0474,  ...,  0.0316,  0.0009,  0.0108],\n",
      "        [-0.0258, -0.0096,  0.0075,  ..., -0.0291, -0.0626, -0.0089],\n",
      "        [ 0.0060, -0.0028, -0.0319,  ..., -0.0128,  0.0170, -0.0358]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_default.shape: torch.Size([32, 512])\n",
      "recons_image_embeddings_default: torch.Size([32, 512])\n",
      "recons_image_embeddings_feat_altered: tensor([[-0.0138,  0.0199,  0.0133,  ..., -0.0445, -0.0163, -0.0183],\n",
      "        [-0.0103,  0.0243,  0.0165,  ..., -0.0430, -0.0206, -0.0148],\n",
      "        [-0.0085,  0.0230,  0.0123,  ..., -0.0415, -0.0200, -0.0171],\n",
      "        ...,\n",
      "        [-0.0097,  0.0249,  0.0051,  ..., -0.0440, -0.0204, -0.0182],\n",
      "        [-0.0116,  0.0260,  0.0134,  ..., -0.0395, -0.0220, -0.0223],\n",
      "        [-0.0065,  0.0242,  0.0101,  ..., -0.0401, -0.0200, -0.0220]],\n",
      "       device='cuda:0')\n",
      "recons_image_embeddings_feat_altered.shape: torch.Size([32, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "max_samples = cfg.eval_max\n",
    "\n",
    "# top_activations = {i: (None, None) for i in interesting_features_indices}\n",
    "encoder_biases = sparse_autoencoder.b_enc#[interesting_features_indices]\n",
    "encoder_weights = sparse_autoencoder.W_enc#[:, interesting_features_indices]\n",
    "\n",
    "top_k=10\n",
    "processed_samples = 0\n",
    "default_embeds_list = []\n",
    "feature_steered_embeds = defaultdict(list)\n",
    "l = 0\n",
    "for batch_images, _, batch_indices in tqdm(val_dataloader, total=max_samples // cfg.batch_size):\n",
    "    batch_images = batch_images.to(cfg.device)\n",
    "    batch_indices = batch_indices.to(cfg.device)\n",
    "    batch_size = batch_images.shape[0]\n",
    "\n",
    "    altered_embeds_list, default_embeds = compute_feature_activations_set_feat(\n",
    "        batch_images, model, sparse_autoencoder, encoder_weights, encoder_biases,\n",
    "        None, None, top_k\n",
    "    )\n",
    "    default_embeds_list.append(default_embeds)\n",
    "    for j, altered_embeds in enumerate(altered_embeds_list):\n",
    "        feature_steered_embeds[random_feat_idxs[j]].extend(altered_embeds)\n",
    "    # either label embeds or optimize to maximal token in text transformer embedding face\n",
    "    l += 1\n",
    "    if l >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_steered_embeds[random_feat_idxs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([160, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_embeds.shape\n",
    "len(default_embeds_list)\n",
    "default_embeds = torch.cat(default_embeds_list)\n",
    "default_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, torch.Size([32, 512]), torch.Size([160, 512]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(altered_embeds_list), altered_embeds_list[0].shape, default_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"/workspace/clip_dissect_raw.txt\", \"r\") as f:\n",
    "    larger_vocab = [line[:-1] for line in f.readlines()][:5000]\n",
    "\n",
    "# with open(\"/workspace/better_img_desc.txt\", \"r\") as f:\n",
    "#     larger_vocab = [line[:-1] for line in f.readlines()][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_features_normed.shape: torch.Size([5000, 512])\n",
      "2434\n",
      "2843\n",
      "1418\n",
      "32\n",
      "749\n",
      "2600\n",
      "1035\n",
      "2929\n",
      "690\n",
      "2789\n",
      "1346\n",
      "2473\n",
      "469\n",
      "1730\n",
      "1507\n",
      "1462\n",
      "80\n",
      "2295\n",
      "529\n",
      "184\n",
      "1585\n",
      "289\n",
      "2790\n",
      "2007\n",
      "2452\n",
      "Label probs altered: torch.Size([160, 5000])\n",
      "Label probs default: torch.Size([160, 5000])\n"
     ]
    }
   ],
   "source": [
    "# use clip vocab here and compare embeds\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "text = tokenizer(larger_vocab)\n",
    "text_features = og_model.encode_text(text.cuda())\n",
    "text_features_normed = text_features/text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "print(f\"text_features_normed.shape: {text_features_normed.shape}\")\n",
    "text_probs_altered_list = []\n",
    "# can probs make this one tensor \n",
    "with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "    for key in feature_steered_embeds:\n",
    "        print(key)\n",
    "        # embeds already have L2 norm of 1\n",
    "        text_probs_altered = (100.0 * torch.stack(feature_steered_embeds[key]) @ text_features_normed.T).softmax(dim=-1)\n",
    "        text_probs_altered_list.append(text_probs_altered)\n",
    "    text_probs_default = (100.0 * default_embeds @ text_features_normed.T).softmax(dim=-1)\n",
    "\n",
    "\n",
    "# for altered_embeds in altered_embeds_list:\n",
    "#     with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "#         # might want to still normalize\n",
    "        \n",
    "#         # already normalized\n",
    "#         # altered_embeds /= altered_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#         text_probs_altered = (100.0 * altered_embeds @ text_features_normed.T).softmax(dim=-1)\n",
    "#         text_probs_altered_list.append(text_probs_altered)\n",
    "#     # default_embds_norm = default_embeds.norm(dim=-1, keepdim=True)\n",
    "#     text_probs_default = (100.0 * default_embeds @ text_features_normed.T).softmax(dim=-1)\n",
    "\n",
    "print(\"Label probs altered:\", text_probs_altered.shape)  # prints: [[1., 0., 0.]]\n",
    "print(\"Label probs default:\", text_probs_default.shape)  # prints: [[1., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([2434, 2843, 1418, 32, 749, 2600, 1035, 2929, 690, 2789, 1346, 2473, 469, 1730, 1507, 1462, 80, 2295, 529, 184, 1585, 289, 2790, 2007, 2452])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_steered_embeds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3729e-04, 1.4903e-04, 6.8765e-05,  ..., 5.7455e-05, 3.8794e-06,\n",
       "         1.3545e-03],\n",
       "        [1.2647e-04, 1.3568e-04, 7.2343e-05,  ..., 6.3099e-05, 4.0338e-06,\n",
       "         1.5959e-03],\n",
       "        [1.0636e-04, 1.0762e-04, 5.8511e-05,  ..., 5.8283e-05, 4.1565e-06,\n",
       "         1.1170e-03],\n",
       "        ...,\n",
       "        [1.2676e-04, 1.0716e-04, 5.4520e-05,  ..., 7.5398e-05, 5.4405e-06,\n",
       "         1.7228e-03],\n",
       "        [1.2987e-04, 9.3910e-05, 5.0661e-05,  ..., 9.3180e-05, 4.4964e-06,\n",
       "         1.1990e-03],\n",
       "        [1.5640e-04, 1.5824e-04, 6.3191e-05,  ..., 9.7872e-05, 3.6782e-06,\n",
       "         1.1201e-03]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_probs_altered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summed Logit Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================\n",
      "\n",
      "For Feature 2434\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0064, 0.0064,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0064, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0064, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['floor' 'ads' 'met' ... 'run' 'set' 'sole']\n",
      " ['ny' 'met' 'got' ... 'run' 'ff' 'arrival']\n",
      " ['ny' 'pet' 'floor' ... 'nintendo' 'amsterdam' 'pa']\n",
      " ...\n",
      " ['ny' 'homepage' 'figure' ... 'ads' 'toy' 'amazon']\n",
      " ['met' 'restaurants' 'floor' ... 'figure' 'ny' 'hall']\n",
      " ['met' 'ny' 'game' ... 'sets' 'got' 'places']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([17.1667, 14.1610,  7.5290,  5.4182,  4.5506,  3.0757,  2.8357,  2.7700,\n",
      "         2.3444,  2.0970], device='cuda:0')\n",
      "['ny' 'met' 'got' 'restaurants' 'ads' 'homepage' 'up' 'amazon' 'food'\n",
      " 'floor']\n",
      "tensor([-2.3710, -2.0290, -1.7595, -1.5459, -1.4837, -1.3091, -1.2239, -1.2053,\n",
      "        -1.1591, -1.1526], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([3093649.2500, 2332244.7500, 2179414.7500, 1977857.3750, 1265464.2500,\n",
      "         640249.1250,  559837.4375,  544405.5000,  493757.4062,  310411.4688],\n",
      "       device='cuda:0')\n",
      "['restaurants' 'ny' 'philadelphia' 'arrival' 'foods' 'street' 'got'\n",
      " 'recipes' 'toronto' 'food']\n",
      "tensor([-2.3710, -2.0290, -1.7595, -1.5459, -1.4837, -1.3091, -1.2239, -1.2053,\n",
      "        -1.1591, -1.1526], device='cuda:0')\n",
      "['sending' 'holder' 'attachment' 'removed' 'cartridge' 'modified'\n",
      " 'secondary' 'excess' 'recommend' 'shemale']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2843\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['crm' 'trailer' 'teams' ... 'dish' 'sets' 'talk']\n",
      " ['staff' 'farmers' 'hills' ... 'administrative' 'careers'\n",
      "  'collaboration']\n",
      " ['staff' 'administrative' 'counties' ... 'admin' 'coordinator' 'talks']\n",
      " ...\n",
      " ['skills' 'counties' 'careers' ... 'cisco' 'trailer' 'covers']\n",
      " ['row' 'offices' 'trailer' ... 'kitchen' 'cabinet' 'clinic']\n",
      " ['row' 'agriculture' 'hills' ... 'census' 'farmers' 'admin']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([2.3714, 2.2631, 2.2385, 2.0799, 1.9783, 1.9509, 1.6179, 1.3155, 1.2025,\n",
      "        1.1904], device='cuda:0')\n",
      "['administrative' 'crm' 'row' 'pakistan' 'skills' 'staff' 'careers'\n",
      " 'counter' 'recipes' 'trailer']\n",
      "tensor([-2.3865, -2.0342, -1.7595, -1.5831, -1.4823, -1.2898, -1.2831, -1.2051,\n",
      "        -1.1867, -1.1356], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([4088748.7500, 2038204.8750, 1732841.2500, 1271274.8750,  943218.6250,\n",
      "         808079.6250,  482649.5625,  434694.5938,  325007.5625,  324407.2188],\n",
      "       device='cuda:0')\n",
      "['recipes' 'farmers' 'pakistan' 'counties' 'careers' 'kitchen' 'maine'\n",
      " 'catering' 'offices' 'catalog']\n",
      "tensor([-2.3865, -2.0342, -1.7595, -1.5831, -1.4823, -1.2898, -1.2831, -1.2051,\n",
      "        -1.1867, -1.1356], device='cuda:0')\n",
      "['passed' 'opened' 'aged' 'three' 'rolling' 'chip' 'satisfaction'\n",
      " 'marshall' 'four' 'comfortable']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1418\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['qty' 'competition' 'boards' ... 'board' 'b' 'clubs']\n",
      " ['boards' 'winners' 'competition' ... 'sweden' 'sending' 'ski']\n",
      " ['paypal' 'prize' 'pci' ... 'counties' 'isbn' 'bc']\n",
      " ...\n",
      " ['handle' 'card' 'binding' ... 'tool' 'catalog' 'isbn']\n",
      " ['boards' 'binding' 'bc' ... 'storage' 'paperback' 'before']\n",
      " ['membership' 'paperback' 'bc' ... 'pci' 'awards' 'award']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([4.7908, 4.0096, 3.0343, 2.9296, 2.3872, 2.3225, 2.2508, 1.9372, 1.8594,\n",
      "        1.8040], device='cuda:0')\n",
      "['binding' 'card' 'prize' 'clubs' 'paperback' 'crafts' 'competition'\n",
      " 'amazon' 'cards' 'paypal']\n",
      "tensor([-2.3869, -2.0335, -1.7591, -1.5872, -1.4875, -1.3144, -1.2004, -1.1457,\n",
      "        -1.1356, -1.1200], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'male' 'seal'\n",
      " 'fox']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1632140.2500, 1128808.5000,  726960.4375,  708571.6250,  631060.6875,\n",
      "         580636.9375,  363693.2188,  335835.5625,  310594.7188,  264362.3125],\n",
      "       device='cuda:0')\n",
      "['crafts' 'bears' 'cincinnati' 'labels' 'counties' 'shops' 'colleges'\n",
      " 'boats' 'catalog' 'catalogue']\n",
      "tensor([-2.3869, -2.0335, -1.7591, -1.5872, -1.4875, -1.3144, -1.2004, -1.1457,\n",
      "        -1.1356, -1.1200], device='cuda:0')\n",
      "['tank' 'wall' 'cell' 'walls' 'ground' 'alexander' 'jump' 'kim' 'amp'\n",
      " 'alive']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 32\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0065, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['d' 'aluminum' 'addition' ... 'qty' 'template' 'coins']\n",
      " ['dean' 'cast' 'owners' ... 'accordance' 'dual' 'combined']\n",
      " ['dean' 'stainless' 'md' ... 'dot' 'commission' 'y']\n",
      " ...\n",
      " ['adapter' 'stainless' 'quarter' ... 'cast' 'tony' 'converter']\n",
      " ['dildo' 'addition' 'dean' ... 'd' 'addresses' 'columns']\n",
      " ['cover' 'outstanding' 'certificate' ... 'disc' 'programme' 'interior']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([11.9496,  6.4097,  3.3576,  3.0610,  2.4719,  2.4240,  2.3504,  1.7277,\n",
      "         1.7058,  1.5218], device='cuda:0')\n",
      "['dean' 'dildo' 'adapter' 'stainless' 'aluminum' 'md' 'd' 'certificate'\n",
      " 'addition' 'outstanding']\n",
      "tensor([-2.3844, -2.0309, -1.7338, -1.4849, -1.4350, -1.3152, -1.2572, -1.2042,\n",
      "        -1.1713, -1.1602], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'python' 'ray' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1541518.2500,  721258.5000,  638470.8750,  569519.6875,  480975.0938,\n",
      "         433585.5938,  240822.8594,  220276.3750,  193106.7500,  183286.3438],\n",
      "       device='cuda:0')\n",
      "['dean' 'cincinnati' 'companies' 'dildo' 'logos' 'certificate' 'aluminum'\n",
      " 'label' 'certificates' 'quotes']\n",
      "tensor([-2.3844, -2.0309, -1.7338, -1.4849, -1.4350, -1.3152, -1.2572, -1.2042,\n",
      "        -1.1713, -1.1602], device='cuda:0')\n",
      "['picture' 'hunter' 'livecam' 'small' 'photos' 'waste' 'ill' 'photo'\n",
      " 'herself' 'still']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 749\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['largest' 'font' 'acquisition' ... 'partnership' 'diff' 'typical']\n",
      " ['largest' 'partnership' 'acquisition' ... 'large' 'equal' 'arguments']\n",
      " ['permalink' 'polls' 'poll' ... 'grants' 'pdt' 'specialty']\n",
      " ...\n",
      " ['johnson' 'segment' 'nationwide' ... 'sizes' 'sec' 'thumbnail']\n",
      " ['candidates' 'settlement' 'replacement' ... 'sets' 'equal' 'matching']\n",
      " ['largest' 'font' 'consolidation' ... 'acquisition' 'licensing' 'census']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([10.4546,  5.0600,  4.1315,  2.2826,  2.0263,  1.9252,  1.8138,  1.7720,\n",
      "         1.6894,  1.5305], device='cuda:0')\n",
      "['largest' 'johnson' 'partnership' 'large' 'consolidation' 'acquisition'\n",
      " 'font' 'candidates' 'size' 'permalink']\n",
      "tensor([-2.3868, -2.0353, -1.7380, -1.5862, -1.4881, -1.3153, -1.2489, -1.2062,\n",
      "        -1.1949, -1.1568], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([252741.1094, 172447.7344, 113792.8125, 104222.0391,  94823.5234,\n",
      "         94772.1719,  73246.4531,  71921.3281,  64916.1953,  63760.0625],\n",
      "       device='cuda:0')\n",
      "['consolidation' 'candidates' 'partnership' 'bowl' 'kansas' 'community'\n",
      " 'warranty' 'nba' 'largest' 'arguments']\n",
      "tensor([-2.3868, -2.0353, -1.7380, -1.5862, -1.4881, -1.3153, -1.2489, -1.2062,\n",
      "        -1.1949, -1.1568], device='cuda:0')\n",
      "['smoking' 'smoke' 'nurse' 'princess' 'my' 'writes' 'tobacco' 'diary'\n",
      " 'mouse' 'july']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2600\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0067, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0065, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['installed' 'brief' 'sets' ... 'centres' 'bestiality' 'centre']\n",
      " ['representatives' 'activities' 'consultants' ... 'sorted'\n",
      "  'respectively' 'clinic']\n",
      " ['representative' 'installed' 'collectables' ... 'foster' 'brief'\n",
      "  'conservative']\n",
      " ...\n",
      " ['representatives' 'installed' 'representative' ... 'specifically'\n",
      "  'article' 'laser']\n",
      " ['representatives' 'installed' 'sets' ... 'cases' 'clinic' 'courts']\n",
      " ['centre' 'center' 'centres' ... 'committees' 'activities' 'sets']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([95.8284,  2.3830,  2.0257,  1.3113,  1.2969,  1.2723,  1.2361,  1.1434,\n",
      "         1.0306,  1.0139], device='cuda:0')\n",
      "['representatives' 'representative' 'collectables' 'plates' 'committees'\n",
      " 'brief' 'constitutes' 'bestiality' 'programmes' 'centres']\n",
      "tensor([-2.3870, -2.0358, -1.7566, -1.5908, -1.4887, -1.3154, -1.2833, -1.2064,\n",
      "        -1.1730, -1.1519], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([7834349.0000,  267806.8125,  180225.7969,  176144.2344,  149472.6094,\n",
      "         149215.5781,  130653.3984,  121009.5547,  118544.4531,  109374.0859],\n",
      "       device='cuda:0')\n",
      "['representatives' 'collectables' 'restaurants' 'activities'\n",
      " 'organisations' 'researchers' 'companies' 'consultants' 'physicians'\n",
      " 'volunteers']\n",
      "tensor([-2.3870, -2.0358, -1.7566, -1.5908, -1.4887, -1.3154, -1.2833, -1.2064,\n",
      "        -1.1730, -1.1519], device='cuda:0')\n",
      "['narrow' 'unknown' 'serious' 'anthony' 'outdoors' 'harbor' 'afternoon'\n",
      " 'gordon' 'obvious' 'jackson']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1035\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0064, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['flat' 'leg' 'foot' ... 'c' 'symbol' 'site']\n",
      " ['leg' 'flat' 'foot' ... 'wall' 'site' 'board']\n",
      " ['star' 'fan' 'fox' ... 'photo' 'column' 'stone']\n",
      " ...\n",
      " ['leg' 'memorabilia' 'tube' ... 'console' 'r' 'laser']\n",
      " ['lounge' 'flat' 'leg' ... 'interior' 'hall' 'indoor']\n",
      " ['lounge' 'memorial' 'column' ... 'score' 'interior' 'library']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([15.0479,  7.9055,  4.7027,  3.6334,  2.7743,  2.5065,  2.1720,  1.6029,\n",
      "         1.5630,  1.4431], device='cuda:0')\n",
      "['tube' 'leg' 'flat' 'foot' 'atom' 'column' 'lounge' 'memorabilia' 'star'\n",
      " 'sponsor']\n",
      "tensor([-2.3865, -2.0091, -1.7542, -1.4709, -1.3320, -1.2999, -1.2751, -1.1921,\n",
      "        -1.1492, -1.1302], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'python' 'ray' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([3627206.0000, 1559293.1250,  406472.0000,  321267.3125,  179266.1094,\n",
      "         178879.6406,  176376.5625,  140088.1406,  132923.9844,  112562.7266],\n",
      "       device='cuda:0')\n",
      "['lounge' 'apartment' 'pharmacy' 'poland' 'warehouse' 'apartments'\n",
      " 'bedroom' 'fan' 'column' 'library']\n",
      "tensor([-2.3865, -2.0091, -1.7542, -1.4709, -1.3320, -1.2999, -1.2751, -1.1921,\n",
      "        -1.1492, -1.1302], device='cuda:0')\n",
      "['msn' 'milfhunter' 'carefully' 'edge' 'particular' 'adults'\n",
      " 'respectively' 'modified' 'apache' 'double']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2929\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0064, 0.0064, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['detroit' 'dish' 'strip' ... 'substances' 'served' 'salt']\n",
      " ['ontario' 'officers' 'detroit' ... 'cleveland' 'crew' 'northern']\n",
      " ['ontario' 'detroit' 'unfortunately' ... 'smoking' 'officer' 'wi']\n",
      " ...\n",
      " ['gun' 'weapons' 'ontario' ... 'range' 'jersey' 'michigan']\n",
      " ['restaurants' 'dish' 'connecticut' ... 'windows' 'transportation'\n",
      "  'tripadvisor']\n",
      " ['dish' 'detroit' 'connecticut' ... 'served' 'settlement' 'counties']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([7.7753, 7.2736, 4.4373, 3.8749, 3.1381, 2.7898, 2.2929, 1.7745, 1.7215,\n",
      "        1.6743], device='cuda:0')\n",
      "['dish' 'ontario' 'food' 'detroit' 'served' 'officers' 'cleveland'\n",
      " 'kentucky' 'unfortunately' 'connecticut']\n",
      "tensor([-2.3835, -2.0335, -1.7426, -1.5898, -1.4401, -1.2895, -1.2453, -1.2051,\n",
      "        -1.1945, -1.1626], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([3611421.2500, 1903337.3750, 1795752.0000, 1792671.8750, 1498734.3750,\n",
      "         913432.5000,  758236.9375,  733655.4375,  717380.6875,  695968.4375],\n",
      "       device='cuda:0')\n",
      "['connecticut' 'kentucky' 'detroit' 'bears' 'foods' 'disclosure'\n",
      " 'michigan' 'pittsburgh' 'tripadvisor' 'restaurants']\n",
      "tensor([-2.3835, -2.0335, -1.7426, -1.5898, -1.4401, -1.2895, -1.2453, -1.2051,\n",
      "        -1.1945, -1.1626], device='cuda:0')\n",
      "['sold' 'mp' 'mailing' 'king' 'postal' 'rc' 'postposted' 'fell'\n",
      " 'available' 'smith']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 690\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0065, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['pottery' 'foundation' 'clay' ... 'aid' 'qty' 'url']\n",
      " ['lodging' 'wv' 'log' ... 'lodge' 'camp' 'contractors']\n",
      " ['contest' 'dogs' 'pa' ... 'pull' 'straight' 'tag']\n",
      " ...\n",
      " ['auctions' 'wv' 'womens' ... 'homepage' 'bidding' 'gun']\n",
      " ['lodging' 'rentals' 'accommodation' ... 'wv' 'rental' 'stands']\n",
      " ['oct' 'housing' 'lodging' ... 'foundation' 'seminars' 'accommodation']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([6.6803, 3.7390, 2.7317, 2.7181, 2.7131, 2.2345, 2.1568, 2.0244, 1.7393,\n",
      "        1.7030], device='cuda:0')\n",
      "['lodging' 'womens' 'rentals' 'auctions' 'pottery' 'wv' 'contest'\n",
      " 'homepage' 'recipes' 'programs']\n",
      "tensor([-2.3836, -1.9525, -1.7595, -1.5857, -1.4767, -1.3116, -1.1951, -1.1943,\n",
      "        -1.1940, -1.1630], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([6039317.5000,  563604.0000,  559300.1875,  497584.3125,  485545.1562,\n",
      "         477827.4688,  306185.6562,  249572.0781,  234479.0469,  227095.3281],\n",
      "       device='cuda:0')\n",
      "['recipes' 'crafts' 'colleges' 'pottery' 'apartments' 'wv' 'apartment'\n",
      " 'scholarship' 'lodging' 'maryland']\n",
      "tensor([-2.3836, -1.9525, -1.7595, -1.5857, -1.4767, -1.3116, -1.1951, -1.1943,\n",
      "        -1.1940, -1.1630], device='cuda:0')\n",
      "['glance' 'swimming' 'fred' 'explorer' 'putting' 'thinking' 'itself'\n",
      " 'inner' 'returned' 'solo']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2789\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0068, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0068, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0064, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['blue' 'cover' 'soul' ... 'signed' 'stars' 'sea']\n",
      " ['front' 'edge' 'view' ... 'dean' 'north' 'season']\n",
      " ['front' 'profile' 'craig' ... 'outside' 'cute' 'pretty']\n",
      " ...\n",
      " ['front' 'rear' 'custom' ... 'these' 'miles' 'back']\n",
      " ['front' 'rear' 'estate' ... 'back' 'located' 'src']\n",
      " ['front' 'cover' 'view' ... 'tour' 'square' 'where']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([47.6611, 11.3530,  2.5422,  1.5170,  1.2855,  1.1138,  1.0908,  1.0156,\n",
      "         0.8909,  0.8478], device='cuda:0')\n",
      "['front' 'cover' 'blue' 'wayne' 'estate' 'edge' 'view' 'best' 'craig'\n",
      " 'ave']\n",
      "tensor([-2.3865, -2.0344, -1.7583, -1.5453, -1.4883, -1.3145, -1.1934, -1.1922,\n",
      "        -1.1327, -1.1196], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'detail' 'eagle' 'seal'\n",
      " 'fox']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1281399.8750,  236311.2031,  212524.7500,  187465.9531,  160121.3438,\n",
      "         154965.2969,  137993.5312,  119627.8984,  113873.0859,   97779.4609],\n",
      "       device='cuda:0')\n",
      "['street' 'shorts' 'stars' 'streets' 'front' 'purple' 'beach' 'blue'\n",
      " 'cover' 'colleges']\n",
      "tensor([-2.3865, -2.0344, -1.7583, -1.5453, -1.4883, -1.3145, -1.1934, -1.1922,\n",
      "        -1.1327, -1.1196], device='cuda:0')\n",
      "['tank' 'charges' 'phones' 'yield' 'filled' 'shaved' 'usb' 'pmid' 'phil'\n",
      " 'measures']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1346\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['got' 'hockey' 'sitemap' ... 'kings' 'mercury' 'ringtone']\n",
      " ['got' 'customers' 'hockey' ... 'mt' 'team' 'family']\n",
      " ['permalink' 'strongly' 'manchester' ... 'golden' 'il' 'post']\n",
      " ...\n",
      " ['tn' 'stainless' 'custom' ... 'blowjobs' 'wallpaper' 'gun']\n",
      " ['tn' 'hockey' 'aa' ... 'lincoln' 'er' 'poverty']\n",
      " ['got' 'tn' 'memorial' ... 'aa' 'post' 'manchester']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([5.4787, 3.3364, 2.6888, 1.9179, 1.5088, 1.4424, 1.2127, 1.2049, 1.0782,\n",
      "        0.9585], device='cuda:0')\n",
      "['got' 'tn' 'customers' 'permalink' 'wallpapers' 'aa' 'post' 'subscribe'\n",
      " 'wallpaper' 'manchester']\n",
      "tensor([-2.3867, -2.0356, -1.7555, -1.5888, -1.4883, -1.3142, -1.2842, -1.2030,\n",
      "        -1.1963, -1.1554], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([2627556.7500, 1777389.6250, 1285786.8750,  981928.3125,  809786.4375,\n",
      "         430407.5625,  408704.0938,  358602.2500,  345929.7812,  324092.0312],\n",
      "       device='cuda:0')\n",
      "['pittsburgh' 'editors' 'cincinnati' 'customers' 'tampa' 'tennessee' 'got'\n",
      " 'wallpapers' 'nashville' 'nba']\n",
      "tensor([-2.3867, -2.0356, -1.7555, -1.5888, -1.4883, -1.3142, -1.2842, -1.2030,\n",
      "        -1.1963, -1.1554], device='cuda:0')\n",
      "['nude' 'feet' 'legs' 'leg' 'ken' 'greg' 'side' 'naked' 'susan' 'horse']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2473\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['teams' 'strip' 'night' ... 'chain' 'rubber' 'aluminum']\n",
      " ['boards' 'teams' 'iowa' ... 'anywhere' 'michigan' 'hundreds']\n",
      " ['night' 'hampshire' 'stick' ... 'german' 'shirts' 'douglas']\n",
      " ...\n",
      " ['cartridge' 'bars' 'items' ... 'johnson' 'pipe' 'charger']\n",
      " ['boards' 'bars' 'tables' ... 'table' 'plates' 'wood']\n",
      " ['trees' 'hampshire' 'evening' ... 'hundreds' 'posts' 'interior']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([4.8792, 3.6457, 2.4325, 2.4188, 2.3663, 2.2956, 1.7484, 1.7039, 1.3546,\n",
      "        1.3494], device='cuda:0')\n",
      "['bars' 'night' 'teams' 'boards' 'cards' 'clubs' 'shirts' 'hampshire'\n",
      " 'cartridge' 'iowa']\n",
      "tensor([-2.3837, -1.9900, -1.7484, -1.5815, -1.4864, -1.3149, -1.2053, -1.1955,\n",
      "        -1.1819, -1.1635], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'detail' 'pair'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1488514.1250, 1023092.1875,  907031.1250,  419497.7188,  413153.7812,\n",
      "         306080.4375,  252146.3281,  197980.2500,  189185.4844,  182319.7969],\n",
      "       device='cuda:0')\n",
      "['shirts' 'trees' 'kansas' 'nebraska' 'iowa' 'teams' 'bowl' 'pittsburgh'\n",
      " 'hampshire' 'night']\n",
      "tensor([-2.3837, -1.9900, -1.7484, -1.5815, -1.4864, -1.3149, -1.2053, -1.1955,\n",
      "        -1.1819, -1.1635], device='cuda:0')\n",
      "['failed' 'model' 'smaller' 'blue' 'milf' 'male' 'filed' 'pda' 'side' 'sw']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 469\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['floor' 'follow' 'portion' ... 'steps' 'following' 'land']\n",
      " ['largest' 'follow' 'iowa' ... 'layer' 'replaced' 'extreme']\n",
      " ['lane' 'chip' 'golden' ... 'still' 'after' 'applying']\n",
      " ...\n",
      " ['cells' 'toyota' 'resume' ... 'iowa' 'skills' 'top']\n",
      " ['portion' 'floor' 'after' ... 'replace' 'placement' 'narrow']\n",
      " ['lane' 'largest' 'land' ... 'floor' 'roads' 'placement']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([7.0487, 5.7333, 2.5213, 2.1937, 1.8490, 1.6697, 1.5192, 1.5009, 1.1148,\n",
      "        1.0742], device='cuda:0')\n",
      "['portion' 'lane' 'chip' 'cells' 'led' 'iowa' 'cellular' 'largest' 'holds'\n",
      " 'toyota']\n",
      "tensor([-2.3773, -2.0240, -1.7475, -1.5899, -1.4877, -1.3087, -1.2739, -1.1865,\n",
      "        -1.1853, -1.1575], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'detail' 'eagle'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1014407.4375,  591925.5000,  538430.3750,  432701.0000,  258482.6562,\n",
      "         221903.1094,  189259.3750,  165989.6875,  150812.5000,  149310.3594],\n",
      "       device='cuda:0')\n",
      "['recipes' 'portion' 'iowa' 'foods' 'pittsburgh' 'led' 'resume' 'roads'\n",
      " 'anime' 'cells']\n",
      "tensor([-2.3773, -2.0240, -1.7475, -1.5899, -1.4877, -1.3087, -1.2739, -1.1865,\n",
      "        -1.1853, -1.1575], device='cuda:0')\n",
      "['acrobat' 'demonstrate' 'thompson' 'domestic' 'concerning' 'coach'\n",
      " 'awareness' 'recorder' 'shemale' 'nude']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1730\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['tracking' 'boot' 'still' ... 'path' 'cycle' 'road']\n",
      " ['tracking' 'twin' 'mount' ... 'walk' 'aim' 'road']\n",
      " ['jack' 'bear' 'tom' ... 'wind' 'myself' 'still']\n",
      " ...\n",
      " ['boot' 'tool' 'driver' ... 'finder' 'milfhunter' 'wheel']\n",
      " ['twin' 'boot' 'move' ... 'rack' 'upper' 'rail']\n",
      " ['move' 'path' 'tool' ... 'mill' 'post' 'turning']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([7.6371, 3.7838, 3.5791, 3.5126, 2.4930, 2.4350, 2.2177, 1.9252, 1.8192,\n",
      "        1.6404], device='cuda:0')\n",
      "['still' 'boot' 'driver' 'tracking' 'twin' 'wheel' 'move' 'mouse' 'jack'\n",
      " 'ipod']\n",
      "tensor([-2.3759, -1.7611, -1.7325, -1.5649, -1.4858, -1.3133, -1.2092, -1.1934,\n",
      "        -1.1566, -1.1436], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'detail' 'eagle'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([200754.6562, 146297.7656, 141460.8750,  86629.3125,  84732.8516,\n",
      "         75355.8672,  72094.3750,  68958.1797,  68915.3359,  61340.4023],\n",
      "       device='cuda:0')\n",
      "['road' 'bear' 'mouse' 'ipod' 'pink' 'driver' 'taken' 'saw' 'tracking'\n",
      " 'bears']\n",
      "tensor([-2.3759, -1.7611, -1.7325, -1.5649, -1.4858, -1.3133, -1.2092, -1.1934,\n",
      "        -1.1566, -1.1436], device='cuda:0')\n",
      "['sw' 'ref' 'share' 'observations' 'consists' 'shaved' 'responsible' 'sri'\n",
      " 'demonstrate' 'available']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1507\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['foundation' 'et' 'twiki' ... 'empty' 'brian' 'proposal']\n",
      " ['poland' 'completed' 'scotland' ... 'ak' 'li' 'edt']\n",
      " ['twiki' 'polish' 'ian' ... 'howard' 'lewis' 'li']\n",
      " ...\n",
      " ['milfhunter' 'ak' 'thompson' ... 'sexcam' 'solo' 'lower']\n",
      " ['up' 'empty' 'ltd' ... 'ron' 'improved' 'replaced']\n",
      " ['proposal' 'howard' 'foundation' ... 'phoenix' 'completed' 'width']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([3.3795, 3.1071, 2.5246, 2.1986, 2.1745, 1.9278, 1.7997, 1.7394, 1.4309,\n",
      "        1.2526], device='cuda:0')\n",
      "['ill' 'up' 'polish' 'twiki' 'brian' 'sean' 'includes' 'sd' 'li' 'ian']\n",
      "tensor([-2.3809, -2.0310, -1.7579, -1.5835, -1.4876, -1.3145, -1.2045, -1.1941,\n",
      "        -1.1923, -1.1183], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'pair' 'detail'\n",
      " 'fox']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1563367.1250,  472381.7188,  384953.9062,  367535.6562,  319679.8750,\n",
      "         211743.7969,  176749.6562,  135821.7344,   84880.0938,   72000.6953],\n",
      "       device='cuda:0')\n",
      "['poland' 'et' 'scotland' 'arrival' 'bedroom' 'philadelphia' 'up' 'thing'\n",
      " 'saw' 'pennsylvania']\n",
      "tensor([-2.3809, -2.0310, -1.7579, -1.5835, -1.4876, -1.3145, -1.2045, -1.1941,\n",
      "        -1.1923, -1.1183], device='cuda:0')\n",
      "['fax' 'mailing' 'massage' 'putting' 'viewing' 'graphics' 'opening'\n",
      " 'imaging' 'hole' 'opera']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1462\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0065, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['pursuant' 'acquisition' 'pair' ... 'patch' 'represents' 'displayed']\n",
      " ['acquisition' 'pair' 'consultation' ... 'consultants' 'represents'\n",
      "  'largest']\n",
      " ['consultation' 'pursuant' 'injuries' ... 'para' 'published' 'delaware']\n",
      " ...\n",
      " ['assumes' 'shown' 'instruments' ... 'para' 'measures' 'issue']\n",
      " ['displayed' 'pair' 'shown' ... 'replacement' 'counsel' 'placement']\n",
      " ['pursuant' 'yards' 'yard' ... 'issue' 'par' 'founded']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([20.9635,  7.6929,  4.8024,  2.6241,  2.2046,  1.5869,  1.5810,  1.4597,\n",
      "         1.2884,  1.1809], device='cuda:0')\n",
      "['assumes' 'pursuant' 'acquisition' 'consultation' 'para' 'yards'\n",
      " 'injuries' 'shown' 'pass' 'pair']\n",
      "tensor([-2.3843, -2.0358, -1.7566, -1.5845, -1.4886, -1.3151, -1.2062, -1.1963,\n",
      "        -1.1342, -1.1297], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'detail' 'male'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([228744.5000, 211913.9219, 148892.6094,  93803.3906,  81097.4609,\n",
      "         73511.9766,  68020.0234,  61386.8828,  57645.5430,  55787.9336],\n",
      "       device='cuda:0')\n",
      "['delaware' 'pubmed' 'assumes' 'discussions' 'consultants' 'statements'\n",
      " 'consultation' 'injuries' 'labels' 'acquisition']\n",
      "tensor([-2.3843, -2.0358, -1.7566, -1.5845, -1.4886, -1.3151, -1.2062, -1.1963,\n",
      "        -1.1342, -1.1297], device='cuda:0')\n",
      "['shooting' 'motor' 'cell' 'slot' 'cst' 'roll' 'photos' 'chips' 'motors'\n",
      " 'debian']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 80\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['board' 'stick' 'counter' ... 'binding' 'of' 'layout']\n",
      " ['board' 'ski' 'boards' ... 'combined' 'press' 'opening']\n",
      " ['stick' 'oxford' 'dog' ... 'po' 'assistant' 'admin']\n",
      " ...\n",
      " ['holder' 'instrument' 'binding' ... 'type' 'toy' 'gun']\n",
      " ['board' 'counter' 'interior' ... 'furniture' 'deck' 'facilities']\n",
      " ['academic' 'oxford' 'interior' ... 'admin' 'department' 'of']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([6.4036, 5.1864, 3.6241, 3.3632, 2.6245, 2.0039, 1.9964, 1.4487, 1.2385,\n",
      "        1.1553], device='cuda:0')\n",
      "['board' 'holder' 'stick' 'binding' 'counter' 'string' 'vol' 'mailing'\n",
      " 'extension' 'partner']\n",
      "tensor([-2.3870, -1.7577, -1.7002, -1.5793, -1.4839, -1.3034, -1.2821, -1.1961,\n",
      "        -1.1895, -1.1631], device='cuda:0')\n",
      "['guinea' 'fucking' 'dog' 'ray' 'python' 'fish' 'pair' 'detail' 'eagle'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([266909.5938, 180005.6250, 115180.1250, 103867.2031, 103151.4141,\n",
      "         74940.9922,  64872.9648,  64457.3203,  62989.5586,  61803.4570],\n",
      "       device='cuda:0')\n",
      "['cities' 'universities' 'kitchen' 'office' 'administrator' 'customer'\n",
      " 'colleges' 'catering' 'departments' 'cincinnati']\n",
      "tensor([-2.3870, -1.7577, -1.7002, -1.5793, -1.4839, -1.3034, -1.2821, -1.1961,\n",
      "        -1.1895, -1.1631], device='cuda:0')\n",
      "['rain' 'photographs' 'removed' 'demonstrate' 'pollution' 'responsible'\n",
      " 'nikon' 'therefore' 'recorded' 'hurricane']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2295\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0064, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['finds' 'of' 'average' ... 'jun' 'or' 'feb']\n",
      " ['carrying' 'yes' 'of' ... 'entries' 'arrived' 'scripts']\n",
      " ['cute' 'pretty' 'me' ... 'seller' 'po' 'porn']\n",
      " ...\n",
      " ['generic' 'for' 'carry' ... 'jersey' 'carrier' 'ms']\n",
      " ['entries' 'pretty' 'by' ... 'perspective' 'yes' 'appreciate']\n",
      " ['gardens' 'of' 'garden' ... 'or' 'my' 'perspective']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([5.6022, 5.2902, 3.9757, 3.6554, 3.4821, 2.1859, 2.1671, 2.1441, 2.0540,\n",
      "        2.0139], device='cuda:0')\n",
      "['generic' 'cute' 'pink' 'pretty' 'for' 'misc' 'carrying' 'of' 'by'\n",
      " 'average']\n",
      "tensor([-2.3776, -2.0352, -1.7580, -1.5866, -1.4884, -1.3149, -1.2063, -1.1963,\n",
      "        -1.1467, -1.1357], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'detail' 'male'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1424080.1250, 1291932.3750,  466311.2188,  363314.7500,  301468.2188,\n",
      "         230724.8750,  178101.0000,  151307.2031,  150246.9219,  150127.5625],\n",
      "       device='cuda:0')\n",
      "['bedroom' 'pink' 'pakistan' 'gardens' 'quotes' 'recipes' 'garden'\n",
      " 'papers' 'purple' 'entries']\n",
      "tensor([-2.3776, -2.0352, -1.7580, -1.5866, -1.4884, -1.3149, -1.2063, -1.1963,\n",
      "        -1.1467, -1.1357], device='cuda:0')\n",
      "['photo' 'remote' 'lead' 'launched' 'laser' 'launch' 'connections' 'head'\n",
      " 'liquid' 'dial']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 529\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['foundation' 'date' 'navy' ... 'matching' 'stone' 'desert']\n",
      " ['korean' 'korea' 'labs' ... 'row' 'cleveland' 'matching']\n",
      " ['labs' 'cute' 'dogs' ... 'london' 'cleveland' 'seal']\n",
      " ...\n",
      " ['navy' 'bit' 'korean' ... 'cleveland' 'antique' 'quarter']\n",
      " ['lighting' 'row' 'windows' ... 'mirror' 'antique' 'addresses']\n",
      " ['green' 'foundation' 'square' ... 'london' 'gardens' 'residential']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([8.4396, 3.8102, 3.1216, 2.6665, 2.6077, 2.3744, 2.2758, 2.2586, 1.9010,\n",
      "        1.4143], device='cuda:0')\n",
      "['green' 'date' 'arizona' 'purple' 'lighting' 'korean' 'thumbnail'\n",
      " 'foundation' 'blue' 'navy']\n",
      "tensor([-1.8394, -1.8390, -1.7235, -1.5741, -1.3138, -1.2979, -1.2304, -1.2001,\n",
      "        -1.1045, -1.0996], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'fox'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([6411545.5000, 1642460.8750, 1200092.6250,  980749.3125,  855922.4375,\n",
      "         756233.1875,  570485.8125,  560641.3125,  522553.0000,  494697.0625],\n",
      "       device='cuda:0')\n",
      "['cincinnati' 'dates' 'arizona' 'philadelphia' 'colleges' 'purple'\n",
      " 'citysearch' 'italy' 'navy' 'sublime']\n",
      "tensor([-1.8394, -1.8390, -1.7235, -1.5741, -1.3138, -1.2979, -1.2304, -1.2001,\n",
      "        -1.1045, -1.0996], device='cuda:0')\n",
      "['demonstrate' 'alarm' 'model' 'elections' 'democratic' 'var' 'temporary'\n",
      " 'arrived' 'vote' 'targets']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 184\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0066, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['rss' 'cart' 'anonymous' ... 'bb' 'bulk' 'cst']\n",
      " ['overall' 'farmers' 'rack' ... 'es' 'users' 'looks']\n",
      " ['overall' 'anonymous' 'po' ... 'walking' 'cute' 'walk']\n",
      " ...\n",
      " ['holder' 'thumbnail' 'rss' ... 'os' 'cm' 'mac']\n",
      " ['cart' 'rack' 'holder' ... 'mm' 'ip' 'ss']\n",
      " ['farm' 'rss' 'sri' ... 'anonymous' 'agricultural' 'ia']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([42.5085,  5.3611,  4.8352,  3.4773,  3.0884,  2.9399,  2.3425,  2.2887,\n",
      "         2.1981,  2.0032], device='cuda:0')\n",
      "['overall' 'cart' 'usr' 'rss' 'sri' 'thumbnail' 'farmers' 'anonymous'\n",
      " 'holder' 'rack']\n",
      "tensor([-2.1898, -1.9823, -1.7587, -1.5895, -1.4819, -1.3134, -1.2838, -1.2058,\n",
      "        -1.1835, -1.1278], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([5211019.0000, 2598207.5000, 1852623.8750, 1537039.0000,  586553.8750,\n",
      "         150774.9531,  148611.4531,  136255.4844,  131813.3125,  124673.2031],\n",
      "       device='cuda:0')\n",
      "['farmers' 'anonymous' 'overall' 'farm' 'rss' 'customer' 'arrival' 'cart'\n",
      " 'customers' 'vendors']\n",
      "tensor([-2.1898, -1.9823, -1.7587, -1.5895, -1.4819, -1.3134, -1.2838, -1.2058,\n",
      "        -1.1835, -1.1278], device='cuda:0')\n",
      "['pages' 'recorder' 'injuries' 'angle' 'diary' 'edge' 'degree' 'japanese'\n",
      " 'reflect' 'four']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 1585\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0072, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0065, 0.0064, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['snow' 'sea' 'beach' ... 'fishing' 'counter' 'bay']\n",
      " ['snow' 'fishing' 'winter' ... 'mine' 'bay' 'lake']\n",
      " ['bath' 'seller' 'snow' ... 'teacher' 'my' 'polish']\n",
      " ...\n",
      " ['collectibles' 'gun' 'counter' ... 'drives' 'collectables' 'side']\n",
      " ['shop' 'bath' 'counter' ... 'workshop' 'pharmacy' 'laboratory']\n",
      " ['bath' 'reception' 'tennis' ... 'garden' 'school' 'den']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([12.9526,  8.2729,  6.9227,  6.8128,  6.6989,  3.8989,  3.2130,  2.7829,\n",
      "         2.1829,  1.7814], device='cuda:0')\n",
      "['bath' 'shop' 'seller' 'collectibles' 'snow' 'collections' 'fishing'\n",
      " 'counter' 'collection' 'principal']\n",
      "tensor([-2.3863, -2.0189, -1.7470, -1.5873, -1.4885, -1.2846, -1.2152, -1.2054,\n",
      "        -1.1553, -1.1519], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'pair' 'fish' 'eagle' 'detail'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1981951.6250, 1843968.0000,  945289.4375,  859730.9375,  771008.0000,\n",
      "         597698.9375,  554062.6250,  503578.0938,  497970.7500,  428887.4688],\n",
      "       device='cuda:0')\n",
      "['shops' 'shop' 'snow' 'collectibles' 'kitchen' 'sea' 'beach' 'computers'\n",
      " 'bath' 'boats']\n",
      "tensor([-2.3863, -2.0189, -1.7470, -1.5873, -1.4885, -1.2846, -1.2152, -1.2054,\n",
      "        -1.1553, -1.1519], device='cuda:0')\n",
      "['thumbnail' 'gratis' 'itself' 'necessary' 'closed' 'becomes' 'grace'\n",
      " 'access' 'fail' 'come']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 289\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['sitemap' 'sand' 'dust' ... 'scripts' 'site' 'script']\n",
      " ['aluminum' 'track' 'mile' ... 'site' 'springs' 'concrete']\n",
      " ['wind' 'dust' 'mile' ... 'scripts' 'shock' 'sand']\n",
      " ...\n",
      " ['aluminum' 'sitemap' 'blowjobs' ... 'manufacturers' 'motors'\n",
      "  'classifieds']\n",
      " ['hotels' 'rentals' 'places' ... 'springs' 'suites' 'lodging']\n",
      " ['plants' 'springs' 'places' ... 'track' 'resort' 'yard']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([8.9852, 4.8095, 3.5293, 3.1515, 1.9944, 1.6587, 1.4927, 1.4567, 1.3822,\n",
      "        1.2389], device='cuda:0')\n",
      "['aluminum' 'sitemap' 'plants' 'wind' 'mile' 'plant' 'sand' 'dust'\n",
      " 'attractions' 'springs']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.3787, -2.0259, -1.7574, -1.5669, -1.4292, -1.3147, -1.2761, -1.2059,\n",
      "        -1.1961, -1.1218], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'eagle' 'detail'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([4399457.5000, 1297957.5000,  889150.9375,  877831.1875,  558686.1875,\n",
      "         398223.8125,  389264.8125,  347424.4688,  344388.2188,  333558.7812],\n",
      "       device='cuda:0')\n",
      "['plants' 'sitemap' 'cincinnati' 'aluminum' 'cities' 'apartments'\n",
      " 'recipes' 'gardens' 'woods' 'bedroom']\n",
      "tensor([-2.3787, -2.0259, -1.7574, -1.5669, -1.4292, -1.3147, -1.2761, -1.2059,\n",
      "        -1.1961, -1.1218], device='cuda:0')\n",
      "['serve' 'depends' 'detailed' 'full' 'fail' 'comparison' 'epson'\n",
      " 'difference' 'spread' 'feed']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2790\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['thongs' 'ft' 'beds' ... 'highest' 'patch' 'sites']\n",
      " ['womens' 'ft' 'highest' ... 'view' 'grid' 'pdt']\n",
      " ['styles' 'title' 'au' ... 'blonde' 'wa' 'sitting']\n",
      " ...\n",
      " ['title' 'womens' 'rear' ... 'back' 'pin' 'bit']\n",
      " ['womens' 'table' 'beds' ... 'estate' 'thongs' 'boards']\n",
      " ['evening' 'ft' 'estate' ... 'university' 'rear' 'back']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([15.5768,  3.5497,  2.0966,  1.9652,  1.7827,  1.7263,  1.5435,  1.4571,\n",
      "         1.4019,  1.3160], device='cuda:0')\n",
      "['womens' 'title' 'evening' 'citizen' 'patch' 'wholesale' 'au' 'ft'\n",
      " 'styles' 'type']\n",
      "tensor([-2.3869, -2.0281, -1.7438, -1.5905, -1.4882, -1.3152, -1.2723, -1.1879,\n",
      "        -1.1760, -1.1584], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'pair' 'detail' 'eagle'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([2781025.5000, 1885177.6250,  557507.8125,  531011.6250,  396588.9062,\n",
      "         368233.0000,  261877.7344,  241444.9062,  174593.7812,  150105.1719],\n",
      "       device='cuda:0')\n",
      "['universities' 'colleges' 'attorneys' 'cumshot' 'sponsors' 'womens'\n",
      " 'logos' 'companies' 'college' 'women']\n",
      "tensor([-2.3869, -2.0281, -1.7438, -1.5905, -1.4882, -1.3152, -1.2723, -1.1879,\n",
      "        -1.1760, -1.1584], device='cuda:0')\n",
      "['marked' 'negative' 'director' 'reply' 'territory' 'planet' 'heating'\n",
      " 'particular' 'larry' 'mark']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2007\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0064, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['springs' 'in' 'flat' ... 'asia' 'bit' 'off']\n",
      " ['kent' 'view' 'ak' ... 'melbourne' 'hunting' 'spring']\n",
      " ['asian' 'kent' 'melbourne' ... 'colleges' 'ontario' 'in']\n",
      " ...\n",
      " ['ak' 'kent' 'milfhunter' ... 'mod' 'aluminum' 'puzzle']\n",
      " ['offices' 'kent' 'accommodation' ... 'residential' 'rentals' 'room']\n",
      " ['kent' 'melbourne' 'springs' ... 'vista' 'spring' 'gardens']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([12.7828,  5.9100,  2.7418,  2.1511,  1.4909,  1.3599,  1.3414,  1.2124,\n",
      "         1.1977,  1.1277], device='cuda:0')\n",
      "['kent' 'asian' 'offices' 'melbourne' 'ut' 'colleges' 'springs'\n",
      " 'stainless' 'wi' 'in']\n",
      "tensor([-2.3653, -2.0338, -1.7572, -1.5785, -1.4688, -1.3128, -1.2046, -1.1960,\n",
      "        -1.1924, -1.1354], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'detail' 'pair'\n",
      " 'seal']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([3615160.0000, 1264274.8750,  874496.9375,  840559.4375,  340229.4688,\n",
      "         290446.4062,  258091.9531,  231799.9062,  220492.9062,  176822.4375],\n",
      "       device='cuda:0')\n",
      "['colleges' 'melbourne' 'universities' 'offices' 'apartments' 'college'\n",
      " 'office' 'university' 'gardens' 'houston']\n",
      "tensor([-2.3653, -2.0338, -1.7572, -1.5785, -1.4688, -1.3128, -1.2046, -1.1960,\n",
      "        -1.1924, -1.1354], device='cuda:0')\n",
      "['spread' 'shadow' 'cookies' 'shared' 'stewart' 'sharing' 'focused'\n",
      " 'talking' 'dress' 'standing']\n",
      "========================================================================================\n",
      "\n",
      "For Feature 2452\n",
      "text_probs_altered.softmax(): torch.Size([160, 5000])\n",
      "\n",
      "Softmax Over 160 Images:\n",
      "tensor([[0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        ...,\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063],\n",
      "        [0.0063, 0.0063, 0.0063,  ..., 0.0063, 0.0063, 0.0063]],\n",
      "       device='cuda:0')\n",
      "[['mesh' 'metabolism' 'nd' ... 'dynamics' 'mb' 'built']\n",
      " ['belgium' 'nd' 'team' ... 'crew' 'ft' 'dynamics']\n",
      " ['comfortable' 'belgium' 'ml' ... 'cute' 'built' 'milf']\n",
      " ...\n",
      " ['machines' 'navy' 'appliances' ... 'commission' 'milfhunter' 'machine']\n",
      " ['comfortable' 'bedrooms' 'wooden' ... 'home' 'appliances' 'lighting']\n",
      " ['outdoors' 'commons' 'outdoor' ... 'farm' 'ml' 'ft']]\n",
      "\n",
      "Most Changed, by Absolute Diff Over 160 Images:\n",
      "tensor([7.3001, 2.8828, 2.3131, 2.1808, 2.0598, 1.6970, 1.6197, 1.5053, 1.4643,\n",
      "        1.3143], device='cuda:0')\n",
      "['comfortable' 'ml' 'traditional' 'nd' 'organic' 'mesh' 'n' 'machines'\n",
      " 'nh' 'metabolism']\n",
      "tensor([-2.3846, -2.0342, -1.7467, -1.5906, -1.4867, -1.3145, -1.2030, -1.1876,\n",
      "        -1.1868, -1.1533], device='cuda:0')\n",
      "['guinea' 'dog' 'fucking' 'ray' 'python' 'fish' 'eagle' 'detail' 'pair'\n",
      " 'male']\n",
      "\n",
      "Most Changed, by Ratio Over 160 Images:\n",
      "tensor([1751680.3750, 1078067.6250,  644718.6250,  610641.6875,  342928.6562,\n",
      "         313877.4062,  268629.6250,  257149.1562,  247520.9062,  212235.6719],\n",
      "       device='cuda:0')\n",
      "['pittsburgh' 'bedroom' 'appliances' 'brands' 'navy' 'trucks'\n",
      " 'philadelphia' 'netherlands' 'farm' 'nj']\n",
      "tensor([-2.3846, -2.0342, -1.7467, -1.5906, -1.4867, -1.3145, -1.2030, -1.1876,\n",
      "        -1.1868, -1.1533], device='cuda:0')\n",
      "['pocket' 'recorder' 'remote' 'walker' 'ipod' 'step' 'survey' 'wake'\n",
      " 'side' 'bookmark']\n"
     ]
    }
   ],
   "source": [
    "# subtract from default, label, and print trends\n",
    "text_probs_altered.shape\n",
    "\n",
    "# selected_vocab = all_imagenet_class_names\n",
    "selected_vocab = larger_vocab\n",
    "\n",
    "# switch to datacomp eval??\n",
    "# we run this for each feature over all of imagenet eval and create average absolute/ratio\n",
    "# difference vectors\n",
    "for j, text_probs_altered in enumerate(text_probs_altered_list):\n",
    "    print(f\"{'============================================'*2}\\n\\nFor Feature {random_feat_idxs[j]}\")\n",
    "    logit_diff = text_probs_altered - text_probs_default\n",
    "#     print(f\"logit_diff.shape: {logit_diff.shape}\")\n",
    "    logit_diff_aggregate = logit_diff.sum(dim=0)\n",
    "#     print(f\"logit_diff_aggregate.shape: {logit_diff_aggregate.shape}\")\n",
    "    \n",
    "    logit_ratio = text_probs_altered/text_probs_default\n",
    "    logit_ratio_aggregate = logit_ratio.mean(dim=0)\n",
    "    \n",
    "    print(f\"text_probs_altered.softmax(): {text_probs_altered.softmax(0).shape}\")\n",
    "    text_probs_altered_softmax = text_probs_altered.softmax(0)\n",
    "    vals_softmax, idxs_softmax = torch.topk(text_probs_altered_softmax,k=10)\n",
    "    \n",
    "    print(f\"\\nSoftmax Over {text_probs_altered.shape[0]} Images:\\n{vals_softmax}\")\n",
    "    print(np.array(selected_vocab)[idxs_softmax.cpu()])\n",
    "    \n",
    "    vals_agg, idxs_agg = torch.topk(logit_diff_aggregate,k=10)\n",
    "    vals_least_agg, idxs_least_agg = torch.topk(logit_diff_aggregate,k=10,largest=False)\n",
    "    \n",
    "    ratios_agg, ratios_idxs_agg = torch.topk(logit_ratio_aggregate,k=10)\n",
    "    ratios_least_agg, ratios_idxs_least_agg = torch.topk(logit_ratio_aggregate,k=10,largest=False)\n",
    "    \n",
    "    vals, idxs = torch.topk(logit_diff,k=5)\n",
    "    vals_least, idxs_least = torch.topk(logit_diff,k=5,largest=False)\n",
    "    \n",
    "    ratios, ratios_idxs = torch.topk(logit_ratio,k=5)\n",
    "    ratios_least, ratios_idxs_least = torch.topk(logit_ratio,k=5,largest=False)\n",
    "    \n",
    "    print(f\"\\nMost Changed, by Absolute Diff Over {logit_diff.shape[0]} Images:\\n{vals_agg}\")\n",
    "    print(np.array(selected_vocab)[idxs_agg.cpu()])\n",
    "    print(vals_least_agg)\n",
    "    print(np.array(selected_vocab)[idxs_least_agg.cpu()])\n",
    "    \n",
    "    print(f\"\\nMost Changed, by Ratio Over {logit_diff.shape[0]} Images:\")\n",
    "    print(ratios_agg)\n",
    "    print(np.array(selected_vocab)[ratios_idxs_agg.cpu()])\n",
    "    print(vals_least_agg)\n",
    "    print(np.array(selected_vocab)[ratios_idxs_least_agg.cpu()])\n",
    "    \n",
    "    \n",
    "#     for i in range(logit_diff.shape[0]):\n",
    "# #         print(f\"\\nImage {i} ========================\\nMost Changed, by Absolute Diff\\n:{vals[i]}\")\n",
    "# #         print(np.array(selected_vocab)[idxs.cpu()][i])\n",
    "# #         print(vals_least[i])\n",
    "# #         print(np.array(selected_vocab)[idxs_least.cpu()][i])\n",
    "#         print(f\"\\nImage {i} Most Changed, by Ratio:\")\n",
    "#         print(ratios[i])\n",
    "#         print(np.array(selected_vocab)[ratios_idxs.cpu()][i])\n",
    "#         print(ratios_least[i])\n",
    "#         print(np.array(selected_vocab)[ratios_idxs_least.cpu()][i])\n",
    "        \n",
    "# #         text = tokenizer(np.array(selected_vocab)[ratios_idxs.cpu()][i])\n",
    "# #         text_least = tokenizer(np.array(selected_vocab)[ratios_idxs_least.cpu()][i])\n",
    "# #         text_features = og_model.encode_text(text.cuda())\n",
    "# #         text_features_least = og_model.encode_text(text_least.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract from default, label, and print trends\n",
    "text_probs_altered.shape\n",
    "\n",
    "# selected_vocab = all_imagenet_class_names\n",
    "selected_vocab = larger_vocab\n",
    "\n",
    "cov_stuff_avgs = []\n",
    "\n",
    "# switch to datacomp eval??\n",
    "# we run this for each feature over all of imagenet eval and create average absolute/ratio\n",
    "# difference vectors\n",
    "per_feat_avg_vectors = []\n",
    "for j, text_probs_altered in enumerate(text_probs_altered_list):\n",
    "    print(f\"\\n\\nFor Feature {random_feat_idxs[j]}\")\n",
    "    print(f\"\\n\\nFor Feature {random_feat_idxs[j]}\")\n",
    "    logit_diff = text_probs_altered - text_probs_default\n",
    "    logit_ratio = text_probs_altered/text_probs_default\n",
    "    \n",
    "    vals, idxs = torch.topk(logit_diff,k=5)\n",
    "    vals_least, idxs_least = torch.topk(logit_diff,k=5,largest=False)\n",
    "    \n",
    "    ratios, ratios_idxs = torch.topk(logit_ratio,k=5)\n",
    "    ratios_least, ratios_idxs_least = torch.topk(logit_ratio,k=5,largest=False)\n",
    "    \n",
    "    feat_avg_vectors = []\n",
    "    for i in range(logit_diff.shape[0]):\n",
    "#         print(f\"\\nImage {i} ========================\\nMost Changed, by Absolute Diff\\n:{vals[i]}\")\n",
    "#         print(np.array(all_imagenet_class_names)[idxs.cpu()][i])\n",
    "#         print(vals_least[i])\n",
    "#         print(np.array(all_imagenet_class_names)[idxs_least.cpu()][i])\n",
    "        \n",
    "        print(\"\\nMost Changed, by Ratio:\")\n",
    "        print(ratios[i])\n",
    "        print(np.array(selected_vocab)[ratios_idxs.cpu()][i])\n",
    "#         print(ratios_least[i])\n",
    "#         print(np.array(selected_vocab)[ratios_idxs_least.cpu()][i])\n",
    "        \n",
    "        text = tokenizer(np.array(selected_vocab)[ratios_idxs.cpu()][i])\n",
    "#         text_least = tokenizer(np.array(selected_vocab)[ratios_idxs_least.cpu()][i])\n",
    "        text_features = og_model.encode_text(text.cuda())\n",
    "        cov_over_images.append(text_features)\n",
    "#         text_features_least = og_model.encode_text(text_least.cuda())\n",
    "        print(torch.tril(torch.cov(text_features), diagonal=-1))\n",
    "        print(torch.tril(torch.cov(text_features), diagonal=-1).sum()/10)\n",
    "#         cov_stuff = torch.tril(torch.cov(text_features), diagonal=-1).sum()/10\n",
    "#         cov_stuff_least = torch.tril(torch.cov(text_features_least), diagonal=-1).sum()/10\n",
    "    print(torch.tril(torch.cov(torch.cat(cov_over_images)), diagonal=-1).shape)\n",
    "    n = torch.tril(torch.cov(torch.cat(cov_over_images)), diagonal=-1).shape[0]\n",
    "    num_elements = (n**2)/2 - n\n",
    "    cov_stuff = torch.tril(torch.cov(torch.cat(cov_over_images)), diagonal=-1).sum()/num_elements\n",
    "    cov_stuff_avgs.append(cov_stuff)\n",
    "#         cov_stuff_avgs_least.append(cov_stuff_least)\n",
    "    if j > 10:\n",
    "        break\n",
    "print(torch.tensor(cov_stuff_avgs).mean())\n",
    "# print(torch.tensor(cov_stuff_avgs_least).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_imgnet_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "feat_autolabels_default = defaultdict(Counter)\n",
    "for i in range(text_probs_default.shape[0]):\n",
    "    vals, idxs = torch.topk(text_probs_default[i],k=top_k_imgnet_labels)\n",
    "    print(i)\n",
    "    for k, idx in enumerate(idxs):\n",
    "        feat_autolabels_default[i][all_imagenet_class_names[idx]] += vals[k]\n",
    "        print(\"\\t\", all_imagenet_class_names[idx])\n",
    "feat_autolabels_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract from default, label, and print trends\n",
    "text_probs_altered.shape\n",
    "\n",
    "for text_probs_altered in text_probs_altered_list:\n",
    "    logit_diff = text_probs_altered - text_probs_default\n",
    "    print(logit_diff)\n",
    "    vals, idxs = torch.topk(logit_diff,k=5)\n",
    "    print(vals, np.array(all_imagenet_class_names[idxs])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imagenet_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "feat_autolabels_altered_list = []\n",
    "for text_probs_altered in text_probs_altered_list:\n",
    "    feat_autolabels_altered = defaultdict(Counter)\n",
    "    for i in range(text_probs_altered.shape[0]):\n",
    "        vals, idxs = torch.topk(text_probs_altered[i],k=top_k_imgnet_labels)\n",
    "#         print(i)\n",
    "        for k, idx in enumerate(idxs):\n",
    "            feat_autolabels_altered[i][all_imagenet_class_names[idx]] += vals[k]\n",
    "#             print(\"\\t\", all_imagenet_class_names[idx])\n",
    "    feat_autolabels_altered_list.append(feat_autolabels_altered)\n",
    "\n",
    "start_idx = 9\n",
    "end_idx = 10\n",
    "    \n",
    "h = 0\n",
    "for key in feat_autolabels_default:\n",
    "    print(f\"\\nfeat_autolabels_default img {key}:\\n {feat_autolabels_default[key]}\\n\")\n",
    "    h += 1\n",
    "    if h > end_idx:\n",
    "        break\n",
    "for i, f_a_a in enumerate(feat_autolabels_altered_list):\n",
    "    print(\"============= feature number \", i, \"====================\")\n",
    "    h = 0\n",
    "    for key in range(start_idx, end_idx):\n",
    "#         print(\"\\n\", key)\n",
    "#         for item in f_a_a[key]:\n",
    "#             print(\"\\t\", item, f_a_a[key][item].cpu().item())\n",
    "        print(f\"\\nf_a_a img {key}:\\n {f_a_a[key]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(text_probs_default.shape[0]):\n",
    "    vals, idxs = torch.topk(text_probs_default[i],k=1000)\n",
    "    print(i, ind_to_name[str(idxs[0].cpu().item())][1])\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "#     ax.xaxis.set_ticks((1000))\n",
    "#     ax.set_xticks(list(range(1000)), [ind_to_name[str(idxs[idx].cpu().item())][1] for idx in idxs])\n",
    "    plt.bar(idxs.cpu(), vals.cpu(), width=5)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch_images[2].cpu().permute((1,2,0)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def get_heatmap(\n",
    "          image,\n",
    "          model,\n",
    "          sparse_autoencoder,\n",
    "          feature_id,\n",
    "): \n",
    "    image = image.to(cfg.device)\n",
    "    _, cache = model.run_with_cache(image.unsqueeze(0))\n",
    "\n",
    "    post_reshaped = einops.rearrange(cache[sparse_autoencoder.cfg.hook_point], \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "    # Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n",
    "    sae_in =  post_reshaped - sparse_autoencoder.b_dec # Remove decoder bias as per Anthropic\n",
    "    print(f\"sae_in.shape: {sae_in.shape}\")\n",
    "    acts = einops.einsum(\n",
    "            sae_in,\n",
    "            sparse_autoencoder.W_enc[:, feature_id],\n",
    "            \"x d_in, d_in -> x\",\n",
    "        )\n",
    "    return acts \n",
    "     \n",
    "def image_patch_heatmap(activation_values,image_size=224, pixel_num=14):\n",
    "    activation_values = activation_values.detach().cpu().numpy()\n",
    "    activation_values = activation_values[1:]\n",
    "    activation_values = activation_values.reshape(pixel_num, pixel_num)\n",
    "\n",
    "    # Create a heatmap overlay\n",
    "    heatmap = np.zeros((image_size, image_size))\n",
    "    patch_size = image_size // pixel_num\n",
    "\n",
    "    for i in range(pixel_num):\n",
    "        for j in range(pixel_num):\n",
    "            heatmap[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size] = activation_values[i, j]\n",
    "\n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "grid_size = 1\n",
    "fig, axs = plt.subplots(int(np.ceil(len(images)/grid_size)), grid_size, figsize=(15, 15))\n",
    "name=  f\"Category: uhh,  Feature: {0}\"\n",
    "fig.suptitle(name)#, y=0.95)\n",
    "for ax in axs.flatten():\n",
    "    ax.axis('off')\n",
    "complete_bid = []\n",
    "\n",
    "heatmap = get_heatmap(batch_images[2], model,sparse_autoencoder, 10000)\n",
    "heatmap = image_patch_heatmap(heatmap, pixel_num=224//cfg.patch_size)\n",
    "\n",
    "display = batch_images[2].cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "has_zero = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(display)\n",
    "plt.imshow(heatmap, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(display)\n",
    "plt.imshow(heatmap, alpha=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
