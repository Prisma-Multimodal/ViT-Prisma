import torch

from experiments.train_remote_sae import DEVICE
from vit_prisma.configs.HookedViTConfig import HookedViTConfig
from vit_prisma.sae.config import VisionModelSAERunnerConfig
from vit_prisma.utils.constants import DATA_DIR, MODEL_DIR, MODEL_CHECKPOINTS_DIR


# laion/CLIP-ViT-B-16-CommonPool.L-s1B-b8K
CLIP_CONFIG = HookedViTConfig(
    n_layers=8,
    d_model=384,
    d_head=64,
    n_heads=6,
    d_mlp=1536,
    model_name="laion/CLIP-ViT-B-16-CommonPool.L-s1B-b8K",
    activation_name="gelu",
    d_vocab=-1,
    eps=1e-06,
    device=DEVICE,
    initializer_range=0.02,
    init_weights=True,
    dtype=torch.float32,
    rotary_base=10000,
    n_channels=3,
    patch_size=16,
    image_size=128,
    classification_type="cls",
    n_classes=10,
    return_type="class_logits",
    use_wandb=True,
    wandb_team_name="Stevinson",
    wandb_project_name="cifar10",
    lr=5e-4,
    weight_decay=0.01,
    loss_fn_name="CrossEntropy",
    batch_size=256,
    warmup_steps=1000,
    scheduler_step=1000,
    scheduler_gamma=0.9,
    num_epochs=50,
    save_dir=str(MODEL_CHECKPOINTS_DIR / "cifar10/clean"),
    save_checkpoints=True,
    save_cp_frequency=1000,
    log_frequency=50,
    layer_norm_pre=True,
    post_embedding_ln=True,
    normalization_type="LN",
    attn_dropout_rate=0.05,
    mlp_dropout_rate=0.05,
    scheduler_type="CosineAnnealing",
    attack_method="L2",
    attack_epsilon=10.0,
    attack_num_iters=100,
    attack_alpha=0.5,
)


CLIP_SAE_CONFIG = VisionModelSAERunnerConfig(
    model_class_name="HookedViT",
    model_name="open-clip:laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K",
    hook_point_layer=9,
    # architecture="standard",
    # n_validation_runs=10,
    # num_workers=16,
    # cls_token_only=False,
    # use_patches_only=False,
    # layer_subtype="hook_mlp_out",
    hook_point_head_index=None,
    context_size=50,
    use_cached_activations=False,
    cached_activations_path="activations/_network_scratch_s_sonia.joseph_datasets_kaggle_datasets/open-clip:laion_CLIP-ViT-B-32-DataComp.XL-s13B-b90K/blocks.9.hook_mlp_out",
    d_in=768,
    activation_fn_str="relu",
    activation_fn_kwargs={},
    max_grad_norm=1.0,
    initialization_method="encoder_transpose_decoder",
    normalize_activations=None,
    n_batches_in_buffer=20,
    store_batch_size=32,
    num_epochs=10,
    total_training_images=13000000,
    total_training_tokens=650000000,
    image_size=224,
    device={
        "__type__": "torch.device",
        "value": "cuda"
    },
    seed=42,
    dtype={
        "__type__": "torch.dtype",
        "value": "torch.float32"
    },
    verbose=False,
    b_dec_init_method="geometric_median",
    expansion_factor=64,
    from_pretrained_path=None,
    d_sae=49152,
    l1_coefficient=8e-05,
    lp_norm=1,
    lr=0.0004,
    lr_scheduler_name="cosineannealingwarmup",
    lr_warm_up_steps=200,
    train_batch_size=4096,
    dataset_name="imagenet1k",
    dataset_path="/network/scratch/s/sonia.joseph/datasets/kaggle_datasets",
    dataset_train_path="/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/train",
    dataset_val_path="/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/val",
    use_ghost_grads=True,
    feature_sampling_window=1000,
    dead_feature_window=5000,
    dead_feature_threshold=1e-08,
    log_to_wandb=True,
    wandb_project="clip_b_mlp_out_sae_hyperparam_sweep",
    wandb_entity=None,
    wandb_log_frequency=100,
    n_checkpoints=10,
    checkpoint_path="/network/scratch/s/sonia.joseph/checkpoints/clip-b/cc619a60-clip_b_mlp_out_sae_hyperparam_sweep"
)