{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e6357a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_prisma.sae import VisionModelSAERunnerConfig\n",
    "from vit_prisma.sae import VisionSAETrainer\n",
    "from vit_prisma.transforms import get_clip_val_transforms\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac9851",
   "metadata": {},
   "source": [
    "# Train the SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e3838",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Deprecated field 'total_training_images' found in config. It will be ignored.\n",
      "WARNING:root:Deprecated field 'total_training_tokens' found in config. It will be ignored.\n",
      "WARNING:root:Deprecated field 'd_sae' found in config. It will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE from /home/mila/s/sonia.joseph/.cache/huggingface/hub/models--Prisma-Multimodal--sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-1e-05/snapshots/b46d6e9d6c114a53364c5c172ea5023e0e52d4e2/weights.pt...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load an SAE\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "from vit_prisma.sae import SparseAutoencoder\n",
    "\n",
    "def load_sae(repo_id, file_name, config_name):\n",
    "    # Step 1: Download SAE weights and SAE config from Hugginface\n",
    "    sae_path = hf_hub_download(repo_id, file_name) # Download SAE weights\n",
    "    hf_hub_download(repo_id, config_name) # Download SAE config\n",
    "\n",
    "    # Step 2: Now load the pretrained SAE weights from where you just downloaded them\n",
    "    print(f\"Loading SAE from {sae_path}...\")\n",
    "    sae = SparseAutoencoder.load_from_pretrained(sae_path) # This now automatically gets the config.json file in that folder and converts it into a VisionSAERunnerConfig object\n",
    "    return sae\n",
    "\n",
    "# Change the repo_id to the Huggingface repo of your chosen SAE. See /docs for a list of SAEs.\n",
    "repo_id = \"Prisma-Multimodal/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-1e-05\" \n",
    "\n",
    "file_name = \"weights.pt\" # Usually weights.pt but may have slight naming variation. See the chosen HF repo for the exact file name\n",
    "config_name = \"config.json\"\n",
    "\n",
    "sae = load_sae(repo_id, file_name, config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2584867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedViT(\n",
       "  (embed): PatchEmbedding(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "  )\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbedding()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (hook_full_embed): HookPoint()\n",
       "  (ln_pre): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (hook_ln_pre): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (hook_ln_final): HookPoint()\n",
       "  (head): Head()\n",
       "  (hook_post_head_pre_normalize): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vit_prisma.models.model_loader import load_hooked_model\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = sae.cfg.model_name\n",
    "model = load_hooked_model(model_name)\n",
    "model.to(DEVICE) # Move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d885360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your ImageNet Paths here\n",
    "from vit_prisma.transforms import get_clip_val_transforms\n",
    "\n",
    "imagenet_train_path = '/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/train'\n",
    "imagenet_validation_path = '/network/scratch/s/sonia.joseph/datasets/kaggle_datasets/ILSVRC/Data/CLS-LOC/val'\n",
    "\n",
    "data_transforms = get_clip_val_transforms()\n",
    "train_dataset = torchvision.datasets.ImageFolder(imagenet_train_path, transform=data_transforms)\n",
    "eval_dataset = torchvision.datasets.ImageFolder(imagenet_validation_path, transform=data_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29f0fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_trainer_cfg = VisionModelSAERunnerConfig(\n",
    "    hook_point_layer=10,\n",
    "    layer_subtype='hook_resid_post',\n",
    "    dataset_name=\"imagenet\",\n",
    "    feature_sampling_window=1000,\n",
    "    activation_fn_str='relu',\n",
    "    checkpoint_path = '/network/scratch/s/sonia.joseph'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e7d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VisionSAETrainer(sae_trainer_cfg, model, train_dataset, eval_dataset)\n",
    "sae = trainer.run()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
