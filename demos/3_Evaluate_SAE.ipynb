{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df783e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853798d6",
   "metadata": {},
   "source": [
    "# Evaluate vision SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33387114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE from /home/mila/s/sonia.joseph/.cache/huggingface/hub/models--Prisma-Multimodal--sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-1e-05/snapshots/b46d6e9d6c114a53364c5c172ea5023e0e52d4e2/weights.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Deprecated field 'total_training_images' found in config. It will be ignored.\n",
      "WARNING:root:Deprecated field 'total_training_tokens' found in config. It will be ignored.\n",
      "WARNING:root:Deprecated field 'd_sae' found in config. It will be ignored.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StandardSparseAutoencoder(\n",
       "  (hook_sae_in): HookPoint()\n",
       "  (hook_hidden_pre): HookPoint()\n",
       "  (hook_hidden_post): HookPoint()\n",
       "  (hook_sae_out): HookPoint()\n",
       "  (activation_fn): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load SAE\n",
    "\n",
    "# Load an SAE\n",
    "from huggingface_hub import hf_hub_download, list_repo_files\n",
    "from vit_prisma.sae import SparseAutoencoder\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_sae(repo_id, file_name, config_name):\n",
    "    # Step 1: Download SAE weights and SAE config from Hugginface\n",
    "    sae_path = hf_hub_download(repo_id, file_name) # Download SAE weights\n",
    "    hf_hub_download(repo_id, config_name) # Download SAE config\n",
    "\n",
    "    # Step 2: Now load the pretrained SAE weights from where you just downloaded them\n",
    "    print(f\"Loading SAE from {sae_path}...\")\n",
    "    sae = SparseAutoencoder.load_from_pretrained(sae_path) # This now automatically gets the config.json file in that folder and converts it into a VisionSAERunnerConfig object\n",
    "    return sae\n",
    "\n",
    "# Change the repo_id to the Huggingface repo of your chosen SAE. See /docs for a list of SAEs.\n",
    "repo_id = \"Prisma-Multimodal/sparse-autoencoder-clip-b-32-sae-vanilla-x64-layer-10-hook_mlp_out-l1-1e-05\" \n",
    "\n",
    "file_name = \"weights.pt\" # Usually weights.pt but may have slight naming variation. See the chosen HF repo for the exact file name\n",
    "config_name = \"config.json\"\n",
    "\n",
    "sae = load_sae(repo_id, file_name, config_name)\n",
    "sae.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c4eebfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedViT(\n",
       "  (embed): PatchEmbedding(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "  )\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbedding()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (hook_full_embed): HookPoint()\n",
       "  (ln_pre): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (hook_ln_pre): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (hook_ln_final): HookPoint()\n",
       "  (head): Head()\n",
       "  (hook_post_head_pre_normalize): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "from vit_prisma.models.model_loader import load_hooked_model\n",
    "\n",
    "\n",
    "model_name = sae.cfg.model_name\n",
    "model = load_hooked_model(model_name)\n",
    "model.to(DEVICE) # Move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f9abd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 391/391 [01:46<00:00,  3.68it/s, L0=3093.3625, Cosine Sim=0.991870]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average L0 (features activated): 792.035522\n",
      "Average L0 (features activated) per CLS token: 965.520325\n",
      "Average L0 (features activated) per image: 3086.346780\n",
      "Average Cosine Similarity: 0.9917\n",
      "Average Loss: 6.762171\n",
      "Average Reconstruction Loss: 6.762009\n",
      "Average Zero Ablation Loss: 6.772939\n",
      "Average CE Score: 1.015572\n",
      "% CE recovered: 101.496382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate vision SAE\n",
    "from vit_prisma.sae import SparsecoderEval\n",
    "\n",
    "eval_runner = SparsecoderEval(sae, model)\n",
    "\n",
    "metrics = eval_runner.run_eval(is_clip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e474c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
